# Multimodal Transformer System

A sophisticated multimodal transformer neural network system for processing and learning from multiple types of sequential data simultaneously. Originally designed with stock movement prediction in mind, but can be used for training on any type of time-series data including financial markets, weather patterns, sensor data, or any sequential multimodal datasets.

**For Developers**: See [`TECHNICAL_NOTES.md`](TECHNICAL_NOTES.md) for detailed implementation specifics, batch generation logic, and system internals.

## TL;DR

**What it is**: A modern multimodal transformer that trains on multiple data types simultaneously (prices, volume, time, indicators) with selective cross-attention and sophisticated processing pipelines.

**Key Features**: YAML configuration, sequential data processing, directional prediction metrics, vocabulary size control, and planned live broker API integration for real-time trading predictions.

**Getting Started**: Configure `input_schemas.yaml` with your data sources, set training parameters in `config.yaml`, run `python main.py`. The system handles everything from data loading to multimodal training with comprehensive evaluation metrics.

## Disclaimer

**IMPORTANT: This software is provided for educational and research purposes only.**

- **No Financial Advice**: This software does not provide financial, investment, or trading advice.
- **No Warranty**: This software is provided "as is" without warranty of any kind, express or implied.
- **No Liability**: The author accepts no responsibility for any financial losses, damages, or other consequences resulting from the use of this software.
- **Trading Risks**: Trading and investing in financial markets involves substantial risk of loss. Past performance does not guarantee future results.
- **Use at Your Own Risk**: Users are solely responsible for any decisions made based on this software's outputs.

By using this software, you acknowledge that you understand and accept these terms.

## User Interface Guide

### Primary Files You'll Interact With

**Configuration Files:**
- **`input_schemas.yaml`** - **(Recommended)** Define your data sources and processing pipelines with type-safe validation
- **`config.py`** - Set training hyperparameters (`batch_size`, `learning_rate`, etc.) and system settings
- **`main.py`** - Execution entry point (no editing needed)

**Optional Advanced Files:**
- **`config_manager.py`** - Programmatic YAML configuration management for advanced users
- **Custom processing functions** - Create your own `.py` files for specialized data transformations

### Files You Should NOT Edit
The following are internal library code that handles the heavy lifting:
- `model.py`, `training_utils.py`, `data_utils.py` - Core system implementation
- `schema.py`, `processing_registry.py`, `processing_pipeline.py` - Internal infrastructure
- `compatibility_layer.py`, `experimental_utils.py` - System internals

### Configuration Methods

The system supports two configuration approaches to suit different use cases:

#### **Method 1: YAML Configuration (Recommended for Most Users)**
**Best for:** Standard usage, experimentation, sharing configurations
- **Simple setup**: User-friendly YAML files with validation and documentation
- **Two files**: `input_schemas.yaml` (data sources) + `config.yaml` (training parameters)
- **Validated**: Automatic error checking with helpful messages

**Quick Start:**
1. Edit `input_schemas.yaml` -> Define your data sources and processing
2. Edit `config.yaml` -> Set training parameters and model architecture
3. Run `python main.py` -> Start training

#### **Method 2: Programmatic Configuration (Advanced Users)**
**Best for:** Automation, dynamic setups, hyperparameter sweeps, ML pipelines
- **Flexible**: Full Python power for conditional logic, loops, dynamic data discovery
- **Single file**: Everything defined in `config.py` as Python variables
- **Programmable**: Can be generated by scripts or integrated with other systems

**Quick Start:**
1. Edit `config.py` -> Set both training parameters AND data sources (as Python variables)
2. Run `python main.py` -> Start training

#### **Advanced Features**
- **Custom Processing**: Create your own data processing functions in separate `.py` files
- **External Functions**: Reference them in YAML with `module: "your_module.your_function"`
- **Seamless Switching**: Use either method without code changes

---

## Table of Contents

- [TL;DR](#tldr)
- [Disclaimer](#disclaimer)
- [User Interface Guide](#user-interface-guide)
  - [Primary Files You'll Interact With](#primary-files-youll-interact-with)
  - [Typical Workflows](#-typical-workflows)
- [Features](#features)
  - [Core Capabilities](#core-capabilities)
  - [Processing Pipeline](#processing-pipeline)
  - [Configuration System](#configuration-system)
- [Quick Start](#quick-start)
  - [Try the Examples First!](#try-the-examples-first)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
  - [Basic Usage](#basic-usage)
- [Project Structure](#project-structure)
- [Configuration](#configuration)
  - [Input Schemas (`input_schemas.yaml`)](#input-schemas-input_schemasyaml)
  - [System Configuration (`config.yaml`)](#system-configuration-configyaml)
- [Processing Functions](#processing-functions)
  - [Built-in Functions](#built-in-functions)
  - [External Functions](#external-functions)
- [Model Architecture](#model-architecture)
  - [Multimodal Transformer](#multimodal-transformer)
  - [Key Components](#key-components)
- [Training and Evaluation](#training-and-evaluation)
  - [Training Process](#training-process)
  - [Evaluation Metrics](#evaluation-metrics)
  - [Training Command](#training-command)
  - [Generation & Prediction (Future Integration)](#generation--prediction-future-integration)
- [Troubleshooting](#troubleshooting)
- [Advanced Usage](#advanced-usage)
  - [Example Modality Configurations](#example-modality-configurations)
  - [Custom Processing Pipelines](#custom-processing-pipelines)
  - [Hyperparameter Tuning](#hyperparameter-tuning)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgments](#acknowledgments)

---

## Features

### Core Capabilities
- **Multimodal Learning**: Process multiple data types (prices, volume, time, categorical) simultaneously
- **Cross-Attention Mechanism**: Learn relationships between different modalities
- **Sequential Processing Pipelines**: Chain unlimited data processing functions
- **Flexible Configuration**: Modern YAML-based configuration with full compatibility for both configuration methods

### Processing Pipeline
- **Built-in Functions**: Range scaling, binning, conversion to percent changes
- **External Functions**: Import and use custom processing functions from any Python module
- **Sequential Data Flow**: `raw_data -> function1 -> function2 -> function3 -> processed_data`
- **Step Control**: Use `enabled: false` to temporarily skip processing steps without deleting configuration
- **Error Handling**: Comprehensive validation and clear error messages

### Configuration System
- **YAML Configuration**: User-friendly configuration files with extensive documentation
- **Comprehensive Validation**: System validates all settings on startup with clear error messages

## Quick Start

### Try the Examples First!

**New to the system?** Start with the ready-to-run examples in the `examples/` folder:

```bash
# Install dependencies
pip install torch pyyaml numpy pandas

# Run Example 1 (Basic)
python examples/run_example.py 1

# Run Example 2 (Advanced)
python examples/run_example.py 2
```

Both examples include sample data and complete configurations. See [`examples/README.md`](examples/README.md) for detailed documentation.

### Prerequisites
- Python 3.7+
- PyTorch
- PyYAML
- NumPy
- Pandas

### Installation
```bash
# Clone or download the project
git clone https://github.com/tsnuk/trade-AId-multimodal-transformer
cd trade-AId-multimodal-transformer

# Install dependencies
pip install torch pyyaml numpy pandas
```

### Basic Usage

1. **Configure your data sources** in `input_schemas.yaml`:
```yaml
modalities:
  - modality_name: "Stock Prices"
    path: "./data/stocks/"  # Folder (loads all files) OR "./data/stocks/AAPL.csv" (single file with extension)
    column_number: 4
    has_header: true
    processing_steps:
      - function: range_numeric_data
        args: {num_whole_digits: 2, decimal_places: 1}
    cross_attention: true
```

2. **Set training parameters** in `config.yaml`:
```yaml
training_parameters:
  batch_size: 32
  max_iters: 5000
  learning_rate: 0.0003
model_architecture:
  n_embd: 128
  n_head: 8
  n_layer: 6
```

3. **Run training**:
```bash
python main.py
```

The system automatically creates an `output/` directory for:
- **Training logs**: `output/training_log.txt` (comprehensive training progress and metrics)
- **Model weights**: `output/TransformerModel.pth` (saved model checkpoints)

### Important Data Loading Considerations

**Path Configuration**:
- **Single file**: Include full filename with extension: `"./data/stocks/AAPL.csv"` or `"./data/stocks/AAPL.txt"`
- **Multiple files (folder)**: Use folder path ending with `/`: `"./data/stocks/"`
- **IMPORTANT**: When specifying a single file, you MUST include the file extension (`.csv`, `.txt`, etc.)

**File Boundary Handling**: When loading multiple files, the system ensures training sequences never cross file boundaries, maintaining data integrity.

**Data Alignment Requirements**: All modalities must have equal-length datasets where data points at the same index correspond to the same time period.

**Single vs. Multiple Modalities Strategy**:
- **Single Modality**: Combine similar data sources (e.g., multiple stock tickers) to learn general patterns
- **Separate Modalities**: Use for fundamentally different data types (e.g., prices vs. sentiment) with explicit cross-modal attention

**File Caching System**: The system includes automatic file caching to optimize performance when the same data files are accessed multiple times during processing. Files are cached in memory after first load, eliminating redundant disk reads. The cache is automatically cleared after data preparation completes to free memory before training begins.

### Data Splitting Methods

The system supports two methods for splitting the loaded data into training and validation sets. These methods are configured in the `data_splitting` section of the `config.yaml` file:

**Splitting by Percentage (`validation_size`)**: You can specify a fraction of the data to be used for the validation set using the `validation_size` parameter (e.g., `validation_size: 0.1` for 10% validation data). If `num_validation_files` is set to 0, this method is used. The split is applied to the entire loaded dataset for each modality, taking the specified percentage of data from the end of the dataset for validation and the remainder for training.

**Splitting by Number of Files (`num_validation_files`)**: If you are loading data from a folder (`path` in config points to a folder) and you set the `num_validation_files` parameter to a positive integer (e.g., `num_validation_files: 2`), the combined length of the specified number of files from the end of the list of loaded files for the first modality will be calculated. This calculated length will then be used as the size of the validation set for all modalities. The data points up to this length (from the end) will form the validation set, and the preceding data will form the training set. This method is useful when you want to designate a specific number of files (typically the last ones) to be used for validation, allowing for scenarios like training on data from a set of files (e.g., individual stocks) and validating on data from a different set of files that appear later in the concatenated list (e.g., an index like S&P500), to evaluate the model's ability to generalize or predict different but related data sources. When this parameter is set, it overrides the `validation_size` setting for determining the split.

The data splitting logic ensures that the split points are consistent across all modalities to maintain alignment between modalities in the training and validation sets.

## Project Structure

```
trade-AId-multimodal-transformer/
|-- README.md                  # Main documentation
|-- LICENSE                    # MIT License
|-- .gitignore                 # Git ignore rules
|
|-- Core System Files:
|   |-- main.py                    # Main training script
|   |-- model.py                   # Neural network architecture
|   |-- data_utils.py              # Data loading and processing
|   |-- training_utils.py          # Training and evaluation utilities
|   |-- file_cache.py              # File caching system
|
|-- Configuration System:
|   |-- config.yaml                # System settings (YAML)
|   |-- input_schemas.yaml         # Modality configurations (YAML)
|   |-- config.py                  # Python configuration variables
|   |-- config_manager.py          # YAML configuration management
|   |-- config_utils.py            # Configuration utilities
|   |-- schema.py                  # Configuration validation
|   |-- compatibility_layer.py     # Configuration compatibility
|
|-- Processing System:
|   |-- processing_registry.py     # Function registry
|   |-- processing_pipeline.py     # Sequential processing engine
|
|-- Documentation:
|   |-- PROGRAM_FLOW.md            # Execution flow diagram
|   |-- TECHNICAL_NOTES.md         # Implementation details
|
|-- data/                      # Sample data for testing
|   '-- 1day_candles/          # Stock price data (AAPL, MSFT, etc.)
|
|-- examples/                 # Example configurations
|   |-- README.md             # Examples documentation
|   |-- configs/              # Sample config files
|   '-- sample_data/          # Example datasets
|
|-- tests/                    # Test files
|   |-- test_parameter_coverage.py
|   '-- test_runtime_parameter_behavior.py
|
|-- output/                   # Generated during training (auto-created)
|   |-- training_log.txt      # Training logs and metrics
|   '-- TransformerModel.pth  # Saved model weights
|
'-- run_example.py            # Example runner script
```

## Configuration

The system offers two powerful configuration methods. Choose the one that best fits your workflow:

---

## Method 1: YAML Configuration (Recommended)

**Perfect for:** Most users, experimentation, sharing setups, production deployments

### **Advantages:**
- **User-friendly**: Easy to read and edit
- **Validated**: Automatic error checking with helpful messages
- **Documented**: Built-in comments explain every parameter
- **Shareable**: Easy to share configurations between projects
- **Version control friendly**: Clean diffs in git
- **No programming required**: Just edit text files

### **Setup Process:**

#### **Step 1: Configure Data Sources (`input_schemas.yaml`)**

Define your data sources and processing pipelines:

```yaml
modalities:
  # Stock price data with scaling
  - modality_name: "Stock Prices"
    path: "./your_data_folder/"
    column_number: 13
    has_header: true
    processing_steps:
      - function: range_numeric_data
        args:
          num_whole_digits: 2
          decimal_places: 1
    cross_attention: true

  # Same data converted to percentages and binned
  - modality_name: "Price Changes"
    path: "./your_data_folder/"
    column_number: 13
    has_header: true
    processing_steps:
      - function: convert_to_percent_changes
        args: {}
      - function: bin_numeric_data
        args:
          num_bins: 6
          outlier_percentile: 10
          exponent: 2.0
    cross_attention: false
```

#### **Step 2: Configure Training Parameters (`config.yaml`)**

Set training parameters and model architecture:

```yaml
training_parameters:
  batch_size: 32           # Training batch size
  block_size: 64          # Sequence length
  max_iters: 10000         # Training iterations
  learning_rate: 0.0003    # Learning rate

model_architecture:
  n_embd: 128              # Embedding dimension
  n_head: 6                # Attention heads
  n_layer: 6               # Transformer layers
  dropout: 0.2             # Dropout rate

project_settings:
  device: cuda             # 'cuda', 'cpu', or 'auto'
  create_new_model: 1      # 1=new model, 0=load existing
  save_model: 1            # 1=save during training, 0=no saving
```

#### **Step 3: Run Training**
```bash
python main.py
```

The system automatically detects and validates your YAML configuration files.

---

## Method 2: Programmatic Configuration (Advanced)

**Perfect for:** Automation, dynamic setups, hyperparameter sweeps, ML pipelines, integration with other systems

### **Advantages:**
- **Dynamic**: Compute values, use conditionals, loops, environment variables
- **Programmable**: Generated by scripts, integrated with Python workflows
- **Flexible**: Full Python power for complex logic
- **Automated**: Perfect for hyperparameter sweeps and batch experiments
- **No file parsing**: Direct Python objects and variables

### **Setup Process:**

#### **Single File Configuration (`config.py`)**

Define everything in Python variables:

```python
# Training parameters
batch_size = 32
block_size = 64
max_iters = 10000
learning_rate = 0.0003

# Model architecture
n_embd = 128
n_head = 6
n_layer = 6
dropout = 0.2

# Project settings
device = 'cuda'
create_new_model = 1
save_model = 1

# Data sources (input schemas)
input_schema_1 = [
    './your_data_folder/',  # path
    13,                      # column number
    True,                    # has header
    False,                   # convert to percentages
    2,                       # num whole digits
    1,                       # decimal places
    None,                    # num bins
    None,                    # randomness size
    True,                    # cross attention
    'S&P 500 Stock Prices'   # modality name
]

input_schema_2 = [
    './your_data_folder/',  # same data, different processing
    13,                      # column number
    True,                    # has header
    True,                    # convert to percentages
    None,                    # num whole digits (not used for percentages)
    None,                    # decimal places (not used for percentages)
    6,                       # num bins (group into 6 categories)
    None,                    # randomness size
    False,                   # cross attention
    'Price Changes'          # modality name
]

# Number of input schemas to use
num_input_schemas = 2
```

### **Use Cases for Programmatic Configuration:**

#### **1. Automated Hyperparameter Sweeps**
```python
# Grid search example
for learning_rate in [0.001, 0.0003, 0.0001]:
    for n_embd in [128, 256, 512]:
        # Configure and run training
        run_training_experiment()
```

#### **2. Dynamic Data Discovery**
```python
import glob
# Automatically find all CSV files
data_files = glob.glob("./data/*.csv")
input_schema_1[0] = data_files[0]  # Use first file found
```

#### **3. Environment-Based Configuration**
```python
import os
if os.getenv('ENVIRONMENT') == 'production':
    batch_size = 64
    max_iters = 50000
else:
    batch_size = 8
    max_iters = 1000
```

#### **4. Integration with ML Pipelines**
```python
def configure_for_dataset(dataset_path, model_size):
    global input_schema_1, n_embd
    input_schema_1[0] = dataset_path
    n_embd = model_size

# Called from another Python script
configure_for_dataset('./new_data/', 512)
```

---

## Choosing the Right Method

### **Use YAML Configuration When:**
- You're a standard user wanting to train models
- You want validation and error checking
- You're sharing configurations with others
- You want clean, readable configuration files
- You don't need dynamic behavior

### **Use Programmatic Configuration When:**
- You're building automated ML pipelines
- You need dynamic configuration based on data discovery
- You're doing hyperparameter sweeps or grid searches
- You're integrating with other Python systems
- You need conditional logic in your configurations
- You're an advanced user comfortable with Python

### **Technical Notes:**
- **Automatic Detection**: System automatically detects which method you're using
- **No Conflicts**: YAML takes priority if both configurations exist
- **Seamless Switching**: Switch between methods without code changes
- **Unified Backend**: Both methods feed into the same training system

## Processing Functions

### Built-in Functions

#### Data Normalization & Vocabulary Control

**`range_numeric_data`**: Scale values to specified digits and decimal places
- **Purpose**: Normalizes data magnitude and **controls vocabulary size** by ranging data and limiting decimal precision
- **Why important**: Smaller vocabularies improve training efficiency and reduce memory usage
- **Parameters**:
  - `num_whole_digits` (int or null): Desired number of digits before decimal point
  - `decimal_places` (int or null): Number of decimal places to round to
- **Examples**:
  ```yaml
  # Scale stock prices to 3 whole digits, 2 decimal places
  # 1,234.5678 -> 123.46
  - function: range_numeric_data
    args: {num_whole_digits: 3, decimal_places: 2}

  # Round to 1 decimal place only
  # 15.789 -> 15.8
  - function: range_numeric_data
    args: {decimal_places: 1}
  ```
- **Vocabulary Impact**: Reduces unique values from thousands to hundreds
- **Rounding Effect**: Rounding has a similar effect on vocab size as ranging
  - **Example**: Reducing decimal places from 2 to 1 truncates vocab size by factor of 10
  - **Trade-off**: Comes at a cost - rounding reduces data resolution, which reduces learnt accuracy from the data
  - **Data Conforming**: Ensures uniformity when datasets contain inconsistent decimal places
    - **Problem**: Some price datasets mix 2-decimal (45.67) and 3+ decimal (45.678) prices
    - **Solution**: Rounding to 2 decimal places ensures uniformity and eliminates unnecessary vocabulary additions
    - **Result**: Cleaner data and more efficient vocabulary without precision artifacts
    - **Why this matters**: Real financial datasets often have inconsistent precision that creates artificial vocabulary expansion - rounding solves data quality issues while improving model efficiency
  - **Balance needed**: Between vocabulary efficiency and data precision

**`bin_numeric_data`**: Group values into discrete bins/categories
- **Purpose**: Dramatically **reduces vocabulary size** by grouping similar values using exponential distribution
- **Use case**: When exact values matter less than relative ranges or trends
- **Ideal for**: Percentage changes, volume data, VIX (volatility index), RSI, MACD, and other indicators where relative magnitude is more important than precise values
- **Parameters**:
  - `num_bins` (int): Number of groups for positive values (total = 2 x num_bins + 1 including zero)
  - `outlier_percentile` (float, default=5): Excludes extreme values from influencing bin boundaries
    - **Purpose**: Prevents data spikes from skewing bin boundary calculations
    - **How it works**: Calculates bin boundaries using only values between specified percentiles
    - **Important**: Outlier values are still placed in bins (the outermost ones), just don't influence boundary placement
    - **Example**: `outlier_percentile=10` uses only middle 80% of data to set bin boundaries
    - **Range**: 0-100 (percentage of data to exclude from each end)
  - `exponent` (float, default=2.0): Controls exponential bin distribution
    - **Purpose**: Creates bins with sizes that grow exponentially from center outward
    - **How it works**: Higher exponents create smaller bins near zero, larger bins for extreme values
    - **Mathematical**: Bin sizes increase exponentially based on distance from zero
    - **Example**: `exponent=2.0` creates exponential growth, `exponent=1.0` creates linear spacing
    - **Range**: > 0 (typical values: 1.0-3.0)
- **Examples**:
  ```yaml
  # Create 6 bins for price changes: 3 negative + zero + 3 positive
  # Results in labels like: "large_decrease", "small_decrease", "flat", "small_increase", "large_increase"
  - function: bin_numeric_data
    args: {num_bins: 3, outlier_percentile: 10}
  ```
- **Vocabulary Impact**: Reduces any numeric range to fixed number of categorical labels

#### Data Transformation

**`convert_to_percent_changes`**: Convert to percentage changes
- **Purpose**: Focus on relative movement rather than absolute values
- **Benefit**: Often more meaningful for prediction tasks than raw prices
- **Parameters**:
  - `decimal_places` (int or null, default=2): Number of decimal places for percentage values
- **How it works**: Calculates `(current - previous) / previous x 100` for each adjacent pair
- **Examples**:
  ```yaml
  # Convert prices to percentage changes with 2 decimal places
  # [100, 105, 102] -> [0, 5.00, -2.86]
  - function: convert_to_percent_changes
    args: {decimal_places: 2}

  # Higher precision for small movements
  - function: convert_to_percent_changes
    args: {decimal_places: 4}
  ```
- **Output**: Same-length list with first element as 0 (no previous value to compare)
- **Trading Value**: Normalizes different price scales, focuses on movement patterns


### External Functions

Create custom processing functions in separate Python files for advanced data transformations beyond the built-in functions.

#### When to Use External Functions
- **Technical Indicators**: RSI, MACD, Bollinger Bands, Stochastic Oscillator, etc.
- **Custom Financial Metrics**: Sector-specific ratios, proprietary indicators
- **Data Preprocessing**: Outlier detection, noise filtering, data cleaning
- **Feature Engineering**: Moving averages, momentum indicators, volatility measures
- **Domain-Specific Transformations**: Industry-specific calculations

#### Function Requirements
All external functions must follow this signature:
```python
def your_function_name(data, **kwargs):
    """
    Args:
        data: List of values from the specified column
        **kwargs: Any arguments specified in YAML configuration

    Returns:
        List of processed values (same length as input)
    """
    # Your processing logic here
    return processed_data
```

#### Implementation Examples

**Technical Indicators:**
```python
# technical_indicators.py
import pandas as pd
import numpy as np

def calculate_rsi(data, **kwargs):
    """Calculate Relative Strength Index"""
    period = kwargs.get('period', 14)

    # Convert to pandas Series for easier calculation
    series = pd.Series(data)
    delta = series.diff()

    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)

    avg_gain = gain.rolling(window=period).mean()
    avg_loss = loss.rolling(window=period).mean()

    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))

    # Fill NaN values with neutral RSI value
    rsi = rsi.fillna(50)

    return rsi.tolist()

def bollinger_bands(data, **kwargs):
    """Calculate Bollinger Bands upper band"""
    window = kwargs.get('window', 20)
    std_dev = kwargs.get('std_dev', 2)

    series = pd.Series(data)
    rolling_mean = series.rolling(window=window).mean()
    rolling_std = series.rolling(window=window).std()

    upper_band = rolling_mean + (rolling_std * std_dev)

    # Fill NaN values with original data
    upper_band = upper_band.fillna(series)

    return upper_band.tolist()

def macd_signal(data, **kwargs):
    """Calculate MACD signal line"""
    fast_period = kwargs.get('fast_period', 12)
    slow_period = kwargs.get('slow_period', 26)
    signal_period = kwargs.get('signal_period', 9)

    series = pd.Series(data)

    # Calculate EMAs
    ema_fast = series.ewm(span=fast_period).mean()
    ema_slow = series.ewm(span=slow_period).mean()

    # MACD line
    macd_line = ema_fast - ema_slow

    # Signal line
    signal_line = macd_line.ewm(span=signal_period).mean()

    return signal_line.fillna(0).tolist()
```

**Data Preprocessing:**
```python
# data_preprocessing.py
def remove_outliers(data, **kwargs):
    """Remove outliers using IQR method"""
    method = kwargs.get('method', 'iqr')
    factor = kwargs.get('factor', 1.5)

    if method == 'iqr':
        q1 = np.percentile(data, 25)
        q3 = np.percentile(data, 75)
        iqr = q3 - q1

        lower_bound = q1 - factor * iqr
        upper_bound = q3 + factor * iqr

        # Cap outliers instead of removing (maintains list length)
        cleaned_data = [max(min(x, upper_bound), lower_bound) for x in data]
        return cleaned_data

    return data

def smooth_data(data, **kwargs):
    """Apply moving average smoothing"""
    window = kwargs.get('window', 5)

    smoothed = []
    for i in range(len(data)):
        if i < window:
            # For early values, use available data
            smoothed.append(np.mean(data[:i+1]))
        else:
            smoothed.append(np.mean(data[i-window+1:i+1]))

    return smoothed
```

#### YAML Configuration Examples

```yaml
modalities:
  - modality_name: "RSI Indicator"
    path: "./data/stocks/"
    column_number: 4  # Close price column
    processing_steps:
      # First calculate RSI from prices
      - function: technical_indicators.calculate_rsi
        args: {period: 14}
      # Then bin the RSI values
      - function: bin_numeric_data
        args: {num_bins: 5}
    cross_attention: true

  - modality_name: "Bollinger Upper Band"
    path: "./data/stocks/"
    column_number: 4
    processing_steps:
      - function: technical_indicators.bollinger_bands
        args: {window: 20, std_dev: 2}
      - function: range_numeric_data
        args: {num_whole_digits: 3, decimal_places: 2}
    cross_attention: true

  - modality_name: "Cleaned Volume"
    path: "./data/stocks/"
    column_number: 6  # Volume column
    processing_steps:
      # Remove volume spikes (outliers)
      - function: data_preprocessing.remove_outliers
        args: {method: "iqr", factor: 2.0}
      # Smooth the data
      - function: data_preprocessing.smooth_data
        args: {window: 3}
      # Bin for vocabulary control
      - function: bin_numeric_data
        args: {num_bins: 7}
    cross_attention: true
```

#### Best Practices

1. **Error Handling**: Always handle edge cases (NaN values, empty data, etc.)
2. **Length Preservation**: Return same-length lists to maintain sequence alignment
3. **Parameter Validation**: Validate kwargs parameters and provide defaults
4. **Documentation**: Include docstrings explaining what the function does
5. **Testing**: Test functions independently before using in pipeline
6. **Dependencies**: Keep external dependencies minimal (pandas, numpy are recommended)

#### Common Use Cases

- **Financial Indicators**: RSI, MACD, Bollinger Bands, Stochastic, Williams %R
- **Statistical Measures**: Volatility, skewness, correlation coefficients
- **Data Quality**: Outlier removal, gap filling, noise reduction
- **Feature Engineering**: Lag variables, rolling statistics, momentum calculations
- **Custom Metrics**: Proprietary trading signals, sector-specific ratios

## Model Architecture

### Multimodal Transformer
- **Pre-processing Block**: Separate embedding tables for each modality
- **Transformer Layers**: Standard attention mechanism with optional cross-attention
- **Selective Cross-Attention**: Learn relationships between different modalities (configurable per modality)
- **Post-processing Block**: Separate output heads for each modality

### Key Components
- **Multi-Head Attention**: Process relationships within modalities (self-attention)
- **Selective Cross-Attention**: Process relationships between modalities
  - **How it works**: Each modality can optionally attend to other modalities
  - **Flexibility**: Enable/disable per modality via `cross_attention: true/false`
  - **Use cases**:
    - Enable for price data to learn from volume and time patterns
    - Disable for pure time data that should remain independent
    - Enable for correlated assets (stock pairs, currency pairs)
  - **Performance**: Only compute cross-attention where beneficial, saving computational resources
  - **Why this matters**: Not all data types should influence each other (e.g., time should remain independent) - selective attention focuses model learning on meaningful relationships while avoiding computational waste on irrelevant connections
- **Feed-Forward Networks**: Non-linear transformations
- **Layer Normalization**: Stabilize training
- **Dropout**: Prevent overfitting

## Training and Evaluation

### Training Process
1. **Data Loading**: Load and process data according to schemas
2. **Vocabulary Creation**: Build vocabularies for each modality
3. **Model Creation**: Initialize transformer architecture
4. **Training Loop**: Gradient descent with evaluation intervals and data augmentation
5. **Model Saving**: Save checkpoints and final model

#### Data Augmentation During Training

**Randomness Injection**: Applied only during training to create synthetic variations
- **Purpose**: Overcomes scarcity of financial market data by creating slight variations of existing patterns
- **Configuration**: Set `randomness_size` (1-3 or null) in input schema
- **How it Works**: Adds small random variations to numerical values during training
  - **Example**: Original price $152.34 with `randomness_size=2` becomes randomly:
    - $152.32 (original - 2 cents)
    - $152.33 (original - 1 cent)
    - $152.34 (original value)
    - $152.35 (original + 1 cent)
    - $152.36 (original + 2 cents)
- **Key Benefits**:
  - **Massive data expansion**: Creates astronomical variations (5^24 = 59 quadrillion for 24-token sequences with `randomness_size=2`)
  - **Training only**: Applied exclusively during training - validation uses original, unmodified data
  - **Pattern focus**: Helps model learn trends rather than memorize exact historical values
  - **Authentic behavior**: Never invents new market scenarios, only varies existing patterns
- **Important**: Should NOT be used on time data, categorical data, or data where exact values have semantic meaning
- **Good for**: Price data, volume data, technical indicators

*For mathematical details and implementation specifics, see `TECHNICAL_NOTES.md`*

### Evaluation Metrics

#### Standard Training Metrics

**Multimodal Loss Calculation:**

The system combines learning from all data types by summing individual modality losses into a single total loss that drives unified optimization.

**Benefits:**
- **Holistic Learning**: Model learns relationships between all data types simultaneously
- **Cross-Modal Knowledge Transfer**: Insights from one modality improve predictions in others
- **Unified Optimization**: Single loss function drives learning across the entire multimodal system

*For detailed technical implementation, see `TECHNICAL_NOTES.md`*

- **Per-Modality Analysis**: Individual modality performance breakdown during evaluation

#### Directional Success Analysis
The system includes sophisticated **directional prediction evaluation** specifically designed for financial applications.

**IMPORTANT:** This analysis is **only performed on numeric modalities** (prices, indicators, volumes). Categorical data (time, days, text labels) is excluded from directional analysis.

**What is Directional Success?**
- Instead of requiring exact price predictions, measures whether the model correctly predicts **movement direction**
- Evaluates predictions as: **Up** (positive movement), **Down** (negative movement), or **Flat** (no significant change)
- Much more practical for trading applications than exact price matching

**How it Works:**
- **For Percentage Data**: Direction determined directly from sign of percentage change
  - Positive % = Up movement
  - Negative % = Down movement
  - Zero % = Flat movement
- **For Price Data**: Direction calculated by comparing current vs. previous values
  - Current > Previous = Up movement
  - Current < Previous = Down movement
  - Current = Previous = Flat movement

**Metrics Provided:**

1. **Directional Success Rate**: Percentage of correct direction predictions
   - **Calculation**: `Total Wins / (Total Wins + Total Losses) x 100`
   - **Win**: Predicted direction matches actual direction (both up, both down, or both flat)
   - **Loss**: Predicted direction differs from actual direction
   - **Focus**: Last token prediction in each sequence (most relevant for trading)

2. **Directional Certainty**: Model confidence in predicted direction
   - **How calculated**: Sums probabilities of all vocabulary tokens that align with the predicted direction
   - **Example**: If model predicts "up" movement, certainty = sum of probabilities for all positive values
   - **Range**: 0.0 to 1.0 (higher = more confident in direction)
   - **Purpose**: Measures model's conviction, not just accuracy

3. **Win/Loss Counts**: Detailed breakdown of correct vs. incorrect direction calls
4. **Per-Modality Analysis**: Separate directional metrics for each data type

**Why Important for Trading:**
- **Practical Relevance**: Knowing direction is often more valuable than exact prices
- **Trading Signals**: Can directly translate to buy/sell/hold decisions
- **Risk Management**: Helps assess model reliability for different market conditions
- **Model Validation**: Ensures model learns meaningful market patterns, not just memorizes prices
- **Why this matters**: Traditional ML metrics focus on exact prediction accuracy, but traders need directional accuracy - this metric directly measures what matters most for profitable trading decisions

**Example Output:**
```
Directional Prediction Analysis (20 iterations Ã— 8 batches = 160 samples per evaluation)
   Train Set - Stock Prices: Correct=93 | Incorrect=67 | Accuracy=58.1%
   Val Set - Stock Prices: Correct=92 | Incorrect=68 | Accuracy=57.5%
```

**Key Features:**
- Analyzes last token prediction in each sequence (most relevant for forecasting)
- Only calculated for numeric modalities (prices, indicators, volumes)
- Provides realistic performance assessment for trading applications
- Certainty metric helps assess model reliability and confidence

*For detailed calculation methods and implementation specifics, see `TECHNICAL_NOTES.md`*

### Training Command
```bash
python main.py
```

Monitor training progress in the console and log files.

### Generation & Prediction (Future Integration)

The system includes a sophisticated `generate()` function capable of predicting future market movements:

#### Current Capabilities
- **Next Token Prediction**: Generate predictions for any specified modality
- **Multi-step Forecasting**: Generate multiple future time steps
- **Contextual Awareness**: Uses full context window from all modalities
- **Probabilistic Output**: Provides confidence distributions, not just point predictions

#### Implementation Status
**Note**: The generation functionality exists in the codebase but is **not yet integrated** into the main training workflow.

#### Future Integration Roadmap
**Planned Enhancement**: The system is designed to be integrated with **live broker APIs** for real-time market prediction:

**Live Trading Integration:**
1. **Real-time Data Feed**: Connect to broker APIs (Interactive Brokers, TD Ameritrade, Alpaca, etc.)
2. **Near-past Context**: Automatically fetch recent market data as model input
3. **Live Predictions**: Generate real-time predictions for:
   - **Price Movements**: Next price levels or ranges
   - **Direction Forecasting**: Up/Down/Flat movement predictions
   - **Confidence Metrics**: Model certainty in predictions
4. **Multi-timeframe Analysis**: Simultaneous predictions across different timeframes

**Use Cases:**
- **Automated Trading Signals**: Generate buy/sell/hold recommendations
- **Risk Management**: Predict volatility and adjust position sizing
- **Portfolio Optimization**: Multi-asset correlation predictions
- **Market Timing**: Entry and exit point optimization

**Technical Implementation:**
```python
# Future API integration example
predicted_sequences = model.generate(
    idx_list=live_market_data,  # Real-time data from broker API
    max_new_tokens=5,           # Predict next 5 time periods
    modality_to_generate=0      # Generate for primary asset
)
```

This integration will transform the system from a research tool into a production-ready trading system capable of making real-time market predictions based on live data streams.

## Troubleshooting

### Common Issues

**Configuration Errors:**
- `Path does not exist`: Check file/folder paths in YAML
- `Column must be positive`: Column numbers start from 1, not 0
- `Function cannot be resolved`: Check function name spelling

**Memory Issues:**
- Reduce `batch_size` or `block_size` in config.yaml
- Use `device: cpu` if GPU memory is insufficient

**Training Issues:**
- `Loss not decreasing`: Try different learning rate
- `NaN losses`: Reduce learning rate
- `Slow training`: Increase batch_size (if memory allows)

**External Function Errors:**
- `Module not found`: Ensure custom modules are in Python path
- `Function signature error`: Check function accepts `data` and `**kwargs`

### Getting Help

1. **Check YAML files**: Extensive comments and examples included
2. **Review logs**: Detailed error messages in console output
3. **Validate configuration**: System validates on startup
4. **Test functions separately**: Test custom functions before adding to pipeline

## Advanced Usage

### Custom Processing Pipelines
```yaml
processing_steps:
  # Data preprocessing
  - function: my_preprocessing.remove_outliers
    args: {method: "iqr", factor: 1.5}
    enabled: true

  # Feature engineering (temporarily disabled for testing)
  - function: my_features.technical_indicators
    args: {indicators: ["rsi", "macd", "bollinger"]}
    enabled: false  # Skip this step without deleting configuration

  # Final scaling
  - function: range_numeric_data
    args: {num_whole_digits: 3, decimal_places: 2}
    # enabled defaults to true when omitted
```

**Step Control Options:**
- `enabled: true` - Step executes normally (default if omitted)
- `enabled: false` - Step is skipped, data flows to next enabled step

**Use Cases for `enabled: false`:**
- Temporarily disable steps for debugging or testing
- A/B test different processing configurations
- Keep alternative processing options readily available

### Example Modality Configurations

The system was designed with stock movement prediction in mind but can be adapted for any time-series data. Here are practical examples of how to set up different types of multimodal configurations:

#### Financial Trading Examples

**1. S&P 500 Comprehensive Analysis**
```yaml
modalities:
  - modality_name: "S&P 500 Close Prices"
    path: "./data/sp500_daily.csv"
    column_number: 4
    processing_steps:
      - function: range_numeric_data
        args: {num_whole_digits: 3, decimal_places: 2}
    cross_attention: true

  - modality_name: "Price Changes (%)"
    path: "./data/sp500_daily.csv"
    column_number: 4
    processing_steps:
      - function: convert_to_percent_changes
      - function: bin_numeric_data
        args: {num_bins: 7}
    cross_attention: true

  - modality_name: "50-day Moving Average"
    path: "./data/sp500_with_indicators.csv"
    column_number: 5
    cross_attention: true

  - modality_name: "200-day Moving Average"
    path: "./data/sp500_with_indicators.csv"
    column_number: 6
    cross_attention: true

  - modality_name: "Bollinger Upper Band"
    path: "./data/sp500_with_indicators.csv"
    column_number: 7
    cross_attention: true

  - modality_name: "Bollinger Lower Band"
    path: "./data/sp500_with_indicators.csv"
    column_number: 8
    cross_attention: true
```

**2. Intraday MSFT Trading Setup**
```yaml
modalities:
  - modality_name: "MSFT Open"
    path: "./data/msft_10min/"
    column_number: 2
    processing_steps:
      - function: range_numeric_data
        args: {num_whole_digits: 2, decimal_places: 2}
    cross_attention: true

  - modality_name: "MSFT High"
    path: "./data/msft_10min/"
    column_number: 3
    cross_attention: true

  - modality_name: "MSFT Low"
    path: "./data/msft_10min/"
    column_number: 4
    cross_attention: true

  - modality_name: "MSFT Close"
    path: "./data/msft_10min/"
    column_number: 5
    cross_attention: true

  - modality_name: "Volume"
    path: "./data/msft_10min/"
    column_number: 6
    processing_steps:
      - function: bin_numeric_data
        args: {num_bins: 5}
    cross_attention: true

  - modality_name: "VIX (Fear Index)"
    path: "./data/market_indicators.csv"
    column_number: 2
    cross_attention: true

  - modality_name: "Time of Day"
    path: "./data/msft_10min/"
    column_number: 7
    cross_attention: false  # Time should remain independent

  - modality_name: "Day of Week"
    path: "./data/msft_10min/"
    column_number: 8
    cross_attention: false  # Day patterns should remain independent

  - modality_name: "Day of Month"
    path: "./data/msft_10min/"
    column_number: 9
    cross_attention: false
```

**3. Semiconductor Sector Analysis**
```yaml
modalities:
  - modality_name: "Semiconductor Stocks"
    path: "./data/semiconductor_sector/"  # Folder with multiple stock files
    column_number: 4  # Close price column
    processing_steps:
      - function: convert_to_percent_changes
      - function: range_numeric_data
        args: {num_whole_digits: 2, decimal_places: 2}
    cross_attention: true
```

**4. Commodities & Currency Pairs**
```yaml
modalities:
  - modality_name: "Oil ETF (USO)"
    path: "./data/commodities/uso_daily.csv"
    column_number: 4
    processing_steps:
      - function: range_numeric_data
        args: {num_whole_digits: 2, decimal_places: 2}
    cross_attention: true

  - modality_name: "USD Index Futures"
    path: "./data/forex/usdx_daily.csv"
    column_number: 4
    processing_steps:
      - function: range_numeric_data
        args: {num_whole_digits: 3, decimal_places: 2}
    cross_attention: true
```

**5. Pairs Trading Strategy**
```yaml
modalities:
  - modality_name: "Coca-Cola (KO)"
    path: "./data/pairs_trading/ko_daily.csv"
    column_number: 4
    processing_steps:
      - function: convert_to_percent_changes
      - function: add_rand_to_data_points
        args: {rand_size: 1}
    cross_attention: true

  - modality_name: "PepsiCo (PEP)"
    path: "./data/pairs_trading/pep_daily.csv"
    column_number: 4
    processing_steps:
      - function: convert_to_percent_changes
      - function: add_rand_to_data_points
        args: {rand_size: 1}
    cross_attention: true
```

### Multiple Data Sources for Other Domains
```yaml
modalities:
  # Can be adapted for other time-series domains
  - modality_name: "Temperature Readings"
    path: "./data/weather/"
    column_number: 3
    processing_steps:
      - function: range_numeric_data
        args: {num_whole_digits: 2, decimal_places: 1}

  - modality_name: "Sensor Values"
    path: "./data/sensors/"
    column_number: 1
    processing_steps:
      - function: bin_numeric_data
        args: {num_bins: 10}
```

### Hyperparameter Tuning
Experiment with different configurations by editing `config.yaml`:
- **Model size**: Adjust `n_embd`, `n_head`, `n_layer`
- **Training**: Modify `batch_size`, `learning_rate`, `max_iters`
- **Regularization**: Tune `dropout` values
- **Context**: Change `block_size` for different sequence lengths

## Contributing

### Code Organization
- Keep functions in appropriate modules
- Add comprehensive documentation
- Include error handling and validation
- Follow existing code patterns

### Adding Processing Functions
1. **Built-in functions**: Add to `data_utils.py` and `processing_registry.py`
2. **External functions**: Create separate modules with standard signature
3. **Documentation**: Update YAML comments and this README

## License

This project is released as open source under the MIT License. You are free to use, modify, and distribute this software.

## Acknowledgments

Built with PyTorch and designed for multimodal time series analysis. Special attention to seamless configuration compatibility and user-friendly setup.

---

**Ready to start training multimodal transformers!**

For questions or issues, check the troubleshooting section or review the extensive documentation in the YAML configuration files.