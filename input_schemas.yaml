# =============================================================================
# INPUT SCHEMAS CONFIGURATION
# =============================================================================
#
# This file defines the modality configurations for the multimodal transformer system.
# Each modality represents a different type of data (e.g., stock prices, time, volume)
# that will be processed and fed into the transformer model.
#
# YAML STRUCTURE:
# ---------------
# modalities:         # Root key containing list of all modality configurations
#   - modality_name:  # Required: Descriptive name for this modality
#                     # Expected values: string
#     path:           # Required: Path to data file or folder containing data files
#                     # Expected values: string
#     column_number:  # Required: Column number to extract (1-based indexing)
#                     # Expected values: positive integer
#     has_header:     # Required: Whether the data files have header rows (true/false)
#     processing_steps: # Optional: List of sequential data processing functions
#                     # Expected values: list
#       - function:   # Function name (built-in) or fully qualified name (external)
#                     # Expected values: string
#         args:       # Dictionary of arguments to pass to the function
#                     # Expected values: dict
#         enabled:    # Optional: Whether this step is enabled (allows temporary disabling)
#                     # Expected values: true/false (default: true)
#                     # When false: Step is completely skipped, data flows to next enabled step
#     randomness_size: # Optional: Add randomness to data points (1-3 or null)
#     cross_attention: # Optional: Enable cross-attention for this modality
#                     # Expected values: true/false (default: false)
#
# PROCESSING FUNCTIONS:
# --------------------
# Built-in functions (use simple names):
#   - range_numeric_data: Scale numeric values and set decimal places
#   - bin_numeric_data: Group numeric data into bins/clusters
#   - convert_to_percent_changes: Convert values to percentage changes
#   - add_rand_to_data_points: Add randomness to data values
#
# External functions (use fully qualified names):
#   - my_module.custom_function: Import and use external processing functions
#   - sklearn.preprocessing.StandardScaler.fit_transform: Use third-party libraries
#
# DATA FLOW:
# ----------
# Data flows sequentially through processing steps:
# raw_data → function1 → function2 → function3 → final_processed_data
# Each function receives the output of the previous function.
#
# STEP CONTROL:
# - enabled: true  → Step executes normally
# - enabled: false → Step is skipped, data passes through unchanged
# - omitted        → Defaults to true (step executes)
#
# Use cases for enabled: false:
# - Temporarily disable a step without deleting configuration
# - A/B testing different processing pipelines
# - Debugging by isolating specific processing steps
#
# EXAMPLES AND GUIDELINES:
# -----------------------

# =============================================================================
# EXAMPLE CONFIGURATIONS
# =============================================================================

modalities:
  # -------------------------------------------------------------------------
  # EXAMPLE 1: Stock price data with ranging and scaling
  # -------------------------------------------------------------------------
  - modality_name: "NASDAQ Stock Prices"  # Expected values: string
    path: "./data/10min_candles/"  # Expected values: string
    column_number: 13  # Close price column
                       # Expected values: positive integer
    has_header: true  # Expected values: true/false
    processing_steps:
      # Scale prices to 2 whole digits with 1 decimal place (10.0 to 99.9)
      - function: range_numeric_data
        args:
          num_whole_digits: 2    # Number of digits before decimal
                                 # Expected values: positive integer or null
          decimal_places: 1      # Number of digits after decimal
                               # Expected values: non-negative integer or null
        enabled: true
    randomness_size: 2  # Add randomness with size 2
                           # Expected values: 1-3 or null
    cross_attention: true  # Enable cross-attention with other modalities
                           # Expected values: true/false (default: false)

  # -------------------------------------------------------------------------
  # EXAMPLE 2: Stock price data converted to percentages and binned
  # -------------------------------------------------------------------------
  - modality_name: "NASDAQ Percentage Changes"
    path: "./data/10min_candles/"
    column_number: 13  # Same data, different processing
    has_header: true
    processing_steps:
      # Step 1: Convert to percentage changes
      - function: convert_to_percent_changes
        args: {}  # No additional arguments needed
        enabled: true

      # Step 2: Group percentages into 6 bins
      - function: bin_numeric_data
        args:
          num_bins: 6              # Number of positive bins (creates +n, -n, and zero bins)
                                   # Expected values: positive integer
          outlier_percentile: 0.1  # Percentile threshold for outlier exclusion from binning
                                   # Expected values: float, 0.0-1.0
          exponent: 2.2           # Controls bin distribution (higher = more non-uniform)
                                   # Expected values: float
        enabled: true
    randomness_size: null
    cross_attention: false  # Disable cross-attention for this modality

  # -------------------------------------------------------------------------
  # EXAMPLE 3: Time data (no processing needed)
  # -------------------------------------------------------------------------
#  - modality_name: "Trade Times"
#    path: "./data/10min_candles/"
#    column_number: 5   # Time column
#    has_header: true
#    processing_steps: []  # No processing steps - use raw time data
#    randomness_size: null
#    cross_attention: false

  # -------------------------------------------------------------------------
  # EXAMPLE 4: Custom external processing function
  # -------------------------------------------------------------------------
  # - modality_name: "Custom Processed Data"
  #   path: "/path/to/your/data.csv"
  #   column_number: 1
  #   has_header: true
  #   processing_steps:
  #     # Use external function from custom module
  #     - function: my_custom_processing.advanced_transform
  #       args:
  #         multiplier: 2.5
  #         method: "exponential"
  #       enabled: true
  #
  #     # Chain with built-in function
  #     - function: range_numeric_data
  #       args:
  #         num_whole_digits: 3
  #         decimal_places: 2
  #       enabled: true
  #   randomness_size: 2  # Add randomness with size 2
  #   cross_attention: true

# =============================================================================
# FUNCTION REFERENCE
# =============================================================================

# BUILT-IN FUNCTIONS:
# ------------------
#
# 1. range_numeric_data:
#    Purpose: Scale numeric values to specified range and decimal places
#    Arguments:
#      - num_whole_digits: Number of digits before decimal
#                           Expected values: positive integer or null
#      - decimal_places: Number of digits after decimal
#                         Expected values: non-negative integer or null
#    Example: Scales prices from various ranges to 10.0-99.9 format
#
# 2. bin_numeric_data:
#    Purpose: Group numeric data into discrete bins/clusters
#    Arguments:
#      - num_bins: Number of bins to create
#                  Expected values: positive integer
#      - outlier_percentile: Percentage of extreme values to exclude
#                             Expected values: float, 0.0-1.0
#      - exponent: Controls bin distribution (1.0=uniform, >1.0=non-uniform)
#                  Expected values: float
#    Example: Groups percentage changes into 6 categories
#
# 3. convert_to_percent_changes:
#    Purpose: Convert values to percentage changes from previous values
#    Arguments: None
#    Example: [100, 102, 99] → [0, 2.0, -2.94]
#
# 4. add_rand_to_data_points:
#    Purpose: Add controlled randomness to data values
#    Arguments:
#      - rand_size: Size/magnitude of randomness to add
#                   Expected values: integer, 1-3
#    Example: Adds small random variations for data augmentation
#
# EXTERNAL FUNCTIONS:
# ------------------
# External functions must be specified with fully qualified names:
# "module_name.function_name" or "package.module.function_name"
#
# Requirements for external functions:
# - Must accept data as first parameter
# - Must accept additional arguments via **kwargs
# - Must return processed data in same format
# - Must preserve data length (same number of elements as input)
# - Function signature: def my_function(data, **kwargs): return processed_data
#
# Examples:
# - "my_processing.normalize_data"
# - "sklearn.preprocessing.StandardScaler.fit_transform"
# - "custom_indicators.calculate_rsi"

# =============================================================================
# TROUBLESHOOTING
# =============================================================================
#
# Common Issues:
# 1. "Path does not exist" - Check that file/folder paths are correct
# 2. "Column must be positive" - Column numbers start from 1, not 0
# 3. "Function cannot be resolved" - Check spelling of function names
# 4. "External function not found" - Ensure module is in Python path
#
# Tips:
# - Use forward slashes (/) in paths, even on Windows
# - Test external functions separately before adding to pipeline
# - Start with simple configurations and add complexity gradually
# - Check log files for detailed error messages during execution