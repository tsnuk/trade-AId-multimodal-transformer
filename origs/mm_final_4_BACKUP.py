# -*- coding: utf-8 -*-
"""mm_final_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uqhLTzOI84LWUQ_Br4vVWfkAfr4I43Ny

# Imports
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

import pandas as pd
import numpy as np
import numbers
import math

import os
from datetime import datetime
import random
from pathlib import Path

# Removed Google Colab dependencies for local PyCharm execution

"""# Hyperparams"""

batch_size = 8#32   # 64 # 32 # 128 # Batch Size: Helps in parallelism by utilising the the multiple cores of the GPU simultaneously for independent processing
                  # this determines how many training examples are processed together in a single batc
                  # batch_size is the number of sequences processed in parallel.
block_size = 6   # EXPLAIN HOW input_schemas AFFECT block_size
                  # (prev 48) 64 # 512 # Block Size: Context window to pick training samples from
                  # this is the number of previous tokens the model will consider when predicting the next one, ie the length of the input sequence
                  # block_size is the length of each sequence.
max_iters = 20000 #5000 # 400 # 25000 # max_iters: The maximum epochs used for training
                    # this is the maximum number of training iterations (epochs) the model will run for. Training stops when this limit is reached
eval_interval = 50#100   # 50 #400 # 100 # 1000 # eval_interval: The interval after which loss is to be estimated during training
                      # during training, the code will evaluate the model's performance (calculate loss) after every eval_interval iterations
learning_rate = 3e-4  # learning_rate: The magnitude by which we want to update our model weights
                      # controls the step size the model takes when updating its weights during training. Smaller values lead to slower but potentially more stable training
device = 'cuda' if torch.cuda.is_available() else 'cpu'   # device: Allows for the usage of GPU, if available
                                                          # cuda is a parallel computing platform by nvdia that allows usage of nvdia GPUs
eval_iters = 40  # (prev 20) #100  # 20 # 250 # eval_iters: Used to estimate loss, determines the number of batches of data to select (X), predictions to make (Y') and then evaluate with actual values (Y).The loss is calculated based on this Y' and Y
                 # when evaluating the model's performance, the code will use eval_iters batches of data to calculate the average loss. This gives a more robust estimate of performance
n_embd = 16#64  #16#32#64#128  # (prev 256) could be 1/20th, or even smaller, of the vocab size. 64 # 512 # n_embd: The size of the embedding, converts the OHE representation of the character into a vec of n_embd dimensions
              # this is the dimensionality of the word embeddings used in the model. Embeddings are vector representations of words or characters that capture their meaning
              # this means that each character will be represented by a 256-dimensional vector, where each element of the vector can be thought of as a "feature" or an aspect of the word/character's meaning
              # larger embedding dimensions can potentially represent more complex relationships but require more memory
n_head = 4#8  #4#16#8    # (prev 32) use one of: 8 / 12 / 16  ### This is the number of attention heads used in the multi-head attention mechanism of the Transformer model
n_layer = 4#6  #8#4#16#8   # (prev 32) use one of: 6 / 12 (or even 4) ### This determines the number of layers (blocks) in the Transformer model. Deeper models can learn more complex patterns but might be harder to train
dropout = 0.2   # dropout: Values between 0 and 1 represent the probability of keeping a neuron's output during training
                # this is a regularization technique used to prevent overfitting
                # it randomly drops out (sets to zero) a fraction of the neuron activations during training, forcing the model to be more robust and less reliant on specific neurons

project_file_path = 'C:/Users/tsalo/gpt_proj_6/multimodal_1/'

# Path for saving and loading the model weights
model_file_name = project_file_path + 'output/' + 'TransformerModel.pth'

# this is the file where training data and the model's parameters will be saved. make sure the path to it (project_file_path) is correct
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_file_name = f'output_run_{timestamp}.txt'


# validation_size specifies the portion of data (1 being the whole data) that will be allocated to the validation set,
# (the rest will be allocated to the train set). Value must be set within 0.1-0.3.
# validation_size is only used if num_val_files (below) is 0.
validation_size = 0.1

# An alternative method for specifying/allocating loaded data for validation is using 'num_validation_files'.
# When specifying to load data from a folder (as opposed to a file),
# the user has the option to set a number of the files within that folder to be used for validation.
# These files will be used for validation in their entirity, and not be included in the training set.
# For that, the user should set num_validation_files to a number other than 0, in which case,
# the last uploaded n files in the folder will be used for this porpuse (this will override 'validation_size').
# This will apply only to the 1sy modality  --EXPLAIN THIS BETTERRRRRR (SEE COMMENT IN get_batch FUNC)
# MAKE SURE TO MAKE IT VERY CLEAR THAT num_validation_files WILL BE APPLIED ON THE 1ST MODALITY ONLY !!!!
num_validation_files = 0


# --- Model Creation and Loading ---
#
# The 'create_new_model' variable controls whether
# a new model is created or a previously saved one is loaded.
# Set create_new_model = 1 to create a new model and start training from scratch.
# Set create_new_model = 0 to attempt to load a model from model_file_name.
#
# The 'model_file_name' variable ####DEFINE LOCATION BETTER(defined in a settings cell)#### specifies the path
# to save the model to, or load the model from. Ensure this path is correct.
#
# IMPORTANT CONSIDERATION WHEN LOADING A MODEL (create_new_model = 0):
# The code assumes that the data loading and processing steps executed
# BEFORE attempting to load the model generate the *same* vocabulary
# and that the hyperparameters (like n_embd, n_head, n_layer, block_size,
# num_modalities) match those of the saved model.
# If the data, vocabulary, or hyperparameters change between saving and loading,
# the loaded model might not work correctly with the current data or evaluation
# logic and could produce nonsensical results.

# --- Model Saving ---
#
# The 'save_model' variable ####DEFINE LOCATION BETTER(defined in a settings cell)#### controls whether
# the model's parameters are saved during and after training.
# Set save_model = 1 to save the model periodically during training (at eval_interval)
# and at the end of training.
# Set save_model = 0 to disable model saving for this training run.
#
# When save_model = 1, the model will be saved to the path specified by
# 'model_file_name'.

create_new_model = 0
save_model = 1
####DEFINE LOCATION BETTER(defined in a settings cell) - ALSO SIMILAR IN THE CODE FURTHER DOWN !!!!!!!

"""# Input Data"""

'''

                                     -------  I N P U T   D A T A  -------


  'input_schema' is a python list in which the user specifies the location from which to retreive data- path and column number within the data file,
  along with additional info regarding the data (described below), in order for it to be trained on by the system.
  Each input_schema contains/details information designed to construct one modality, and the system is designed to handle
  and process multiple modalities applying selective cross-attention in order to find connections and patterns between modalities.


  Notes:
    1. 10 empty input_schemas are provided below, available to be filled out by you, and used by the system.
       You can leave any unused input_schema as is - an empty list.
       You can also add additional input_schemas beyond the ones provided, acording to your needs.
       (you may need to find a balance between too many and too few input_schemas, as overdoing it may compromise results).
    2. All input_schemas should be synchronized to the same dates and times (and idealy be of the same length),
       that is, first elements of all input_schemas would be of the same date and time, all second elements would be of the same date and time, etc.
    3. Please note that the variable 'num_input_schemas' must match the number of existing input_schema lists (both used and empty).
       If you add additional input_schema lists please make sure to modify num_input_schemas accordingly.
    4. Depending on the amount of data you provide (number and length of input_schemas), and its variability (affecting vocabulary size),
       you may want to tune transformer parameters such as n_embd, n_head, n_layer, etc.,
       which contribute to the model's ability to learn from data, in order to optimize model performance.
       (if you want to learn more about these transformer parameters, please refer to external sources).


  Data can consist of:
    - Historical price data of any traded financial assset, such as stocks, bonds, ETFs, forex, commodities, cryptocurrencies, derivatives, ...),
    - Statistical data analysis and technical indicators, such as percentage changes, moving averages, Bollinger Bands, MACD, VWAP, RSI, ...
    - Trade-related metrics, such as volume, VIX, open interest, ....
    - Date and time intervals coinciding with any of the parameters used above that could add context to the data, enhancing its training,
      such as time, day of week, day of month, ... (these can include non-numerical values as in days of week).


  You can choose to apply one or more of the available built in functionality for processing the data,
  such as percent changes, price ranging, and k-means clustering.
  Data quality- make sure the data is 'clean', meaning no missing or erroneous elements.


  Examples of input data sets you can implement:
    1. S&P500 1-day close prices, pct change, 50-day moving avg, 200-day moving avg, upper Bollinger Band, lower Bollinger Band
    2. MSFT 10-min open, high, low, close, volume, VIX, time, day of week, day of month
    3. Closing prices of stocks belonging to the semiconductor sector
    4. Oil ETF (USO), U.S. Dollar Index (USDX) futures
    5. Pairs trading: Coca-Cola (KO), PepsiCo (PEP)


  input_schema Elements:

  Loading Parameters:
  The following elements are required for loading data:
    1. Path to a data file or to a folder containing data files to upload. Files must be of file extensions .csv or .txt.
       If a file is specified (its extension must be included in the specified file name), then only that file will be uploaded.
       If a folder is specified, then all files with the specified file extensions within that folder will be uploaded.
       So for example, if a folder contains the files:
       "AAPL.csv", "AMZN.csv", "GOOGL.csv", "META.csv", "MSFT.csv", "NVDA.csv", "TSLA.csv",
       then all 7 files will be uploaded (magnificently).
       CONCATENATED INTO ONE DATA SET AND TREATED AS A SINGLE input_schema
       (while keeping track of the individual data files in order to later create sequence batches that do not cross from one data set to the next)
    2. The column number containing the data to upload.
       If a folder was specified in the prev element, then the same column number will be applied to all files in the folder.
    3. Does the data column have a header (True/False).
       If header is set to True, when reading the column, the first element of the column will be skipped, so make sure your header is only one row.
       If a folder was specified in the first element, then the header setting will apply to all files.

  Processing Parameters:
  The following elements are optional and are intended for processing the raw data:
  [ranging values and specifing the number of decimal places allow for controle over the number of unique elements, and therefore controle of the vocabulary size]
    4. Convert numeric data to percent changes (bool)
    5. Number of whole digits are ______. This should be applied to numeric data only, such as prices.
       So for example:
          num_whole_digits = 1 will scale prices to a range of 1.00 to 9.99
          num_whole_digits = 2 will scale prices to a range of 10.00 to 99.99
          num_whole_digits = 3 will scale prices to a range of 100.00 to 999.99
          and so on... (the number of decimal places is specified in the next element)
    6. Number of dec places should be applied to numeric data only, such as prices
    SINCE SPECIFYING number of whole digits AND number of dec places IS OPTIONAL, YOU CAN CHOOSE NOT TO SET THEM.
    NOTE: If you opt to specify only one of either number of whole digits or number of dec places, the system will assume number of whole digits is the one specified.
    IF YOUR INTENTION IS TO SPECIFY only number of dec places, THEN YOU MUST SET BOTH (IN ORDER FOR THE SYS KNOW'S WHICH IS WHICH), AND in this case YOU CAN SET number of whole digits TO ZERO FOR IT NOT TO BE APPLIED.
    7. Bin numeric data (int)
    8. Randomness size (int between 1-3, or None)
    9. Cross-attention status (bool)
    10. Modality name (str)
    ###################################################################################
    ############  ADD DESCRIPTION FOR rand size (IT SHOULD BE BETWEEN 1-3, OR None), cross-attention STATUS, convert to percents STATUS
    ############  GET RID OF THE WORDS "optional" AND "required"

  So, each input_schema would contain:
  input_schema_1 = [0: 'path to data file or folder'(str), 1: data column number(int), 2: column has header?(bool),
                   3: convert to percentages(bool), 4: number of whole digits(int), 5: number of dec places(int),
                   6: bin data(int), 7: randomness size(int), 8: cross-attention status(bool), 9: modality name(str)]

  Example:
  input_schema_1 = ['C:/Users/tsalo/gpt_proj_6/multimodal_1/data_1/tick_10m/', 5, True, False, 2, 2, None, 2, True, 'S&P 500 15 min close values']
  input_schema_2 = ['C:/Users/tsalo/gpt_proj_6/multimodal_1/data_1/tick_10m/', 5, True, False, None, None, None, None, False, 'Candle time']
  input_schema_3 = []
  input_schema_4 = []
  input_schema_5 = []
  input_schema_6 = []
  input_schema_7 = []
  input_schema_8 = []
  input_schema_9 = []
  input_schema_10 = []


  Considerations:

  1- Synchronized dates and times:
  If your different inputs are meant to be trained together at similar dates and times,
  then make sure all relevant data is synchronized to the same dates and times in the files they're uploaded from.

  2- Matching data lengths:
  Different inputs should be of the same length in order to train together along the entire set.
  !!!!!!!! THINK WHAT TO DO IF THEY ARE OF DIFFERENT LENGTHS  --> ALERT THE USER, CUT THE LONGER DATA TO MATCH THAT OF THE SHORTER (NOT IDEAL), OR EXTEND THE SHORT ONE (MAYBE DO THIS ONE)
  !!!!!!!! MAYBE DO BOTH THE 1ST AND LAST OPTIONS ABOVE

 Use as many input_schema lists as you need, in succession, starting from the 1st.
 You can leave unused lists as empty lists, or you can add additional ones in addition to the ones provided here below.
 num_input_schemas must match the number of input_schema lists below that are in use (exclude empty lists)

'''

################################## SEE IF I CAN FIND A WAY NO USING num_input_schemas  #####################################
#! SEE IF GEMINI CAN SUGGEST SOMETHING
# num_input_schemas is the number of 'input_schema's the code will look at for forming modalities.
# Make sure to update this variable to match the number of 'input_schema's in use.
num_input_schemas = 10

# ADD TO GITHUB A FEW OF STOCK PRICE FILES SO THAT PEOPLE CAN TRY RUNNING THIS CODE WITHOUT HAVING TO LOOK FOR DATA
# AND SPECIFY THE BELOW input_schemas ACCORDINGLY



# input_schema_n is a list containing 10 elements:
    # 1. Path to a data file or a folder containing data files. Files must have '.csv' or '.txt' extensions (str).
    # 2. Column number - The 1-based index of the column to extract data from (int).
    # 3. Header - Boolean indicating if the data column has a header row (bool).
    # 4. Percent changes - Boolean indicating if the data should be converted to percentage changes (bool or None).
    # 5. Range - Number of whole digits, used for ranging (int or None).
    # 6. Decimal places - Number of decimal places (int or None).
    # 7. Bins - Number bins, used for binning data (int or None).
    # 8. Randomness size, used for data augmentation (int or None).
    # 9. Cross-attention status, used for model configuration (bool or None).
    # 10. Modality name (str or None).

    # Elements:  [Path, Col Num, Header, Percent Changes, Num Whole Digits, Decimal Places, Bins, Rand Size, Cross-Attend, Modality Name]
    # Types:     [(str), (int), (bool), (bool or None), (int or None), (int or None), (int or None), (int or None), (bool or None), (str or None)]
# Updated for local Windows environment #
input_schema_1 = ['C:/Users/tsalo/gpt_proj_6/multimodal_1/data_1/tick_10m/', 13, True, False, 2, 1, None, None, True, '200 stocks']
input_schema_2 = ['C:/Users/tsalo/gpt_proj_6/multimodal_1/data_1/tick_10m/', 13, True, True, None, 2, 6, None, False, '200 stocks - percents']
input_schema_3 = ['C:/Users/tsalo/gpt_proj_6/multimodal_1/data_1/tick_10m/', 9, True, False, None, None, None, None, False, 'Time']
input_schema_4 = ['C:/Users/tsalo/gpt_proj_6/multimodal_1/data_1/tick_10m/', 5, True, False, None, None, None, None, False, 'Day of week']
input_schema_5 = []
input_schema_6 = []
input_schema_7 = []
input_schema_8 = []
input_schema_9 = []
input_schema_10 = []

"""# Library"""

def load_file_data(input_info):
    """
    Reads data from a specified file or folder and extracts data from a
    given column. This data will be used to form a single modality for the
    multimodal processing framework. Handles CSV and TXT formats with optional header,
    attempting both comma and semicolon delimiters.

    Optionally, the extracted numeric data can be converted into percentage changes.

    Args:
        input_info: A list containing 10 elements:
            1. Path to a data file or a folder containing data files. Files
               must have '.csv' or '.txt' extensions (str).
            2. The 1-based index of the column to extract data from (int).
            3. Boolean indicating if the data column has a header row (bool).
            4. Boolean indicating if the data should be converted to percentage changes (bool or None).
            5. Number of whole digits (int or None, for ranging - not used in this function).
            6. Number of decimal places (int or None - not used in this function).
            7. Bin data (int or None, for binning - not used in this function).
            8. Randomness size (int or None, for data augmentation - not used in this function).
            9. Cross-attention status (bool or None, for model configuration - not used in this function).
            10. Modality name (str or None - not used in this function).


    Returns:
        A tuple containing:
        - A list of the loaded data points (can be of various data types: numeric, string, ...).
          If 'convert_to_percentages' is True, this list will contain float percentage changes.
        - A list containing the names and lengths of the loaded files:
            [file1_name (str), file1_length (int), file2_name (str), file2_length (int), ...]

    Raises:
        TypeError: If input_info or its elements are not of the expected types.
        ValueError: If input_info is empty or does not contain exactly 10 elements,
                    if the data path is invalid or no supported files are found,
                    or if the specified column does not exist.
        RuntimeError: If an unexpected error occurs during file loading.
        ZeroDivisionError: If attempting to calculate percentage change with a zero value.
    """

    if not isinstance(input_info, list):
        raise TypeError("'input_info' must be a list.")
    # Validation to check for 10 elements
    if len(input_info) != 10:
        raise ValueError("'input_info' must contain 10 elements: Path, data column number, header status, convert to percentages status, num whole digits, num dec places, bin data, rand size, cross-attention status, modality name.")

    data_path = input_info[0]
    if not isinstance(data_path, str):
        raise TypeError(f"Element 1 (Path) of 'input_info' must be a string, but got {type(data_path).__name__}.")
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"Path '{data_path}' was not found.")

    num_data_column = input_info[1]
    if not isinstance(num_data_column, int):
        raise TypeError(f"Element 2 (data column number) of 'input_info' must be an integer, but got {type(num_data_column).__name__}.")
    if num_data_column < 1:
        raise ValueError("The specified data column number must be greater than or equal to 1.")

    has_header = input_info[2]
    if not isinstance(has_header, bool):
        raise TypeError(f"Element 3 (header status) of 'input_info' must be a boolean, but got {type(has_header).__name__}.")

    # Validate the convert_to_percentages flag
    convert_to_percentages = input_info[3]
    if not (isinstance(convert_to_percentages, bool) or convert_to_percentages is None):
        raise TypeError(f"Element 4 (convert to percentages) of 'input_info' must be a boolean or None, but got {type(convert_to_percentages).__name__}.")

    # Get modality name (element 10)
    modality_name = input_info[9]
    if not (isinstance(modality_name, str) or modality_name is None):
         raise TypeError(f"Element 10 (modality name) of 'input_info' must be a string or None, but got {type(modality_name).__name__}.")

    data_file_paths = []
    if os.path.isdir(data_path):
        # Path to a folder
        load_from = "folder"
        data_file_paths = [os.path.join(data_path, f) for f in os.listdir(data_path) if os.path.isfile(os.path.join(data_path, f)) and (f.endswith('.csv') or f.endswith('.txt'))]
        if not data_file_paths:
            raise ValueError(f"No CSV or TXT files found in folder '{data_path}'.")

    elif os.path.isfile(data_path):
        # Path to a file
        load_from = "file"
        if not (data_path.endswith('.csv') or data_path.endswith('.txt')):
            raise ValueError(f"The specified file '{data_path}' is not a CSV or TXT file.")
        data_file_paths.append(data_path)


    # Read the datafile/s
    loaded_data = []
    data_info = []

    # This will be used for rounding if data is specified to be converted to percentages (convert_to_percentages is true)
    num_dec_places = input_info[5]

    data_name_from_path = Path(data_path).name
    #prep = "for" if modality_name else "from"
    #print(f"  Loading data {prep}: '{modality_name if modality_name else data_name_from_path}'") # Print modality name if provided
    print(f"  Loading data from {load_from}: '{data_name_from_path}'") # Print modality name if provided


    for full_path in data_file_paths:
        filename = os.path.basename(full_path)
        df = pd.DataFrame() # Initialize empty DataFrame
        read_successful = False

        # Try reading with comma delimiter first
        try:
            df = pd.read_csv(full_path, delimiter=',', engine='python', header=None, skiprows=1 if has_header else 0)
            if not df.empty:
                read_successful = True
                print(f'  Successfully read file: {filename}')
        except (pd.errors.EmptyDataError, pd.errors.ParserError, Exception) as e:
            last_error = e # Store the last error

        # If not successful, try reading with semicolon delimiter
        if not read_successful:
            try:
                df = pd.read_csv(full_path, delimiter=';', engine='python', header=None, skiprows=1 if has_header else 0)
                if not df.empty:
                    read_successful = True
                    print(f'  Successfully read file: {filename}')
            except (pd.errors.EmptyDataError, pd.errors.ParserError, Exception) as e:
                last_error = e # Store the last error


        # If after trying both delimiters, the DataFrame is still empty or read was not successful
        if not read_successful or df.empty:
            error_message = f"Failed to load data from file '{filename}' after trying both comma and semicolon delimiters."
            if 'last_error' in locals(): # Check if an error was caught
                error_message += f" Last error: {last_error}"
            print(error_message)
            raise RuntimeError(error_message)


        if num_data_column > df.shape[1]:
            raise ValueError(f"The specified data column ({num_data_column}) does not exist in file '{filename}'. File has {df.shape[1]} columns.")

        column_data = df.iloc[:, num_data_column - 1]

        # Convert column data to a list
        column_data_list = column_data.tolist()

        # Check if convert_to_percentages is True before processing
        if convert_to_percentages is True:
             # Check if data is numeric before calculating percentages
             data_is_numeric = all(isinstance(item, numbers.Number) for item in column_data_list)
             if not data_is_numeric:
                  # Find and report the non-numeric element
                  print(f"\nError: Percentage calculation specified for Modality '{modality_name if modality_name else data_name_from_path}' from file '{filename}', but data is not entirely numeric.")
                  # Create temporary file_info for reporting the error location within this file
                  temp_file_info = [filename, len(column_data_list)]
                  # Call report_non_numeric_error to provide details and raise ValueError
                  report_non_numeric_error(column_data_list, temp_file_info, f"{modality_name if modality_name else data_name_from_path}")
                  # report_non_numeric_error raises ValueError, so the loop will stop

             else:
                  # Calculate percentage changes and extend the loaded_data list
                  try:
                      percentage_changes = calculate_percent_changes(column_data_list, num_dec_places)
                      loaded_data.extend(percentage_changes)
                      # Store the file name and length of the extracted data (percentage changes have same length)
                      data_info.append(filename)
                      data_info.append(len(percentage_changes)) # length of percentage changes is same as original column_data
                      print(f'  Successfully extracted data from column {num_data_column} of file: {filename}, data length:{len(percentage_changes)}')

                  except ZeroDivisionError as e:
                      # Catch and re-raise ZeroDivisionError with more context
                      raise ZeroDivisionError(f"Error processing file '{filename}': {e}") from e
                  except Exception as e:
                       # Catch other potential errors during percentage calculation
                       raise RuntimeError(f"An unexpected error occurred during percentage calculation for file '{filename}': {e}") from e

        else:
            # If not calculating percentages, just extend the loaded_data list
            loaded_data.extend(column_data_list)
            # Store the file name and length of the extracted data
            data_info.append(filename)
            data_info.append(len(column_data_list))
            print(f'  Successfully extracted data from column {num_data_column} of file: {filename}, data length:{len(column_data_list)}')


    if not loaded_data:
        raise ValueError(f"No data was successfully loaded from the path '{data_path}' with the specified criteria.")


    #prep = "for" if modality_name else "from"
    #print(f"\n  Data loading {prep} '{modality_name if modality_name else data_name_from_path}' complete!")
    print(f"\n\n  Data loading for Modality '{modality_name}' complete!\n")
    print(f"  Number of files loaded: {len(data_file_paths)}")
    print(f"  Total data length: {len(loaded_data)}")

    if convert_to_percentages is True:
        print(f"  + Data converted to percent changes")

    # Print vocabulary size (num of unique elements)
    vocabulary = list(set(loaded_data))
    print(f'  Vocabulary size (unique elements): {len(vocabulary)}')

    if len(loaded_data) >= 10:
        print('  Dataset first / last elements:\n', '', *loaded_data[:5], '...', *loaded_data[-5:])


    # Check whether loaded_data is numeric, and if so, print additional data
    all_numbers = True
    for i, data in enumerate(loaded_data):
        if not isinstance(data, numbers.Number):
            all_numbers = False
            break

    if all_numbers:
        print(f'  Min element: {min(loaded_data)}')
        print(f'  Max element: {max(loaded_data)}')


    return loaded_data, data_info

def report_non_numeric_error(data_list, file_info, this_modality):
    """
    Finds the first non-numeric element in a data list and raises a ValueError,
    reporting its location, including the file name and approximate element index within that file,
    as well as the element's value and type.

    Args:
        data_list: A list of data points.
        file_info: A list containing file names and their corresponding data lengths
                   in the format [file1_name, data1_length, file2_name, data2_length, ...].
        this_modality: An integer representing the 1-based index of the modality,
                       or a string representing the name of the modality.

    Raises:
        ValueError: If a non-numeric element is found in the data_list.
    """
    first_non_numeric_index = -1
    non_numeric_value = None
    non_numeric_type = None

    for idx, item in enumerate(data_list):
        if not isinstance(item, numbers.Number):
            first_non_numeric_index = idx
            non_numeric_value = item
            non_numeric_type = type(item).__name__
            break

    if first_non_numeric_index != -1:
        # Determine which file the non-numeric element came from
        current_total_length = 0
        file_name = "Unknown File"
        element_index_in_file = first_non_numeric_index

        # file_info is [file1_name, data1_length, file2_name, data2_length, ...]
        for f_idx in range(0, len(file_info), 2):
            current_file_name = file_info[f_idx]
            current_file_length = file_info[f_idx+1]
            if first_non_numeric_index < current_total_length + current_file_length:
                file_name = current_file_name
                element_index_in_file = first_non_numeric_index - current_total_length
                break
            current_total_length += current_file_length

        # Format the modality identifier for the error message
        modality_identifier = f"Modality {this_modality}" if isinstance(this_modality, int) else f"Modality '{this_modality}'"


        raise ValueError(
            f"Non-numeric data found in {modality_identifier} at overall index {first_non_numeric_index} "
            f"(approximately element {element_index_in_file} in file '{file_name}'). "
            f"Element value: '{non_numeric_value}', Element type: {non_numeric_type}. "
            "Data must be entirely numeric for ranging or decimal places processing."
        )
    # Note: If no non-numeric is found, the function will simply return without raising an error.

def numerical_representation(data_points):
  """
  Converts a list of data points (numeric or other types) into a numerical
  representation by mapping each unique element to an integer index.

  Args:
    data_points: A list of data points. Can be numeric or other types.

  Returns:
    A tuple containing:
    - A list of integers representing the numerical representation of the input data.
    - A list of the unique elements (vocabulary) sorted in ascending order.
  """

  # Create vocabulary of unique elements
  vocabulary = sorted(list(set(data_points)))

  # Map elements to indices
  data_mapping = {element: index for index, element in enumerate(vocabulary)}

  # Transform data_points to its numerical representation
  transformed_data = [data_mapping[element] for element in data_points]

  return transformed_data, vocabulary

def create_train_val_datasets(numeric_rep_data, val_size, num_val_files, file_lengths):
    """
    Splits a combined list of numerical data into training and validation datasets.

    The splitting is done based on either a specified percentage of the total data
    or by allocating a specified number of the *last* data files loaded
    to the validation set.

    Args:
        numeric_rep_data: A list of numerical data representing prices or
                          other types of data, already converted to their numerical
                          representation (e.g., token IDs). This is the combined data
                          from all loaded files for a single modality.
        val_size: A float between 0.1 and 0.3 representing the percentage
                  of the combined data to be used for validation. This is only used if
                  `num_val_files` is 0.
        num_val_files: An integer specifying the number of the *last* files
                       loaded for this modality to be used entirely for the validation set.
                       If 0, `val_size` is used for splitting. Must be a non-negative integer.
                       If greater than 0, it must be less than the total number of files loaded.
        file_lengths: A list of integers representing the length of the data
                      from each individual file or segment that makes up
                      the combined dataset for this modality.
                      Must be a list of positive integers.

    Returns:
        A tuple containing:
        - train_dataset: The list containing the training data. This list will be converted
                         to a tensor at a later stage (in the get_batch function).
        - val_dataset: The tensor containing the validation data.

    Raises:
        TypeError: If inputs are not of the expected types.
        ValueError: If inputs have invalid values (e.g., inconsistent lengths,
                    invalid val_size, invalid num_val_files).
    """

    if not isinstance(numeric_rep_data, (list)):
        raise TypeError("'numeric_rep_data' must be a list.")

    if not isinstance(num_val_files, int) or num_val_files < 0:
        raise TypeError("'num_val_files' must be an integer equal to or larger than 0.")

    if not isinstance(file_lengths, list) or not len(file_lengths) >= 1:
        raise TypeError("'file_lengths' must be a list containing at least 1 element.")

    total_files_loaded = len(file_lengths)

    if num_val_files > 0 and num_val_files >= total_files_loaded:
        # Here we verify that num_val_files is smaller than the number of files that were uploaded (so as to leave some files for the train set)
        raise ValueError(f"'num_val_files' ({num_val_files}) must be smaller than the total number of files uploaded ({total_files_loaded}).")


    val_num_elements = 0
    train_num_elements = 0


    # Train and val set sizes determined by 'num_val_files'
    if num_val_files > 0:
        # Sum the lengths of the last 'num_val_files' file_lengths for the validation set
        start_index = len(file_lengths) - 1
        for j in range(num_val_files):
            val_num_elements += file_lengths[start_index - j]

        train_num_elements = len(numeric_rep_data) - val_num_elements


    # Train and val set sizes determined by 'val_size'
    else:
        if not isinstance(val_size, float):
            raise TypeError("'val_size' must be a number between 0.1 and 0.3.")
        elif val_size > 0.3 or val_size < 0.1:
            raise ValueError("'val_size' must be a number between 0.1 and 0.3.")
        else:
            val_num_elements = int(val_size * len(numeric_rep_data))
            train_num_elements = len(numeric_rep_data) - val_num_elements


    train_dataset = numeric_rep_data[:train_num_elements]
    val_dataset = torch.tensor(numeric_rep_data[train_num_elements:], dtype=torch.long)


    return train_dataset, val_dataset

def add_rand_to_data_points(numeric_data, rand_size, vocab_size):
    """
    Introduces small random changes to numeric data for data augmentation.

    To mitigate limited trading data volume compared to language training,
    this function synthetically increases the amount of data by adding a small random value
    within a specified range to each data point. This creates slightly varied
    training examples without significantly altering the overall data distribution,
    helping to improve training on existing patterns.

    The random value is chosen from the range [-rand_size, rand_size],
    and is added to each element in `numeric_data` only if the result stays within
    the bounds of the vocabulary size.

    This function should be applied only to the training data.

    Args:
      numeric_data: A list or a 1D tensor of integers representing the numeric data.
      rand_size: An integer between 1-3, or None, specifying the maximum absolute value
                 of the random addition. The random value will be in the range
                 [-rand_size, rand_size].
      vocab_size: An integer representing the size of the vocabulary.

    Returns:
      A list or tensor of integers with small random changes applied.

    Raises:
      TypeError: If inputs are not of the expected types.
      ValueError: If inputs are not of the expected values.
    """

    # if numeric_data was input as a tensor, then temporarily turn it into a list
    if isinstance(numeric_data, torch.Tensor):
        numeric_data = numeric_data.tolist()
        numeric_data_is_a_tensor = True
    else:
        numeric_data_is_a_tensor = False

    # Input validation for numeric_data
    if not isinstance(numeric_data, list):
        raise TypeError("'numeric_data' must be a list.")
    for i, item in enumerate(numeric_data):
        if not isinstance(item, numbers.Number):
            raise TypeError(f"Element at index {i} in 'numeric_data' is not a number.")
        if not isinstance(item, int):
            raise TypeError(f"Element at index {i} in 'numeric_data' is not an integer.")

    # Input validation for rand_size
    if not isinstance(rand_size, int):
        raise TypeError("'rand_size' must be an integer.")
    if rand_size < 1 or rand_size > 3:
        raise ValueError("'rand_size' must be between 1 and 3.")

    # Input validation for vocab_size
    if not isinstance(vocab_size, int):
        raise TypeError("'vocab_size' must be an integer.")
    if vocab_size <= 0:
        raise ValueError("'vocab_size' must be a positive integer.")


    rand_list = [0]

    for r in range(rand_size):
        rand_list.extend([r+1, -(r+1)])

    for n in range(len(numeric_data)):
        # Check if adding the maximum possible random value still keeps the element within vocabulary bounds
        if max(rand_list) < numeric_data[n] < vocab_size - max(rand_list):
            # Add rand to data point
            numeric_data[n] += random.choice(rand_list)


    # turn numeric_data back to a tensor
    if numeric_data_is_a_tensor:
        numeric_data = torch.tensor(numeric_data, dtype=torch.long)


    return numeric_data

def generate_batch_starting_indices(data_size, block_size, batch_size, split, file_lengths, is_percents):
    '''
    Generates a batch of random starting indices for extracting sequences
    of a fixed length (block_size) from the data.

    When dealing with a combined dataset of multiple files or segments,
    this function ensures that the generated indices do not result in sequences
    that cross file or segment boundaries.

    Args:
        data_size: The total size of the dataset for the current split ('train' or 'val').
                   Must be a positive integer.
        block_size: The length of each sequence to be extracted (context window size).
                    Must be a positive integer.
                    Also, block_size must be less than `data_size`.
        batch_size: The number of starting indices to generate (the batch size).
                    Must be a positive integer.
        split: Indicates whether the data is for 'train' or 'val'. Must be 'train' or 'val'.
        file_lengths: A list of integers representing the length of the data
                      from each individual file or segment that makes up
                      the combined dataset for this modality.
                      Must be a list of positive integers.
        is_percents: A boolean indicating whether any modality's data is in percentage form.
                     (if True, the 1st element in each file in each modality, will be skipped when
                     generating starting indices). Applied to all modalities for consistent data lengths.

    Returns:
        torch.Tensor: A tensor of shape (batch_size,) containing the
                      random starting indices within the dataset.

    Raises:
        TypeError: If inputs are not of the expected types.
        ValueError: If inputs have invalid values (e.g., non-positive sizes,
                    invalid 'split' value, empty file_lengths list,
                    block_size >= data_size, or insufficient data to form
                    sequences of block_size).
    '''

    # --- Input Validation ---
    if not isinstance(data_size, int) or data_size <= 0:
        raise TypeError("'data_size' must be a positive integer.")
    if not isinstance(block_size, int) or block_size <= 0:
        raise TypeError("'block_size' must be a positive integer.")
    if block_size >= data_size:
        raise ValueError("'block_size' cannot be equal to or greater than 'data_size'.")
    if not isinstance(batch_size, int) or batch_size <= 0:
        raise TypeError("'batch_size' must be a positive integer.")
    if not isinstance(split, str) or (split != 'train' and split != 'val'):
        raise ValueError("'split' must be 'train' or 'val'.")
    if not isinstance(file_lengths, list) or not len(file_lengths) >= 1:
        raise TypeError("'file_lengths' must be a list containing at least 1 element.")
    if not isinstance(is_percents, bool):
        raise TypeError("'is_percents' must be a boolean.")


    block_size_xy = block_size + 1 # block_size_xy is meant to accommodate for both the input seq's length (block_size) plus the target seq being offset by 1


    if is_percents:
        # The 1st element in each file will be skipped when generating starting indices
        first_element_offset = 1
    else:
        first_element_offset = 0


    if len(file_lengths) == 1:
        # If there's only one continuous dataset (only one file was loaded for this modality),
        # then generate random starting indices ensuring sequences fit within the data (sequence + block_size_xy).
        # Adjust the range to start from 'first_element_offset'
        return torch.randint(first_element_offset, data_size - block_size_xy + 1, (batch_size,))


    if len(file_lengths) > 1:
        # When dealing with a combined dataset of multiple files, we need to ensure sequences don't cross file boundaries.
        dataset_file_lengths = [] # dataset_file_lengths will contain file lengths comprising this data set only (as opposed to file_lengths that contains file lengths of the entire data)
        num_files_loaded = len(file_lengths)
        file_size_accum = 0

        for f in range(num_files_loaded):

            if split == 'train':
                this_file_size = file_lengths[f]

            if split == 'val':
                # Here we're going backwards, starting from the end of file_lengths (because the valuation set is taken from the end of the data set)
                this_file_size = file_lengths[num_files_loaded - 1 - f]

            file_size_accum += this_file_size

            if file_size_accum <= data_size:
                dataset_file_lengths.append(this_file_size)

            if file_size_accum > data_size:
                # Add the portion of the last loaded file making this data set
                dataset_file_lengths.append(data_size - (file_size_accum - this_file_size))

            if file_size_accum >= data_size:
                if split == 'val':
                    # Now we reverse the order of lengths because we accumulated file sizes from the end of file_lengths backwards
                    dataset_file_lengths.reverse()
                break


        # Calculate the total number of valid starting positions across all relevant files
        # A valid starting position for a file of length L is from first_element_offset to L - block_size_xy.
        # Total valid positions in a file of length L is L - block_size_xy - first_element_offset + 1.
        total_valid_ix_positions = sum(max(0, length - block_size_xy - first_element_offset + 1) for length in dataset_file_lengths)


        # Handle the case where there are no valid starting positions
        if total_valid_ix_positions <= 0:
            # This could happen if all files are shorter than or equal to block_size_xy + first_element_offset - 1
            # return torch.empty(0, dtype=torch.long)
            raise ValueError(f"No valid starting positions available for the given block size and file lengths.")


        # Generate initial random indices within the range of total valid positions
        initial_indices = torch.randint(total_valid_ix_positions, (batch_size,))

        # Now, map these initial indices back to the correct starting positions in the
        # combined data, ensuring they fall within the valid range of a specific file.
        actual_indices = torch.empty(batch_size, dtype=torch.long)

        for i in range(batch_size):
            cumulative_valid_ix_positions = 0
            found_position = False

            # Iterate through only the relevant file lengths
            for k, length in enumerate(dataset_file_lengths):
                valid_ix_positions_in_this_file = max(0, length - block_size_xy - first_element_offset + 1) # Ensure non-negative

                if initial_indices[i] < cumulative_valid_ix_positions + valid_ix_positions_in_this_file:
                    # The initial index falls within the valid positions of this file
                    # We need to find its position within the file
                    position_within_file = initial_indices[i] - cumulative_valid_ix_positions
                    # The actual index is the sum of the lengths of previous files (start_of_this_file)
                    # + the position within the valid range of the file (position_within_file)
                    start_of_this_file = sum(dataset_file_lengths[:k])
                    actual_indices[i] = start_of_this_file + position_within_file + first_element_offset

                    found_position = True
                    break

                cumulative_valid_ix_positions += valid_ix_positions_in_this_file

                '''
                if cumulative_valid_ix_positions >= initial_indices[i]:
                    # We're now adding back all the block_size_xys that were omitted from initial_indices (through total_valid_ix_positions)
                    # in order to find the actual index position
                    actual_indices[i] = initial_indices[i] + (block_size_xy * k)
                '''


            if not found_position:
                 # This case should ideally not happen if total_valid_ix_positions was calculated correctly
                 # and initial_indices are within that range
                 raise ValueError(f"Could not map initial index {initial_indices[i]} to a valid ix position.")


        return actual_indices

def _get_direction_sign(current_value, previous_value, is_percentage_data):
    """
    Determines the direction sign (1 for up, -1 for down, 0 for flat)
    based on the current and previous numeric values and whether the data is percentages.

    Args:
        current_value: The current numeric value.
        previous_value: The previous numeric value (only used if not percentage data).
        is_percentage_data: Boolean indicating if the data represents percentages.

    Returns:
        An integer: 1 for up, -1 for down, 0 for flat.
    """
    if is_percentage_data:
        if current_value > 0: return 1
        elif current_value < 0: return -1
        else: return 0 # Handles current_value == 0
    else:
        # For value data, direction is based on change from previous value
        if not isinstance(previous_value, numbers.Number):
             # Cannot calculate direction if previous value is not numeric
             return None # Indicate that direction cannot be determined

        change = current_value - previous_value
        if change > 0: return 1
        elif change < 0: return -1
        else: return 0 # Handles change == 0


def calculate_evaluation_metrics(logits_list, yb_list, num_modalities, all_vocabularies, all_modality_params, all_file_info, batch_size, is_percents):
    """
    Calculates success rate (based on predicting the correct direction of change for numeric data)
    and certainty (confidence in the prediction) for each modality from evaluation logits and targets.

    This function processes the output of the model (logits) and the actual target values (yb)
    for a given batch during evaluation. It iterates through each modality and, for numeric data,
    determines if the model's prediction for the last token in a sequence correctly predicted
    the direction of change compared to the previous token.

    For the success rate calculation, *any* predicted direction (up, down, or flat) is compared
    against the actual direction. Wins are counted when the predicted direction matches the
    actual direction (e.g., both up, both down, or both flat). Losses are counted when they do not match.

    The certainty metric represents the model's confidence in the predicted *direction* of change for
    the last token. It is calculated by summing the probabilities (from the softmax of the last token's
    logits) of all possible next tokens that fall within the same direction (up, down, or flat) as the
    single token with the highest predicted probability.

    For applications like stock price prediction, accurately forecasting the direction of movement can
    be highly beneficial, as predicting the exact next price can be significantly more challenging and
    isn't always necessary.

    Note that unusually high directional certainty rates may occur, especially in early training stages
    or with certain data distributions, even if the overall success rate is near chance. This indicates
    the model is strongly skewed towards predicting a particular direction, regardless of accuracy.

    These directional metrics provide additional insights into the model's behavior during evaluation.
    The model's learning process, however, is driven solely by minimizing the calculated loss.

    The function accumulates these metrics across the batch and reports the results for each modality.

    Args:
        logits_list: A list of tensors, one for each modality, containing the logits
                     for each token in the batch. Shape: List of [(batch_size, block_size, vocab_size)]
        yb_list: A list of tensors, one for each modality, containing the target token
                 indices for each token in the batch. Shape: List of [(batch_size, block_size)]
        num_modalities: The total number of modalities being processed. (int)
        all_vocabularies: A list of lists, where each inner list is the vocabulary
                          (unique elements) for a specific modality, sorted in ascending order.
        all_modality_params: A list of lists, where each inner list contains the processing parameters
                             for a specific modality, in the format [num_whole_digits, decimal_places,
                             rand_size, cross_attend, convert_to_percents, num_bins, modality_name].
        all_file_info: A list of lists, where each inner list contains the file information
                       for a specific modality, in the format [file1_name, data1_length, ...].
        batch_size: The number of sequences processed in parallel in each batch. (int)
        is_percents: A boolean indicating whether the data being evaluated is in percentage form. (bool)

    Returns:
        A tuple containing four lists:
        - batch_wins_list: List of wins for each modality in the current batch.
        - batch_losses_list: List of losses for each modality in the current batch.
        - batch_certainty_list: List of total certainty for each modality in the current batch.
        - batches_processed_list: List of 1 (if batch was processed for directional metrics) or 0 for each modality.
    """
    batch_wins_list = [0] * num_modalities
    batch_losses_list = [0] * num_modalities
    batch_certainty_list = [0.0] * num_modalities
    batches_processed_list = [0] * num_modalities # To indicate if directional metrics were calculated for this modality in this batch


    for modality_index in range(num_modalities):
        # Get modality name from all_modality_params
        modality_name = all_modality_params[modality_index][6]
        # Use the first file name as a fallback if modality_name is not provided or is empty string
        if not modality_name or not isinstance(modality_name, str):
             # Get the name of the first file loaded for this modality from all_file_info
             # all_file_info[modality_index][0] is the name of the first file
             if all_file_info and len(all_file_info) > modality_index and all_file_info[modality_index]:
                 modality_name = os.path.basename(all_file_info[modality_index][0])
             else:
                 modality_name = f"Modality {modality_index+1}" # Fallback if no file info is available


        if len(logits_list) > modality_index and len(yb_list) > modality_index:

            modality_vocab = all_vocabularies[modality_index]
            is_percentage_data = all_modality_params[modality_index][4] # Get percentage flag from params

            # Check if the modality data is numeric and sequence length is sufficient for directional calculations
            data_is_numeric = all(isinstance(item, numbers.Number) for item in modality_vocab)
            min_seq_len = 1 if is_percentage_data else 2
            if data_is_numeric and yb_list[modality_index].ndim >= 2 and yb_list[modality_index].shape[1] >= min_seq_len:

                logits_modality = logits_list[modality_index][:, -1, :] # Logits for the last token
                targets_modality = yb_list[modality_index][:, -1] # Target index for the last token

                if targets_modality.shape[0] > 0: # Ensure there are elements in the target batch
                    batches_processed_list[modality_index] = 1 # Mark this modality as processed for directional metrics in this batch
                    batch_wins_modality = 0
                    batch_losses_modality = 0
                    batch_certainty_modality_sum = 0.0


                    for j in range(logits_modality.shape[0]): # Iterate through each sequence in the batch
                        predicted_token_logits = logits_modality[j]
                        predicted_token_index = torch.argmax(predicted_token_logits).item()
                        predicted_token_value = modality_vocab[predicted_token_index]

                        actual_token_index = targets_modality[j].item()
                        actual_token_value = modality_vocab[actual_token_index]

                        # Get previous actual value if needed for non-percentage data
                        prev_actual_token_value = None
                        if not is_percentage_data and yb_list[modality_index].shape[1] >= 2:
                             prev_actual_token_value = modality_vocab[yb_list[modality_index][j, -2].item()]


                        # Determine Predicted and Actual Direction Signs using helper function
                        predicted_direction_sign = _get_direction_sign(predicted_token_value, prev_actual_token_value, is_percentage_data)
                        actual_direction_sign = _get_direction_sign(actual_token_value, prev_actual_token_value, is_percentage_data)


                        # --- Count wins/losses based on direction signs ---
                        # Only count if both predicted and actual directions could be determined
                        if predicted_direction_sign is not None and actual_direction_sign is not None:
                            if predicted_direction_sign == actual_direction_sign:
                                 batch_wins_modality += 1
                            else:
                                 batch_losses_modality += 1


                            # --- Directional Certainty Calculation for this batch instance (j) ---
                            probs = F.softmax(predicted_token_logits, dim=-1)
                            summed_certainty_for_direction = 0.0

                            # Iterate through all possible next tokens in the vocabulary
                            for token_index, token_value in enumerate(modality_vocab):
                                if isinstance(token_value, numbers.Number): # Only consider numeric vocabulary values for certainty
                                    # Determine the direction sign of this possible token relative to the relevant previous value
                                    possible_direction_sign = _get_direction_sign(token_value, prev_actual_token_value, is_percentage_data)

                                    # Check if this possible token's direction sign matches the *predicted* direction sign for this batch instance (j)
                                    if possible_direction_sign is not None and possible_direction_sign == predicted_direction_sign:
                                        summed_certainty_for_direction += probs[token_index].item()

                            # Add the calculated certainty for this batch instance (j) to the batch total
                            batch_certainty_modality_sum += summed_certainty_for_direction
                        else:
                             # If direction could not be determined for this instance, it's not counted for wins/losses or certainty
                             pass


                    # Store the total results for this batch and modality
                    batch_wins_list[modality_index] = batch_wins_modality
                    batch_losses_list[modality_index] = batch_losses_modality
                    batch_certainty_list[modality_index] = batch_certainty_modality_sum


            else:
                 # If directional metrics were skipped for this batch and modality, indicate why (optional print, can be removed for cleaner output during training)
                 # modality_data = all_modality_data[modality_index] # Access processed data to check type
                 # data_is_numeric_check = all(isinstance(item, numbers.Number) for item in modality_data)
                 # if not data_is_numeric_check:
                 #      print(f"Warning: Data for Modality {modality_index+1}: '{modality_name}' is not numeric. Directional metrics skipped for this batch.")
                 # elif yb_list[modality_index].ndim < 2 or yb_list[modality_index].shape[1] < min_seq_len:
                 #      print(f"Warning: Sequence length for Modality {modality_index+1}: '{modality_name}' is less than {min_seq_len} ({yb_list[modality_index].shape[1] if yb_list[modality_index].ndim >= 2 else 'N/A'}). Cannot calculate directional metrics for this batch.")
                 pass # Suppress verbose warnings during training


        else:
            # print(f"Could not perform success rate or certainty calculation for Modality {modality_index+1}: '{modality_name}' due to missing logits or targets for this batch.")
            pass # Suppress verbose warnings during training


    return batch_wins_list, batch_losses_list, batch_certainty_list, batches_processed_list

def range_numeric_data(numeric_data, num_whole_digits, decimal_places):
  """
  Converts numeric data to a specified range by scaling them by factors of 10
  and/or rounds to a specified number of decimal places.

  The purpose is to standardize data magnitude and precision, thereby controlling
  vocabulary size.

  This function scales numeric values to a range defined by `num_whole_digits`
  (including 0 and negative for scaling down) and rounds to `decimal_places`
  (or original precision if None), preserving the sign of negative numbers.

  Args:
    numeric_data: A list of numeric data points. Must be a list containing numeric types.
                  The data may contain values that are positive, zero, or negative.
                  (zero, or negative values could be expected with data types like percentages).
    num_whole_digits: The desired number of whole digits for the value of the ranged values
                      (e.g., 1 for ones: range of 1.00 to 9.99, 2 for tens: range of 10.00 to 99.99, etc.).
                      Must be an integer or None. If None, no scaling based on whole digits.
    decimal_places: The desired number of decimal places for the values (ranged or un-ranged).
                    Must be an integer greater than or equal to 0, or None.
                    If None, the number of decimal places will remain as is in the input data.

  Returns:
    A list of float values that have been ranged and/or rounded.

  Raises:
    TypeError: If inputs are not of the expected types.
    ValueError: If inputs have invalid values (e.g., empty list,
                negative decimal_places if not None, non-numeric data).
    IndexError: If an element in 'numeric_data' is not a number.
  """

  # Input validation
  if not isinstance(numeric_data, list):
      raise TypeError("'numeric_data' must be a list.")
  if not numeric_data:
      raise TypeError("'numeric_data' must be a non-empty list.")

  for i, element in enumerate(numeric_data):
      if not isinstance(element, numbers.Number):
          raise IndexError(f"Element at index {i} in 'numeric_data' is not a number.")

  if num_whole_digits is not None and not isinstance(num_whole_digits, int):
      raise TypeError("'num_whole_digits' must be an integer or None.")

  if decimal_places is not None and not isinstance(decimal_places, int):
      raise TypeError("'decimal_places' must be an integer or None.")
  if decimal_places is not None and decimal_places < 0:
      raise ValueError("'decimal_places' must be greater than or equal to 0.")


  processed_data = []
  has_negative_values = any(element < 0 for element in numeric_data)

  apply_dec_places_for_print_range = 0
  if decimal_places is not None and decimal_places >= 0:
      apply_dec_places_for_print_range = decimal_places
  elif numeric_data:
      s = str(numeric_data[0])
      if '.' in s:
          decimal_part = s.split('.')[-1]
          apply_dec_places_for_print_range = len(decimal_part)

  for element in numeric_data:
      if element == 0:
          power_of_10 = 0
      else:
          abs_element = abs(element)
          if abs_element == 0:
              power_of_10 = 0
          else:
              power_of_10 = int(math.floor(math.log10(abs_element)))

      apply_dec_places = decimal_places if decimal_places is not None else (len(str(element).split('.')[-1]) if '.' in str(element) else 0)
      apply_dec_places = max(0, apply_dec_places)


      scaling_factor = 1
      if num_whole_digits is not None:
          desired_power_of_10 = num_whole_digits - 1
          scaling_factor = 10**(desired_power_of_10 - power_of_10)


      scaled_value = round(element * scaling_factor, apply_dec_places) if scaling_factor != 0 else 0.0

      if num_whole_digits is not None:
           lower_bound_abs = 10**(num_whole_digits - 1)
           upper_bound_abs_compare = 10**num_whole_digits

           abs_scaled_value = abs(scaled_value)

           if abs_scaled_value < lower_bound_abs and abs_scaled_value > 0:
               abs_scaled_value = lower_bound_abs
           if apply_dec_places > 0:
               if abs_scaled_value >= upper_bound_abs_compare:
                  abs_scaled_value = upper_bound_abs_compare - (10**(-apply_dec_places))
           else:
               if abs_scaled_value >= upper_bound_abs_compare:
                  abs_scaled_value = 10**num_whole_digits - 1

           scaled_value = abs_scaled_value * (-1 if element < 0 else 1)


      processed_data.append(scaled_value)


  # Print ranging information
  if num_whole_digits is not None:
      lower_bound_print = 10**(num_whole_digits - 1)
      upper_bound_print = 10**num_whole_digits - (10**(-apply_dec_places_for_print_range) if apply_dec_places_for_print_range > 0 else 1)

      range_str = f'{lower_bound_print:.{apply_dec_places_for_print_range}f} to {upper_bound_print:.{apply_dec_places_for_print_range}f}'
      prefix = '\u00B1 ' if has_negative_values else ''

      print(f"  + Data scaled to a range of: {prefix}{range_str} ('whole digits' is {num_whole_digits})")
  else:
      print(f"  - No ranging specified ('whole digits' is None)")


  # Print rounding information
  if decimal_places is not None:
      print(f'  + Values have been rounded to: {decimal_places} decimal places')
  else:
      print(f"  - No rounding specified ('decimal places' is None)")


  vocabulary = list(set(processed_data))
  print(f'  New vocabulary size: {len(vocabulary)}')


  # Print first and last elements of processed data
  if len(processed_data) >= 10:
      print('  Dataset first / last elements (processed data):\n', '', end=' ')
      for val in processed_data[:5]:
           print(f'{int(round(val)) if decimal_places == 0 and abs(val - round(val)) < 1e-9 else val}', end=' ')
      print('...', end=' ')
      for val in processed_data[-5:]:
           print(f'{int(round(val)) if decimal_places == 0 and abs(val - round(val)) < 1e-9 else val}', end=' ')
      print()
  elif processed_data:
      print('  Processed data:', end=' ')
      for val in processed_data:
          print(f'{int(round(val)) if decimal_places == 0 and abs(val - round(val)) < 1e-9 else val}', end=' ')
      print()


  if processed_data:
      min_val = min(processed_data)
      max_val = max(processed_data)
      min_print = int(round(min_val)) if decimal_places == 0 and abs(min_val - round(min_val)) < 1e-9 else min_val
      max_print = int(round(max_val)) if decimal_places == 0 and abs(max_val - round(max_val)) < 1e-9 else max_val
      print(f'  Min element (processed): {min_print}')
      print(f'  Max element (processed): {max_print}')


  return processed_data

def bin_numeric_data(data, num_groups, outlier_percentile=5, exponent=2.0):
    """
    Divides a list of numeric data into a specified number of groups with
    non-uniform ranges, based on an exponential-like distribution, after
    removing outliers using percentiles, handling both positive and negative values symmetrically.

    Args:
        data: A list of numeric data points.
        num_groups: The desired number of groups for *each* the positive and negative sides.
                    The total number of groups will be up to 2 * num_groups + 1 (including a zero group).
                    Must be an integer > 0.
        outlier_percentile: The percentile to use for removing outliers.
                            Values below this percentile and above (100 - this percentile)
                            will be excluded from the range calculation for both positive
                            and absolute negative values. Must be between 0 and 50.
                            (default: 5, removing values below 5th and above 95th percentile).
        exponent: The exponent to control the non-linear division of the range.
                  A value > 1 creates smaller ranges closer to zero and larger ranges further away.
                  A value = 1 creates uniform ranges. Must be a number >= 1. (default: 2.0).

    Returns:
        A list of integers, where each integer represents the group assignment
        (ranging from -num_groups to num_groups, with 0 for values near zero)
        for the corresponding data point in the original data list.

    Raises:
        TypeError: If inputs are not of the expected types.
        ValueError: If inputs have invalid values (e.g., empty data list,
                    non-positive num_groups, invalid outlier_percentile, invalid exponent).
    """
    # Input validation
    if not isinstance(data, list) or not data:
        raise ValueError("'data' must be a non-empty list.")
    for i, item in enumerate(data):
        if not isinstance(item, numbers.Number):
            raise TypeError(f"Element at index {i} in 'data' is not a number.")

    if not isinstance(num_groups, int) or num_groups <= 0:
        raise ValueError("'num_groups' must be a positive integer.")

    if not isinstance(outlier_percentile, numbers.Number) or not (0 <= outlier_percentile <= 50):
         raise ValueError("'outlier_percentile' must be a number between 0 and 50.")

    if not isinstance(exponent, numbers.Number) or exponent < 1:
        raise ValueError("'exponent' must be a number greater than or equal to 1.")

    # Separate positive, negative, and zero values
    positive_data = [x for x in data if x > 0]
    negative_data = [x for x in data if x < 0]
    # zero_data_indices = [i for i, x in enumerate(data) if x == 0] # This is not used

    # Convert to numpy arrays for percentile calculation
    positive_np = np.array(positive_data) if positive_data else np.array([])
    negative_abs_np = np.abs(np.array(negative_data)) if negative_data else np.array([])

    # Determine effective maximum absolute value considering outliers
    effective_max_abs = 0.0
    if positive_np.size > 0:
        effective_max_positive = np.percentile(positive_np, 100 - outlier_percentile)
        effective_max_abs = max(effective_max_abs, effective_max_positive)

    if negative_abs_np.size > 0:
        effective_max_negative_abs = np.percentile(negative_abs_np, 100 - outlier_percentile)
        effective_max_abs = max(effective_max_abs, effective_max_negative_abs)

    # Handle case where effective_max_abs is very small or zero
    if effective_max_abs <= 1e-9: # Use a small tolerance for near-zero
        print("Warning: Effective max absolute value is near zero. All data will be assigned to group 0.")
        # Assign all data to group 0 and return
        group_assignments = [0] * len(data)
        vocabulary = list(set(group_assignments))
        print(f'  New Vocabulary size (populated bins): {len(vocabulary)}')
        print("  Group 0 Count:", len(data), "data points")
        return group_assignments # Return the list of zeros


    # Generate positive group boundaries
    positive_group_boundaries = [0.0] # Start from zero
    for i in range(1, num_groups + 1):
        normalized_i = i / num_groups
        scaled_position = normalized_i**exponent
        boundary = effective_max_abs * scaled_position
        positive_group_boundaries.append(boundary)

    # Ensure the last positive boundary is the true maximum absolute value
    true_max_abs = max(np.max(positive_np) if positive_np.size > 0 else 0,
                       np.max(negative_abs_np) if negative_abs_np.size > 0 else 0)
    # Adjust the last boundary slightly if it's exactly the true max to ensure inclusion
    if true_max_abs > 0:
        positive_group_boundaries[-1] = true_max_abs + (true_max_abs * 1e-9) # Add a small tolerance


    # Generate negative group boundaries (symmetric to positive)
    # Reverse the positive boundaries and negate them
    negative_group_boundaries = [-b for b in reversed(positive_group_boundaries)]


    # Assign data points to groups
    group_assignments = [0] * len(data) # Initialize with 0 for zero values

    for i, value in enumerate(data):
        if value > 0:
            # Find the group index for positive values
            group_index = 0
            for j in range(num_groups):
                 # Check if value is within the boundary range [boundary_low, boundary_high)
                 if value >= positive_group_boundaries[j] and value < positive_group_boundaries[j+1]:
                      group_index = j + 1 # Group numbers 1 to num_groups
                      break
            # Handle the case where the value is exactly on the upper boundary (should go to the last group)
            if value == positive_group_boundaries[-1]:
                 group_index = num_groups


            group_assignments[i] = group_index

        elif value < 0:
            # Find the group index for negative values
            group_index = 0
            # Iterate through negative boundaries (from most negative towards zero)
            # Note: negative_group_boundaries is in increasing order (e.g., [-100, -50, -20, 0])
            for j in range(num_groups):
                 # Check if value is within the boundary range [boundary_low, boundary_high)
                 if value >= negative_group_boundaries[j] and value < negative_group_boundaries[j+1]:
                       # Map index j to group number -num_groups to -1
                       group_index = -(num_groups - j)
                       break
            # Handle the case where the value is exactly on the first negative boundary (should go to the first group)
            if value == negative_group_boundaries[0]:
                 group_index = -num_groups


            group_assignments[i] = group_index



    # Print binning info
    print(f"  Data binned into the following positive, negative, and zero groups ('num groups' is {num_groups}):\n")

    # Combine and print group boundaries/descriptions and counts
    # Negative Groups
    for i in range(num_groups):
         group_label = -(num_groups - i)
         boundary_low = negative_group_boundaries[i]
         boundary_high = negative_group_boundaries[i+1]
         group_count = group_assignments.count(group_label)
         if i == 0: # For the lowest group, show "and below"
             print(f"  Group {group_label}:  [{boundary_high:.2f} and below)  Count: {group_count}")
         else:
             print(f"  Group {group_label}:  [{boundary_low:.2f}, {boundary_high:.2f})  Count: {group_count}")


    # Zero Group
    group_label = 0
    zero_count = group_assignments.count(group_label)
    print(f"\n  Group {group_label}:  Values equal to zero.  Count: {zero_count}\n")

    # Positive Groups
    for i in range(num_groups):
         group_label = i + 1
         boundary_low = positive_group_boundaries[i]
         boundary_high = positive_group_boundaries[i+1]
         group_count = group_assignments.count(group_label)
         if i == num_groups - 1: # For the highest group, show "and above"
              print(f"  Group {group_label}:  [{boundary_low:.2f} and above]  Count: {group_count}")
         else:
              print(f"  Group {group_label}:  [{boundary_low:.2f}, {boundary_high:.2f})  Count: {group_count}")


    total_assigned = sum(group_assignments.count(i) for i in range(-num_groups, num_groups + 1))
    if total_assigned != len(data):
        print(f"Warning: Total assigned data points ({total_assigned}) does not match input data length ({len(data)}).")


    # Print new vocab size
    vocabulary = list(set(group_assignments))
    print(f'\n  New vocabulary size (populated bins): {len(vocabulary)}')


    # Print first and last elements of binned data
    if len(group_assignments) >= 10:
        print('  Dataset first / last elements (binned):\n', '', end=' ')
        # Determine decimal places for printing based on original data (if applicable) or default
        decimal_places_for_print = 0
        if data and isinstance(data[0], float):
             s = str(data[0])
             if '.' in s:
                 decimal_part = s.split('.')[-1]
                 decimal_places_for_print = len(decimal_part)

        for val in group_assignments[:5]:
            print(f'{val}', end=' ') # Binned data is already integer
        print('...', end=' ')
        for val in group_assignments[-5:]:
            print(f'{val}', end=' ') # Binned data is already integer
        print()
    elif group_assignments:
        print('  Processed data:', end=' ')
        for val in group_assignments:
            print(f'{val}', end=' ') # Binned data is already integer
        print()


    if group_assignments:
      min_val = min(group_assignments)
      max_val = max(group_assignments)
      print(f'  Min element (binned): {min_val}')
      max_val_print = int(round(max_val)) if abs(max_val - round(max_val)) < 1e-9 else max_val
      print(f'  Max element (binned): {max_val_print}')


    return group_assignments

def calculate_percent_changes(data, decimal_places=2):
    """
    Calculates the percentage change between adjacent numeric data points
    and returns a list of the same length by prepending a 0.

    Args:
        data: A list of numeric data points. Must be a list containing numeric types.
              The data may contain values that are positive, zero, or negative.
        decimal_places: The number of decimal places to round the percentage changes to.
                        Must be an integer greater than or equal to 0, or None.
                        If None, the default value of 2 will be used. (default: 2).

    Returns:
        A list of float percentage changes, starting with 0, with the same length as
        the input data list.

    Raises:
        TypeError: If inputs are not of the expected types.
        ValueError: If inputs have invalid values (e.g., empty data list,
                    negative decimal_places if not None).
        ZeroDivisionError: If an attempt is made to divide by zero when calculating
                           percentage change.
    """
    # Input validation
    if not isinstance(data, list) or not data:
        raise ValueError("'data' must be a non-empty list.")
    for i, item in enumerate(data):
        if not isinstance(item, numbers.Number):
            raise TypeError(f"Element at index {i} in 'data' is not a number.")

    # Validate decimal_places: must be int >= 0 or None
    if decimal_places is not None and not isinstance(decimal_places, int):
        raise TypeError("'decimal_places' must be an integer or None.")
    if decimal_places is not None and decimal_places < 0:
        raise ValueError("'decimal_places' must be greater than or equal to 0 when not None.")

    # Determine the actual number of decimal places to use for rounding
    actual_decimal_places = decimal_places if decimal_places is not None else 2


    percent_changes = [0.0] # Prepend 0 as the first element (in order to keep the processed data at the same length as the input data)
                            # (this element will later be skipped over when generating batch starting indices)

    for i in range(len(data) - 1):
        current_value = data[i]
        next_value = data[i+1]

        if current_value == 0:
            # Handle division by zero appropriately. Depending on the data,
            # Here, we'll raise an error to alert the user.
            raise ZeroDivisionError(f"Cannot calculate percentage change: division by zero at index {i}. Current value is 0.")

        percent_change = ((next_value - current_value) / current_value) * 100.0
        percent_changes.append(round(percent_change, actual_decimal_places))

    # Ensure the returned list has the same length as the input list
    if len(percent_changes) != len(data):
        # This should not happen with the current logic, but as a safeguard
        print(f"Warning: Returned list length ({len(percent_changes)}) does not match input list length ({len(data)}).")

    return percent_changes

def write_initial_run_details(file_path, hyperparams, data_info, modality_configs, run_stats):
    """
    Writes the initial run details (hyperparameters, data info, modality configs)
    to the specified output file.

    Args:
        file_path (str): The full path to the output file.
        hyperparams (dict): A dictionary containing the model hyperparameters.
        data_info (dict): A dictionary containing general data information (e.g., split sizes).
        modality_configs (list): A list of dictionaries, where each dictionary
                                 contains the configuration details for a modality.
        run_stats (dict): A dictionary containing overall run statistics (e.g., number of parameters).
    """
    if file_path: # Only write if a file path is provided
        with open(file_path, 'a', encoding='utf-8') as f:
            now = datetime.now()
            current_time_date = now.strftime("%Y-%m-%d %H:%M:%S")
            f.write(f"\n\n{current_time_date}\n")
            f.write("\nModel Settings and Data Information:\n")

            # Write Hyperparameters
            f.write("Hyperparameters:\n")
            for key, value in hyperparams.items():
                f.write(f"  {key}: {value}\n")
            f.write("\n")

            # Write Run Statistics
            f.write("Run Statistics:\n")
            for key, value in run_stats.items():
                f.write(f"  {key}: {value}\n")
            f.write("\n")


            # Write Data Information
            f.write("Data Information:\n")
            for key, value in data_info.items():
                 f.write(f"  {key}: {value}\n")
            f.write("\n")

            # Write Input Schemas/Modality Configurations
            f.write("Input Schemas (Modality Configurations):\n")
            for i, config in enumerate(modality_configs):
                f.write(f"  Modality {i+1}:\n")
                for key, value in config.items():
                    f.write(f"    {key}: {value}\n")
            f.write("\n")

"""# Data loading & processing"""

# Data Preparation:
# - Load raw data from files
# - Process the data (if specified)
# - Create a vocabulary of unique elements and convert it into a numerical representation
# - Split the data into training and validation sets


all_modality_data = []  # For each modality, will contain a list of raw data elements, or of proccessed elements (if specified and if numeric)
all_file_info = []  # For each modality, will contain a list of the loaded file information: [file1_name, data1_length, file2_name, data2_length, ...]
all_modality_params = []  # For each modality, will contain a list of processing parameters: [num_whole_digits, decimal_places, rand_size, cross_attend, convert_to_percents, num_bins, modality_name]

modality_num = 0
is_percents = False
input_schema_in_use = False

for i in range(num_input_schemas):
  input_schema_name = f'input_schema_{i+1}'
  if input_schema_name in globals():
    this_input_schema = globals()[input_schema_name]
    if this_input_schema != []:

      convert_to_percents = this_input_schema[3] if len(this_input_schema) > 3 else None
      num_whole_digits = this_input_schema[4] if len(this_input_schema) > 4 else None
      decimal_places = this_input_schema[5] if len(this_input_schema) > 5 else None
      num_bins = this_input_schema[6] if len(this_input_schema) > 6 else None
      rand_size = this_input_schema[7] if len(this_input_schema) > 7 else None
      cross_attend = this_input_schema[8] if len(this_input_schema) > 8 else None
      modality_name = this_input_schema[9] if len(this_input_schema) > 9 else None


      if cross_attend is None: cross_attend = False

      if convert_to_percents is None: convert_to_percents = False
      if convert_to_percents == True: is_percents = True

      if not (isinstance(modality_name, str) or modality_name is None):
        raise TypeError(f"Element 10 (modality name) of 'input_schema' must be a string or None, but got {type(modality_name).__name__}.")


      print("\n\n----------------------------------------------------------\n\n")
      print("Preparing data...")

      modality_num += 1
      modality_name = modality_name if isinstance(modality_name, str) else ''
      print(f"\nModality {modality_num}: '{modality_name}'")


      # Load data
      this_modality_data, this_file_info = load_file_data(this_input_schema)


      # Range numeric data: scale values and set decimal places
      if num_whole_digits is not None or decimal_places is not None:
        # Check if the loaded data is numeric before processing
        data_is_numeric = all(isinstance(item, numbers.Number) for item in this_modality_data)
        if data_is_numeric:
          print(f"\n\n  Applying Ranging and/or Decimal Places to Modality '{modality_name}'...\n")
          this_modality_data = range_numeric_data(this_modality_data, num_whole_digits, decimal_places)
        else:
          # Find and report the non-numeric element
          print(f"\nWarning: Ranging or Decimal Places specified for Modality {modality_num}, but data is not entirely numeric.")
          report_non_numeric_error(this_modality_data, this_file_info, modality_num)
      #else:
        #print(f'\n  No Ranging or Decimal Places set for Modality {modality_num}. Values will remain as loaded.')


      # Bin numeric data
      if num_bins is not None:
        outlier_percentile = 0.1 # Percentage of extreme values (outliers) to be excluded from bin range calculation
        exponent = 2.2 # Controls how bin ranges are distributed (e.g., uniform ranges with exponent = 1, increasingly non-uniform with higher exponents)
        print(f"\n\n  Applying Binning to Modality '{modality_name}'...\n")
        this_modality_data = bin_numeric_data(this_modality_data, num_bins, outlier_percentile, exponent)


      all_modality_data.append(this_modality_data)
      all_file_info.append(this_file_info)
      all_modality_params.append([num_whole_digits, decimal_places, rand_size, cross_attend, convert_to_percents, num_bins, modality_name])


      input_schema_in_use = True



if not input_schema_in_use:
  raise ValueError("All input_schema lists are empty. You must specify at least one.")


print("\n\n\n✔ Data loading for all specified modalities complete")
num_modalities = len(all_modality_data)

# Check for equal modality lengths
if num_modalities > 1:
    first_modality_length = len(all_modality_data[0])
    for i in range(1, num_modalities):
        if len(all_modality_data[i]) != first_modality_length:
            raise ValueError(
                f"Modality {i+1} has a different data length ({len(all_modality_data[i])}) "
                f"than the first modality ({first_modality_length}). "
                "All modalities must have the same data length."
            )
    print("✔ All modalities have equal data lengths")


# Convert all lists of input data into their numerical representation,
# and create a vocabulary of unique elements for each.
all_numeric_reps = []
all_vocabularies = []

print("\n\n----------------------------------------------------------\n\n")
print("Creating Vocabularies and Numerical Representations...")

for m in range(num_modalities):
  this_modality_name = all_modality_params[m][6]
  print(f"\nModality {m+1}: '{this_modality_name}'")

  numeric_rep, vocab = numerical_representation(all_modality_data[m])
  all_numeric_reps.append(numeric_rep)
  all_vocabularies.append(vocab)
  print(f"  Vocabulary size: {len(vocab)}")
  print(f"  Numerical representation length: {len(numeric_rep)}")


# Split the data into training (all_train_sets) and validation (all_val_sets) sets for all modalities,
# and converted all datasets into PyTorch tensors.
# But first, create a list 'file_lengths' containing the file lengths (or more accurately,
# the lengths of data segments taken from those files) of the files uploaded to create the first modality.
# (the reason for using file lengths from the first modality and applying it to all modalities- insuring similar
# splitting across all modalities, specifically when using num_validation_files).

file_lengths = []

# all_file_info[0] is [file1_name, data1_length, file2_name, data2_length, ...]
# Extract lengths which are at odd indices (1, 3, 5, ...)
for f in range(1, len(all_file_info[0]), 2):
  file_lengths.append(all_file_info[0][f])


all_train_sets = []
all_val_sets = []

print("\n\n----------------------------------------------------------\n\n")
print("Creating Training and Validation datasets...\n")

for i in range(num_modalities):
  # Use the file_lengths derived from the first modality for splitting all modalities
  this_train_set, this_val_set = create_train_val_datasets(all_numeric_reps[i], validation_size, num_validation_files, file_lengths)
  all_train_sets.append(this_train_set)
  all_val_sets.append(this_val_set)

  # Print the method by which train/val set sizes were determined
  # Print only once (if i == 0), (applies for all modalities)
  if i == 0:
    if num_validation_files > 0:
      # Lengths determined by num_validation_files
      print(f"Data splitting by file length (num_validation_files = {num_validation_files}):")
      print(f"Validation sets comprise the combined length of the last {num_validation_files} files from Modality 1")
      print(f"Training sets comprise the length of the remaining data")
      '''
      # Print the file names used for validation in the first modality
      # all_file_info[0] is [file1_name, data1_length, file2_name, data2_length, ...]
      # For the validation set we need to go backwards, so start from the second to last element (index len(all_file_info[0]) - 2) and step backwards by 2
      val_files_counter = 0
      for j in range(len(all_file_info[0]) - 2, -1, -2):
        this_file_name = all_file_info[0][j]
        print(f"  - {this_file_name}")
        val_files_counter += 1
        if val_files_counter == num_validation_files:
          break
      '''

    else:
      # Lengths determined by validation_size
      val_pct = validation_size * 100
      if val_pct == round(val_pct):
        formatted_val_pct = int(val_pct) # Convert to integer if it's a whole number
      else:
        formatted_val_pct = round(val_pct, 2) # Round to 2 decimal places if it's a fraction
      print(f"Validation sets will comprise {formatted_val_pct}% of the total data length (validation_size = {validation_size})")
      print(f"Training sets will comprise the remaining {100 - formatted_val_pct}% of the data")

  this_modality_name = all_modality_params[i][6]
  print(f"\nModality {i+1}: '{this_modality_name}'")
  print(f"  Validation data length: {len(this_val_set)}")
  print(f"  Training data length: {len(this_train_set)}")

  # Print randomness specified for this modality
  this_rand_size = all_modality_params[i][2]
  if isinstance(this_rand_size, int) and 1 <= this_rand_size <= 3:
    print(f"  + Random noise range of: \u00B1{this_rand_size} will be applied to the training set of this modality")
  elif this_rand_size is None:
    print(f'  - Random noise not set for this modality')

  # Print cross-attention specified for this modality
  this_cross_attend = all_modality_params[i][3]
  if this_cross_attend == True:
    print(f"  + Cross-attention is enabled (this modality will attend to all other modalities)")
  elif this_cross_attend == False:
    print(f'  - Cross-attention is not enabled for this modality')


print("\n\n\n✔ Data preparation for all modalities complete")

"""# Building the transformer"""

def get_batch(split, is_training):
    # Generate for all modalities batches of data of inputs (xb_list) and targets (yb_list)

    # Create a temporary list for train sets to potentially apply randomness
    temp_all_train_sets_processed = [t for t in all_train_sets]

    for r in range(num_modalities):
      this_rand_size = all_modality_params[r][2]
      this_vocab_size = len(all_vocabularies[r])

      # Randomness would only be applied to training sets (is_training = 1)
      if this_rand_size is not None and is_training == 1:
        # Apply randomness to the temporary list for this modality
        temp_all_train_sets_processed[r] = add_rand_to_data_points(temp_all_train_sets_processed[r], this_rand_size, this_vocab_size)

    # Convert processed training data lists to tensors
    temp_all_train_sets_tensors = [torch.tensor(t, dtype=torch.long) for t in temp_all_train_sets_processed]


    # all_val_sets are already tensors from create_train_val_datasets
    temp_data_list = temp_all_train_sets_tensors if split == 'train' else all_val_sets


    # Generate starting indices for the first modality (assuming all modalities have the same length and structure)
    # (we might need to adjust this if modalities have different structures/lengths)
    ix = generate_batch_starting_indices(len(temp_data_list[0]), block_size, batch_size, split, file_lengths, is_percents)


    # Create batches for all modalities
    xb_list = []
    yb_list = []
    for r in range(num_modalities):
        temp_data = temp_data_list[r]
        xb = torch.stack([temp_data[i:i+block_size] for i in ix])
        yb = torch.stack([temp_data[i+1:i+block_size+1] for i in ix])
        xb, yb = xb.to(device), yb.to(device)
        xb_list.append(xb)
        yb_list.append(yb)

    return xb_list, yb_list



class Head(nn.Module):  # The Head class represents a single attention head within a multi-head attention mechanism
                        # Head inheriting from nn.Module is a standard way to define a custom layer/component in PyTorch

    def __init__(self, head_size):  # head_size determines the dimensionality of the output of the head.
                                    # ie, the dimensionality of the output of a single attention head.
                                    # it determines how many "features" or "dimensions" the attention head will use to represent the relationships between tokens in the input sequence.

        super().__init__()  # constructor

        self.key = nn.Sequential(
            nn.Linear(n_embd, head_size // 2),
            nn.Tanh(),
            nn.Linear(head_size // 2, head_size, bias=False)
        )
        self.query = nn.Sequential(
            nn.Linear(n_embd, head_size // 2),
            nn.Tanh(),
            nn.Linear(head_size // 2, head_size, bias=False)
        )
        self.value = nn.Sequential(
            nn.Linear(n_embd, head_size // 2),
            nn.Tanh(),
            nn.Linear(head_size // 2, head_size, bias=False)
        )


        '''
        self.key = nn.Sequential(   # nn.Sequential is a container that allows to define a sequence of neural network modules in a sequential order,
                                    # it is used to define the structure of the key, query, and value transformations.
                                    # when we pass an input to an nn.Sequential instance, it will be passed through each module in the defined order, and the output of the last module will be returned.


            nn.Linear(n_embd, head_size // 2),  # a linear transformation that maps the input embedding (n_embd) to an intermediate size (head_size // 2).
                                                # the intermediate size in the attention head (head_size // 2) aims to balance dimensionality reduction and information preservation.
                                                # this is particularly beneficial when dealing with large embedding dimensions (n_embd) or long input sequences.
                                                # while head_size // 2 is a common choice, it's worth considering other values based on the specific application.
                                                # // divides and rounds down to the nearest integer. For example, 7 // 2 would result in 3.
                                                # head_size // 2 is effectively half the value of head_size, rounded down to the nearest whole number.
                                                # we can experiment with other intermediate sizes, such as head_size // 4 or head_size * 3 // 4, and observe their impact on the model's performance.
                                                #
                                                # what is a linear transformation? a linear transformation takes an input vector and applies a weighted sum to produce an output vector.
                                                # this operation can be represented mathematically as:
                                                # y = x * W^T + b
                                                # where:
                                                # x is the input vector, W is the weight matrix (^T denotes the transpose operation), b is the bias vector, y is the output vector
                                                #
                                                # how nn.Linear is applied:
                                                # 1. Input: The input to the first nn.Linear layer is the token embedding (n_embd dimensions).
                                                # 2. Transformation: The nn.Linear layer applies a linear transformation using a learned weight matrix and a bias vector (if bias=True, which is the default).
                                                # this maps the input to an intermediate size (head_size // 2).
                                                # 3. Non-linearity: The nn.Tanh activation function introduces non-linearity.
                                                # 4. Output: The second nn.Linear layer further transforms the intermediate representation to the final head_size dimension.
                                                # in this case, bias=False, meaning no bias term is used.
                                                #
                                                # nn.Linear is a fundamental module in PyTorch that performs linear transformations.
                                                # it's used within the Head class and other parts of the Transformer model to
                                                # extract features, connect neural network layers, and enable the model to learn and make predictions.


            nn.Tanh(),  # An activation function (hyperbolic tangent) to introduce non-linearity.
                        # it's a non-linear function that maps input values to a range between -1 and 1.
                        # this prevents the activations from becoming too large or too small, which can lead to issues like vanishing or exploding gradients during training.
                        # by introducing a non-linear activation function, the model gains the ability to learn more complex and nuanced patterns in the data.
                        # alternative activation functions that could instead be used are: ReLU, Sigmoid, etc.


            nn.Linear(head_size // 2, head_size, bias=False)  # another linear transformation that maps the intermediate representation to the final head_size dimension.
                                                              # bias=False indicates that the linear layer will not use a bias term.
        )
        '''

        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))
        # This line registers a buffer called tril
        # torch.tril creates a lower triangular matrix (all elements above the main diagonal are zero) with size block_size x block_size.
        # this is used for masking in the attention mechanism to prevent the model from attending to future tokens in the sequence.

        self.dropout = nn.Dropout(dropout)
        # This adds a dropout layer to the attention head.
        # Dropout helps to prevent overfitting by randomly setting a fraction of the input units to zero during training.
        # (dropout) is a hyperparameter that controls the dropout rate.


    def forward(self, x):   # This function is the core of the attention mechanism within a single "head" of a Transformer model.
                            # This function defines how the attention head processes its input during the forward pass of the neural network.

        Ba,Bl,C = x.shape # x is the input to the attention head
                          # x has shape (batch_size, block_size, n_embd)
                          # batch_size (Ba): The number of independent sequences processed in parallel
                          # block_size (Bl): The length of each sequence (context window)
                          # n_embd (C): The dimensionality of the token embeddings
        k = self.key(x)   # k has shape (batch_size, block_size, head_size)
                          # The input x is transformed into "keys" (k) using a linear layer (self.key)
        q = self.query(x) # q has shape (batch_size, block_size, head_size)
                          # The input x is transformed into "queries" (q) using a linear layer (self.query).
        aff = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5  # (Ba, Bl, hs) @ (Ba, hs, Bl) -> (Ba, Bl, Bl)
                                                          # affinities (aff) represent the relationships or similarities between different tokens in the input sequence
                                                          # transpose- is an operation that flips the matrix over its diagonal, switching the rows and columns
                                                          # k.transpose(-2,-1): transposes the last two dimensions of the keys (k)
                                                          # in the attention mechanism, the transpose operation is applied to the key matrix (k) before calculating the dot product with the query matrix (q)
                                                          # @ is a matrix multiplication operation
                                                          # dot product is a mathematical operation that takes two vectors as input and produces a single scalar value as output
                                                          # the dot product of two vectors results in a scalar value (scalar value is a single numerical value, often a real number),
                                                          # which is used to calculate the affinities between queries and keys.
                                                          # in the attention mechanism, the dot product is used to calculate the affinities between the queries (q) and keys (k)
                                                          # this is the core of how the attention mechanism determines the relationships between different tokens in the input sequence
                                                          # q @ k.transpose(-2,-1) calculates the dot product between the query matrix (q) and the transposed key matrix (k)
                                                          # the result is a matrix where each element represents the dot product between a specific query and a specific key
                                                          # k.shape[-1]**-0.5: the dot products are then scaled down by the square root of the head size
                                                          # this scaling helps prevent the dot products from becoming too large and helps stabilize the training process
                                                          # the dot product is a good measure of similarity between two vectors.
                                                          # when the dot product is high, it means that the two vectors are pointing in similar directions (i.e., they are more similar)
                                                          # the resulting aff tensor represents how much each token in the sequence should "attend" to other tokens

        aff = aff.masked_fill(self.tril[:Bl, :Bl] == 0, float('-inf'))  # (Ba, Bl, Bl)
                                                                        # this line applies a mask to the affinities to prevent the model from attending to future tokens in the sequence
                                                                        # self.tril: a lower triangular matrix used for masking
                                                                        # masked_fill: fills elements in the aff tensor with -inf where the mask is 0, preventing attention to those positions

        aff = F.softmax(aff, dim=-1)  # (Ba, Bl, Bl)
                                      # applies the softmax function to the affinities to normalize them into probabilities
                                      # this ensures that the attention weights sum to 1 for each token

        aff = self.dropout(aff)   # dropout is a regularization technique used in neural networks to prevent overfitting
                                  # overfitting occurs when a model learns the training data too well and performs poorly on unseen data
                                  # during each training step, dropout randomly selects a subset of neurons and temporarily deactivates them ("dropping out" means setting to zero)
                                  # this means their output is ignored during that training step.
                                  # the dropout rate is a hyperparameter that controls the probability of dropping out a neuron or connection
                                  # it is typically set between 0 and 1, where 0 means no dropout and 1 means dropping out all neurons (which would effectively disable the network)
                                  # common values for the dropout rate are in the range of 0.1 to 0.5

        # perform the weighted aggregation of the values
        v = self.value(x) # (Ba,Bl,head_size)
                          # The input x is transformed into "values" (v) using a linear layer (self.value).
        att_out = aff @ v # (Ba, Bl, Bl) @ (Ba, Bl, head_size) -> (Ba, Bl, head_size)
                          # this calculates the final output of the attention head by performing a weighted aggregation of the values (v) based on the attention weights (aff)
                          # this is where the model combines information from different tokens in the sequence based on their relevance

        return att_out


class MultiHeadAttention(nn.Module):

    def __init__(self, num_heads, head_size):
        super().__init__()  # initializes the parent class (nn.Module), which is a standard practice in PyTorch when defining custom modules
        self.heads = nn.ModuleList([Head(head_size) for cur_head in range(num_heads)])
        # creates a list of Head objects representing the multiple attention heads
        # nn.ModuleList is a container that holds these heads as individual modules
        # the Head class encapsulates a single attention mechanism
        # it takes the input sequence, calculates keys, queries, and values, performs the attention calculation, and produces an output
        # this output represents how much each token in the sequence should "attend" to other tokens
        self.proj = nn.Sequential(
            nn.Linear(head_size * num_heads, n_embd//2),
            nn.Tanh(),
            nn.Linear(n_embd//2, n_embd)
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        out = torch.cat([head(x) for head in self.heads], dim=-1)
        # this iterates through each attention head (head in self.heads), applies it to the input (x), and concatenates the outputs along the last dimension (dim=-1)
        # this effectively combines the information from all the attention heads
        out = self.dropout(self.proj(out))
        # the concatenated output is then passed through the projection layer (self.proj) and a dropout layer (self.dropout) for further processing and regularization
        return out


class CrossAttention(nn.Module):
    def __init__(self, num_heads, head_size, num_kv_modalities):
        super().__init__()
        self.num_kv_modalities = num_kv_modalities
        # Each head now needs to process keys/values from multiple modalities
        self.heads = nn.ModuleList([self.Head(head_size, num_kv_modalities) for _ in range(num_heads)])

        # The projection layer needs to handle the concatenated output from all heads
        # The output from each head is head_size, and there are num_heads.
        # The input to proj is the concatenation of head outputs, which will be num_heads * head_size
        self.proj = nn.Sequential(
            nn.Linear(head_size * num_heads, n_embd // 2),
            nn.Tanh(),
            nn.Linear(n_embd // 2, n_embd)
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self, query_x, key_value_x_list):
        # query_x: the modality that is querying, shape (batch_size, block_size, n_embd)
        # key_value_x_list: List of tensors, one for each modality providing keys and values
        # Each tensor in key_value_x_list should have shape (batch_size, block_size, n_embd)

        # Concatenate outputs from all heads
        # Each head now takes query_x and the list of key_value_x_list
        out = torch.cat([head(query_x, key_value_x_list) for head in self.heads], dim=-1)
        out = self.dropout(self.proj(out))
        return out

    # Modified Head forward for Cross-Attention
    class Head(nn.Module):
        def __init__(self, head_size, num_kv_modalities):
            super().__init__()
            self.num_kv_modalities = num_kv_modalities
            self.head_size = head_size # Store head_size

            # Query layer remains for the query modality
            self.query = nn.Sequential(
                nn.Linear(n_embd, head_size // 2),
                nn.Tanh(),
                nn.Linear(head_size // 2, head_size, bias=False)
            )

            # Linear layers for keys and values for each KV modality
            self.key_layers = nn.ModuleList([nn.Sequential(
                nn.Linear(n_embd, head_size // 2),
                nn.Tanh(),
                nn.Linear(head_size // 2, head_size, bias=False)
            ) for _ in range(num_kv_modalities)])

            self.value_layers = nn.ModuleList([nn.Sequential(
                nn.Linear(n_embd, head_size // 2),
                nn.Tanh(),
                nn.Linear(head_size // 2, head_size, bias=False)
            ) for _ in range(num_kv_modalities)])


            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # Still needed for potential self-attention scenarios or specific masking needs
            self.dropout = nn.Dropout(dropout)


        def forward(self, query_x, key_value_x_list):
            # query_x: shape (batch_size, block_size, n_embd)
            # key_value_x_list: List of tensors, each shape (batch_size, block_size, n_embd)

            Ba, Bl, C = query_x.shape

            q = self.query(query_x) # Queries from query_x, shape (batch_size, block_size, head_size)

            # Calculate affinities and weighted sums for each KV modality separately
            # and then combine the results.
            if not key_value_x_list:
                # If there are no KV modalities to attend to, return zeros
                return torch.zeros(Ba, Bl, self.head_size, device=query_x.device)

            attended_outputs = []
            for i in range(self.num_kv_modalities):
                k = self.key_layers[i](key_value_x_list[i])   # (Ba, Bl, head_size)
                v = self.value_layers[i](key_value_x_list[i]) # (Ba, Bl, head_size)

                # Calculate affinity for this specific KV modality
                # (Ba, Bl, hs) @ (Ba, hs, Bl) -> (Ba, Bl, Bl)
                aff = q @ k.transpose(-2,-1) * self.head_size**-0.5

                # Masking (optional, depending on the cross-attention scenario)
                # aff = aff.masked_fill(self.tril[:Bl, :Bl] == 0, float('-inf'))

                aff = F.softmax(aff, dim=-1)  # Softmax over the sequence length dimension
                aff = self.dropout(aff)

                # Calculate weighted aggregation for this KV modality
                # (Ba, Bl, Bl) @ (Ba, Bl, hs) -> (Ba, Bl, hs)
                att_out = aff @ v
                attended_outputs.append(att_out)

            # Combine the attended outputs from different KV modalities
            # One simple way is to sum them
            combined_att_out = sum(attended_outputs)

            return combined_att_out


class FeedForward(nn.Module): # this line defines the class FeedForward, inheriting from nn.Module
                              # nn.Module is the base class for all neural network modules. Inheriting from it provides essential functionalities for building and training neural networks

    def __init__(self, n_embd):
        super().__init__()
        self.nonlin = nn.Sequential(        # this defines the core structure of the feedforward network using nn.Sequential
                                            # nn.Sequential allows chaining multiple layers together, ensuring that the output of one layer becomes the input to the next
            nn.Linear(n_embd, 6 * n_embd),  # this is a linear layer that takes an input of dimension n_embd and projects it to a higher-dimensional space of size 6 * n_embd
                                            # this expansion allows the network to learn more complex relationships
            nn.Tanh(),                      # this applies the hyperbolic tangent activation function to the output of the previous linear layer
                                            # the activation function introduces non-linearity, enabling the network to model complex patterns
            nn.Linear(6 * n_embd, n_embd),  # this is another linear layer that projects the output back down to the original dimensionality (n_embd)
            nn.Dropout(dropout),            # this applies dropout regularization, randomly setting a fraction of the input units to zero during training
        )

    def forward(self, x):   # this method takes the input x and passes it through the self.nonlin sequence of layers, returning the final output
        return self.nonlin(x)


class MultimodalBlock(nn.Module):

    def __init__(self, n_embd, n_head, num_modalities, all_modality_params):
        super().__init__()
        head_size = n_embd // n_head

        self.num_modalities = num_modalities
        self.all_modality_params = all_modality_params

        # Self-attention for each modality
        self.self_attention_heads = nn.ModuleList([MultiHeadAttention(n_head, head_size) for _ in range(num_modalities)])

        # Cross-attention (optional and selective)
        self.cross_attention_heads = nn.ModuleDict()
        # Only create cross-attention heads if there is more than one modality
        if num_modalities > 1:
            for i in range(num_modalities):
                # Check if this modality is configured to cross-attend
                if all_modality_params[i][3] is True: # all_modality_params[i][3] is the cross_attend status
                    # This modality will attend to all *other* modalities
                    num_kv_modalities = num_modalities - 1
                    # Create a cross-attention head for this querying modality
                    self.cross_attention_heads[f'{i}_to_all_others'] = CrossAttention(n_head, head_size, num_kv_modalities)


        # Feedforward and normalization for each modality
        self.ffd_layers = nn.ModuleList([FeedForward(n_embd) for _ in range(num_modalities)])
        self.norm1_layers = nn.ModuleList([nn.LayerNorm(n_embd) for _ in range(num_modalities)])
        self.norm2_layers = nn.ModuleList([nn.LayerNorm(n_embd) for _ in range(num_modalities)])


    def forward(self, x_list): # x_list is a list of tensors, one for each modality
        # x_list: List of tensors, each shape (batch_size, block_size, n_embd)

        attended_x_list = []
        for i in range(self.num_modalities):
            # Apply Layer Norm and Self-Attention
            norm_x = self.norm1_layers[i](x_list[i])
            self_attended_x = x_list[i] + self.self_attention_heads[i](norm_x)
            attended_x_list.append(self_attended_x)

        # Apply Cross-Attention based on the specified modalities
        cross_attended_x_list = [x.clone() for x in attended_x_list] # Create a copy to add cross-attention results
        # Only attempt cross-attention if there is more than one modality and cross-attention heads were created
        if self.num_modalities > 1 and self.cross_attention_heads:
            for i in range(self.num_modalities):
                # Check if this modality is configured to cross-attend
                if self.all_modality_params[i][3] is True: # all_modality_params[i][3] is the cross_attend status
                    query_x = attended_x_list[i]
                    # Create the list of key/value tensors from *all other* modalities
                    key_value_x_list = [attended_x_list[j] for j in range(self.num_modalities) if j != i]

                    # Ensure the corresponding cross-attention head exists before calling it
                    head_key = f'{i}_to_all_others'
                    if head_key in self.cross_attention_heads:
                        cross_attended_output = self.cross_attention_heads[head_key](query_x, key_value_x_list)
                        cross_attended_x_list[i] = cross_attended_x_list[i] + cross_attended_output
                    else:
                        # This case should ideally not happen if __init__ is correct, but good for debugging
                        print(f"Warning: Cross-attention head '{head_key}' not found for modality {i}.")


        # Apply Layer Norm and FeedForward
        final_x_list = []
        for i in range(self.num_modalities):
            norm_x = self.norm2_layers[i](cross_attended_x_list[i])
            final_x = cross_attended_x_list[i] + self.ffd_layers[i](norm_x)
            final_x_list.append(final_x)

        return final_x_list


## Fixed embedding table:
class FixedEmbedding(nn.Module):  # this class defines a custom embedding layer where the embedding values are fixed and not learned during training
    def __init__(self, vocab_size, n_embd, fixed_values): # vocab_size: the size of the vocabulary (number of unique tokens)
                                                          # n_embd: the dimensionality of the embedding vectors
                                                          # fixed_values: a list of predefined values from which the embedding values will be randomly selected
        super(FixedEmbedding, self).__init__()
        self.vocab_size = vocab_size  # this stores the vocabulary size as attributes of the class
        self.n_embd = n_embd          # this stores the embedding dimension as attributes of the class

        # Create embedding table with fixed values
        embedding_table = torch.tensor([  # this creates a tensor named embedding_table to store the fixed embeddings
                                          # it iterates through each token in the vocabulary (vocab_size) and for each token, it creates an embedding vector of size n_embd
            [random.choice(fixed_values) for _ in range(n_embd)]  # the values within the embedding vector are randomly chosen from the fixed_values list using random.choice
            for _ in range(vocab_size)
        ], dtype=torch.float32) # specifies the data type of the tensor

        # Register embedding_table as a buffer (non-trainable parameter)
        self.register_buffer('embedding_table', embedding_table)

    def forward(self, input_tokens):
        # this forward method of this class defines how the embedding layer processes its input
        # it takes input_tokens (a tensor representing token indices) as input
        # and retrieves the corresponding fixed embeddings from the embedding_table based on the input_tokens and returns them as output
        """
        Args:
            input_tokens (torch.Tensor): Indices of tokens. Shape: [batch_size, seq_len]
        Returns:
            torch.Tensor: Fixed embeddings. Shape: [batch_size, seq_len, n_embd]
        """
        return self.embedding_table[input_tokens]

fixed_values = [-0.5, -0.2, -0.1, 0, 0.1, 0.2, 0.5]
# when creating the embedding table, each element of the embedding vectors is randomly selected from this fixed_values list


def long_tanh(x):
    return x.tanh().long()
# the long_tanh function takes a tensor (X), squishes its values to be between -1 and 1 using the tanh function,
# and then turns those squished values into integers (using the long integer data type --> the resulting tensor will contain 64-bit integers)

### ???- how can we have integers of type long and between -1 and 1  ###


class MultimodalPreBlock(nn.Module):
    '''
    MultimodalPreBlock is responsible for converting input tokens from multiple modalities into numerical representations called embeddings.
    It also adds information about the position of each token in the sequence, consistently across all modalities.
    '''
    def __init__(self, num_modalities, vocab_sizes):
        super().__init__()
        self.num_modalities = num_modalities
        self.vocab_sizes = vocab_sizes # list of vocab sizes, one for each modality

        # Token embeddings for each modality
        self.token_embedding_tables = nn.ModuleList([nn.Embedding(vocab_sizes[i], n_embd) for i in range(num_modalities)])

        # Positional embedding table (shared across modalities)
        self.position_embedding_table = nn.Embedding(block_size, n_embd)

    def forward(self, idx_list): # idx_list is a list of tensors, one for each modality
        # idx_list: List of tensors, each shape (batch_size, block_size)

        embedded_output_list = []
        for i in range(self.num_modalities):
            B, T = idx_list[i].shape
            tok_emb = self.token_embedding_tables[i](idx_list[i]) # Token embeddings for modality i

            # Positional embeddings (shared and expanded)
            pos_emb = self.position_embedding_table(torch.arange(T, device=idx_list[i].device))
            pos_emb = pos_emb.expand_as(tok_emb)

            embedded_output = tok_emb + pos_emb
            embedded_output_list.append(embedded_output)

        return embedded_output_list


class MultimodalPostBlock(nn.Module):
    '''
    MultimodalPostBlock takes the processed output from the multimodal transformer blocks
    and transforms it into logits for each modality for predicting the next token.
    '''
    def __init__(self, num_modalities, vocab_sizes):
        super().__init__()
        self.num_modalities = num_modalities
        self.vocab_sizes = vocab_sizes

        # Layer normalization and linear layers for each modality
        self.fin_norm_layers = nn.ModuleList([nn.LayerNorm(n_embd) for _ in range(num_modalities)])
        self.soft_score_layers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(n_embd, vocab_sizes[i] // 2),
                nn.Tanh(),
                nn.Linear(vocab_sizes[i] // 2, vocab_sizes[i])
            ) for i in range(self.num_modalities)
        ])

    def forward(self, x_list): # x_list is a list of tensors, one for each modality
        # x_list: List of tensors, each shape (batch_size, block_size, n_embd)

        logits_list = []
        for i in range(self.num_modalities):
            x = self.fin_norm_layers[i](x_list[i])
            logits = self.soft_score_layers[i](x)
            logits_list.append(logits)

        return logits_list


'''
The MultimodalTransformer class performs the following operations:
1. MultimodalPreBlock: Prepares input from multiple modalities by converting them into embeddings and adding positional information.
2. MultimodalBlocks: These are the core processing units. Each block performs self-attention within each modality and selective cross-attention between specified modalities.
3. forward: Defines the entire multimodal transformer process.
4. generate: Is used to generate new tokens for a specified modality based on the context from all modalities.
'''
class MultimodalTransformer(nn.Module):

    def __init__(self, num_modalities, vocab_sizes, all_modality_params):
        super().__init__()
        self.num_modalities = num_modalities
        self.vocab_sizes = vocab_sizes
        self.all_modality_params = all_modality_params # Store all_modality_params

        self.pre_block = MultimodalPreBlock(num_modalities, vocab_sizes)
        # Pass all_modality_params to the MultimodalBlock
        self.blocks = nn.Sequential(*[MultimodalBlock(n_embd, n_head, num_modalities, all_modality_params) for _ in range(n_layer)])
        self.post_block = MultimodalPostBlock(num_modalities, vocab_sizes)

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx_list, targets_list=None):
        # idx_list: List of tensors, one for each modality, each shape (batch_size, block_size)
        # targets_list: List of tensors (optional), one for each modality, each shape (batch_size, block_size)

        x_list = self.pre_block(idx_list) # Process input through PreBlock

        x_list = self.blocks(x_list) # Process through Transformer blocks

        logits_list = self.post_block(x_list) # Get logits for each modality

        losses = [None] * self.num_modalities
        if targets_list is not None:
            for i in range(self.num_modalities):
                B, T, V = logits_list[i].shape
                logits = logits_list[i].view(B * T, V)
                targets = targets_list[i].view(B * T)
                losses[i] = F.cross_entropy(logits, targets)

        return logits_list, losses


    def generate(self, idx_list, max_new_tokens=1, modality_to_generate=0):
        # idx_list: List of initial input tensors, one for each modality, each shape (batch_size, initial_seq_len)
        # max_new_tokens: Number of tokens to generate
        # modality_to_generate: Index of the modality for which to generate tokens

        for _ in range(max_new_tokens):
            # Crop the sequence to the block size
            idx_cond_list = [idx[:, -block_size:] for idx in idx_list]

            # get the predictions
            # The forward method now returns only logits_list and losses, so we unpack accordingly
            logits_list, _ = self(idx_cond_list)
            logits = logits_list[modality_to_generate][:, -1, :] # Get logits for the last token of the modality to generate

            probs = F.softmax(logits, dim=-1) # apply softmax to get probabilities

            idx_next = torch.multinomial(probs, num_samples=1) # get next token, shape is (batch_size, 1)

            # Append sampled index only to the specified modality
            new_idx_list = []
            for i in range(self.num_modalities):
                if i == modality_to_generate:
                    new_idx_list.append(torch.cat((idx_list[i], idx_next), dim=1))
                else:
                    # For other modalities, you might need a strategy to handle their sequence length
                    # For now, assuming they are padded or handled appropriately elsewhere
                    new_idx_list.append(idx_list[i])
            idx_list = new_idx_list

        return idx_list # Return the updated list of modality tensors

"""# Running the transformer"""

def estimate_loss():
    out = {}
    m.eval() # Use 'm' instead of 'model'
    for state in ['train', 'val']:
        total_losses = [] # List to store total loss for each evaluation iteration

        now = datetime.now()
        current_time = now.strftime("%H:%M:%S")
        print(f'\nEvaluating {state} set ({eval_iters} iterations)... Current time: {current_time}')
        # Initialize counters for success rate and certainty calculation for all modalities
        all_modalities_total_batches_processed = [0] * num_modalities
        all_modalities_total_correct = [0] * num_modalities
        all_modalities_total_incorrect = [0] * num_modalities
        all_modalities_total_certainty = [0] * num_modalities

        # Track if the non-numeric data warning has been printed for each modality in this evaluation run
        non_numeric_warning_printed = [False] * num_modalities


        for k in range(eval_iters):
            # get_batch returns lists of tensors: [xb_mod1, xb_mod2, ...], [yb_mod1, yb_mod2, ...]
            xb_list, yb_list = get_batch(state, 0)

            # Pass lists of tensors to the multimodal model
            logits_list, losses_list = m(xb_list, yb_list) # Use 'm' instead of 'model'

            # Calculate total loss for this evaluation iteration by summing modality losses
            # Ensure losses_list is not None and contains tensors
            if losses_list and all(l is not None for l in losses_list):
                 total_loss_this_iter = sum(losses_list)
                 total_losses.append(total_loss_this_iter.item()) # Store the scalar loss value
            else:
                 # Handle cases where losses might not be calculated (e.g., during generation if targets are None)
                 print(f"Warning: Losses not calculated for iteration {k} in state {state}. Skipping loss recording for this iter.")


            # Print evaluation progress (optional, but helpful)
            # print(f"Evaluation ({state} set):", k+1, "/", eval_iters) # Removed this for cleaner output


            # Call calculate_evaluation_metrics to calculate evaluation metrics for this batch
            batch_correct, batch_incorrect, batch_certainty, batches_processed_list = calculate_evaluation_metrics(
                logits_list, yb_list, num_modalities, all_vocabularies, all_modality_params, all_file_info, batch_size, is_percents
            )

            # Check if any modality was skipped due to non-numeric data and print a warning once per eval run
            for modality_index in range(num_modalities):
                if not non_numeric_warning_printed[modality_index]:
                     modality_vocab = all_vocabularies[modality_index]
                     data_is_numeric = all(isinstance(item, numbers.Number) for item in modality_vocab)
                     if not data_is_numeric:
                          modality_name = all_modality_params[modality_index][6]
                          # Use path name as a fallback if modality_name is not provided or is empty string
                          if not modality_name or not isinstance(modality_name, str):
                               # Get the name of the first file loaded for this modality from all_file_info
                               # all_file_info[modality_index][0] is the name of the first file
                               if all_file_info and len(all_file_info) > modality_index and all_file_info[modality_index]:
                                   modality_name = os.path.basename(all_file_info[modality_index][0])
                               else:
                                   modality_name = f"Modality {modality_index+1}" # Fallback if no file info is available
                          #print(f"Warning: Data for Modality {modality_index+1}: '{modality_name}' is not numeric. Directional metrics skipped for this evaluation run.")
                          non_numeric_warning_printed[modality_index] = True


            # Accumulate the results returned by the separate function
            for modality_index in range(num_modalities):
                 all_modalities_total_correct[modality_index] += batch_correct[modality_index]
                 all_modalities_total_incorrect[modality_index] += batch_incorrect[modality_index]
                 all_modalities_total_certainty[modality_index] += batch_certainty[modality_index]
                 all_modalities_total_batches_processed[modality_index] += batches_processed_list[modality_index] # Accumulate based on batches_processed_list


        # Report accumulated success rate and certainty for all modalities after all evaluation iterations
        print_state = 'Train' if state == 'train' else 'Val'
        print(f"\n\n-------  Directional Metrics Summary  -------")
        print(f"\n{print_state} set:")
        for modality_index in range(num_modalities):
            # Get modality name from all_modality_params
            modality_name = all_modality_params[modality_index][6]
            # Use the first file name as a fallback if modality_name is not provided or is empty string
            if not modality_name or not isinstance(modality_name, str):
                 # Get the name of the first file loaded for this modality from all_file_info
                 # all_file_info[modality_index][0] is the name of the first file
                 if all_file_info and len(all_file_info) > modality_index and all_file_info[modality_index]:
                     modality_name = os.path.basename(all_file_info[modality_index][0])
                 else:
                     modality_name = f"Modality {modality_index+1}" # Fallback if no file info is available

            print(f"\nModality {modality_index+1}: '{modality_name}'")
            this_num_batches_processed = all_modalities_total_batches_processed[modality_index]

            # Only report correct/incorrect and success rate if there were batches where directional calculation was attempted
            if this_num_batches_processed > 0:
                print(f'  Total batches processed (iters x batches): {this_num_batches_processed * batch_size}')
                print(f'  Correct direction predictions: {all_modalities_total_correct[modality_index]}')
                print(f'  Incorrect direction predictions: {all_modalities_total_incorrect[modality_index]}')
                total_movements_counted = all_modalities_total_correct[modality_index] + all_modalities_total_incorrect[modality_index]
                if total_movements_counted > 0:
                     overall_success_rate_modality = round(all_modalities_total_correct[modality_index] / total_movements_counted * 100, 1)
                     print(f'  Overall directional success rate (correct/incorrect): {overall_success_rate_modality}%')
                else:
                     print(f'  Overall directional success rate: NA (No movements predicted or occurred in counted batches)')

                # Calculate and report overall average directional certainty
                overall_average_certainty_modality = all_modalities_total_certainty[modality_index] / (this_num_batches_processed * batch_size) # Assuming batch_size is constant and used for certainty accumulation
                #print(f"  Overall Average Directional Certainty: {round(overall_average_certainty_modality * 100, 1)}%") # Not displaying at the moment

            else:
                 # If no batches were processed for directional metrics for this modality, indicate why
                 modality_data = all_modality_data[modality_index] # Access processed data to check type
                 data_is_numeric = all(isinstance(item, numbers.Number) for item in modality_data)
                 if not data_is_numeric:
                      print("  Directional metrics skipped: Modality data is not numeric")
                 elif yb_list and len(yb_list) > modality_index and yb_list[modality_index].shape[1] < (1 if is_percents else 2): # Check sequence length (assuming yb_list from the last batch is representative)
                      print(f"  Directional metrics skipped: Sequence length ({yb_list[modality_index].shape[1] if len(yb_list) > modality_index else 'N/A'}) too short for directional calculation (needs at least {1 if is_percents else 2}).")
                 else:
                      # Should not reach here if batches_processed is 0 but data is numeric and sequence length is sufficient
                      print("  Directional metrics skipped: Reason unknown (batches_processed is 0)")


        #if state == 'train':
        print('\n\n-----------------------------------\n')


        if state == 'val' and output_file_name != '':
          with open(output_file_path, 'a', encoding='utf-8') as f:
            for modality_index in range(num_modalities):
                # Get modality name from all_modality_params
                modality_name = all_modality_params[modality_index][6]
                # Use the first file name as a fallback if modality_name is not provided or is empty string
                if not modality_name or not isinstance(modality_name, str):
                     # Get the name of the first file loaded for this modality from all_file_info
                     # all_file_info[modality_index][0] is the name of the first file
                     if all_file_info and len(all_file_info) > modality_index and all_file_info[modality_index]:
                         modality_name = os.path.basename(all_file_info[modality_index][0])
                     else:
                         modality_name = f"Modality {modality_index+1}" # Fallback if no file info is available

                # Write the success rate and certainty summary for each Modality to the output file
                # Log validation metrics only, as this data was not used for training
                f.write(f"Validation set (Modality {modality_index+1}: {modality_name}): Total Batches={all_modalities_total_batches_processed[modality_index]*batch_size}, Directional Correct={all_modalities_total_correct[modality_index]}, Directional Incorrect={all_modalities_total_incorrect[modality_index]}")
                total_movements_counted = all_modalities_total_correct[modality_index] + all_modalities_total_incorrect[modality_index]
                if total_movements_counted > 0:
                     f.write(f", Directional Success Rate (correct/incorrect)={round(all_modalities_total_correct[modality_index] / total_movements_counted * 100, 1)}%\n")
                else:
                     f.write(f", Directional Success Rate (correct/incorrect)=NA\n")

                # if all_modalities_total_batches_processed[modality_index] > 0:
                #      f.write(f", Average Directional Certainty={round(all_modalities_total_certainty[modality_index] / (all_modalities_total_batches_processed[modality_index] * batch_size) * 100, 1)}%\n") # Assuming batch_size is constant
                # else:
                #      f.write(f", Average Directional Certainty=NA\n")


        # Calculate the mean of the total losses collected across evaluation iterations
        # Handle case where no losses were recorded
        out[state] = torch.tensor(total_losses).mean().item() if total_losses else float('nan')

    m.train() # Use 'm' instead of 'model'
    return out


# --- Model Creation and Loading ---
#
# The 'create_new_model' variable (defined in a settings cell) controls whether
# a new model is created or a previously saved one is loaded.
# Set create_new_model = 1 to create a new model and start training from scratch.
# Set create_new_model = 0 to attempt to load a model from model_file_name.
#
# The 'model_file_name' variable (defined in a settings cell) specifies the path
# to save the model to, or load the model from. Ensure this path is correct.
#
# IMPORTANT CONSIDERATION WHEN LOADING A MODEL (create_new_model = 0):
# The code assumes that the data loading and processing steps executed
# BEFORE attempting to load the model generate the *same* vocabulary
# and that the hyperparameters (like n_embd, n_head, n_layer, block_size,
# num_modalities) match those of the saved model.
# If the data, vocabulary, or hyperparameters change between saving and loading,
# the loaded model might not work correctly with the current data or evaluation
# logic and could produce nonsensical results.

# --- Model Saving ---
#
# The 'save_model' variable (defined in a settings cell) controls whether
# the model's parameters are saved during and after training.
# Set save_model = 1 to save the model periodically during training (at eval_interval)
# and at the end of training.
# Set save_model = 0 to disable model saving for this training run.
#
# When save_model = 1, the model will be saved to the path specified by
# 'model_file_name'.


# Create a list of vocabulary sizes for all modalities
all_vocab_sizes = [len(vocab) for vocab in all_vocabularies]


print('\n\n==========================================================\n\n')
# Instantiate the model based on create_new_model flag
if create_new_model == 1:
    print("Creating a new model...")
    # Pass the list of vocab sizes and all_modality_params to the model constructor
    m = MultimodalTransformer(num_modalities, all_vocab_sizes, all_modality_params).to(device)
    optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)
else:
    print(f"Attempting to load model from: {model_file_name}...")
    # Pass the list of vocab sizes and all_modality_params when instantiating the model for loading
    m = MultimodalTransformer(num_modalities, all_vocab_sizes, all_modality_params).to(device)
    try:
        m.load_state_dict(torch.load(model_file_name))
        print("Model loaded successfully.")
        optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)
        print("Optimizer created with loaded model parameters.")
    except FileNotFoundError:
        print(f"Model file not found at: {model_file_name}.\nCreating a new model instead.")
        # Pass the list of vocab sizes and all_modality_params to the model constructor
        m = MultimodalTransformer(num_modalities, all_vocab_sizes, all_modality_params).to(device)
        optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)
        print("Optimizer created for the new model.")
    except Exception as e:
        print(f"An error occurred while loading the model: {e}")
        print("Creating a new model instead.")
        # Pass the list of vocab sizes and all_modality_params to the model constructor
        m = MultimodalTransformer(num_modalities, all_vocab_sizes, all_modality_params).to(device)
        optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)
        print("Optimizer created for the new model.")


# Calculate and write the number of parameters after the model 'm' is instantiated
num_params = sum(p.numel() for p in m.parameters())/1e6
print(f"Model parameter size: {round(num_params, 2)}M\n")

# --- Prepare data structures for initial file writing ---

# 1. Hyperparameters dictionary
hyperparams = {
    "n_embd": n_embd,
    "n_head": n_head,
    "n_layer": n_layer,
    "block_size": block_size,
    "batch_size": batch_size,
    "dropout": dropout,
    "learning_rate": learning_rate
}

# 2. Run Statistics dictionary
run_stats = {
    "Model parameter size (M)": round(num_params, 2)
}

# 3. Data Information dictionary
# Assuming train/val sizes are the same for all modalities
train_size = len(all_train_sets[0])
val_size_actual = len(all_val_sets[0])
split_method = f"validation_size={validation_size}" if num_validation_files == 0 else f"num_validation_files={num_validation_files}"

# Extract vocab sizes and data lengths for data_info summary
modality_vocab_sizes_summary = ", ".join([f"Modality {i+1}={len(all_vocabularies[i])}" for i in range(num_modalities)])
modality_data_lengths_summary = ", ".join([f"Modality {i+1}={len(all_modality_data[i])}" for i in range(num_modalities)])


data_info = {
    "Number of modalities": num_modalities,
    "Train set size": train_size,
    "Val set size": val_size_actual,
    "Split method": split_method,
    "Modality vocabulary sizes": modality_vocab_sizes_summary,
    "Modality data lengths": modality_data_lengths_summary
}

# 4. Modality Configurations list of dictionaries
modality_configs = []
for i in range(num_modalities):
    modality_params = all_modality_params[i]
    modality_file_info = all_file_info[i]

    config = {
        "Source": os.path.basename(modality_file_info[0]) if modality_file_info else 'N/A',
        "Modality Name": modality_params[6],
        "Num Whole Digits": modality_params[0],
        "Decimal Places": modality_params[1],
        "Rand Size": modality_params[2],
        "Cross-Attend": modality_params[3],
        "Convert to Percents": modality_params[4],
        "Num Bins": modality_params[5],
        # Placeholder for original info not available in processed params
        "Original Col Num": "N/A (not in processed params)",
        "Original Has Header": "N/A (not in processed params)"
    }
    modality_configs.append(config)

# --- End of data structure preparation ---


# Write initial run details to output file
output_file_path = project_file_path + 'output/' + output_file_name
if output_file_name != '':
    write_initial_run_details(output_file_path, hyperparams, data_info, modality_configs, run_stats)
    with open(output_file_path, 'a', encoding='utf-8') as f:
        f.write("\n\n--- Evaluation Results ---\n") # Add the header


# Training loop:
best_val_loss = float('inf')  # Initialize best validation loss
patience = 6 #3  # Number of epochs to wait for improvement
epochs_since_improvement = 0  # Track number of epochs without improvement

# Track if the non-numeric data warning has been printed for each modality in this evaluation run
non_numeric_warning_printed_train = [False] * num_modalities
non_numeric_warning_printed_val = [False] * num_modalities

print("Starting training and evaluation loops...")
print("This process involves a lot of computation and can take a considerable amount of time.\n")

for iter in range(max_iters): # the loop iterates for a maximum number of iterations (max_iters)
                              # it periodically estimates the loss and prints it
                              # in each iteration, the loop:
                              # 1. gets a batch of training data (get_batch)
                              # 2. passes the data through the model to get predictions and calculate the loss
                              # 3. updates the model's parameters using the optimizer to minimize the loss

    # Evaluate loss every eval_interval iterations or at the end
    now = datetime.now()
    current_time = now.strftime("%H:%M:%S")
    if iter % 100 == 0 : print(f'Training progress: Iteration {iter} of {max_iters}\n')
    if iter % eval_interval == 0 or iter == max_iters - 1:
        # Pass the warning tracking list to estimate_loss
        print(f"Starting evaluation (step {iter})...")
        losses = estimate_loss()
        now = datetime.now()
        current_time = now.strftime("%H:%M:%S")
        # Check if losses are valid before printing
        if not torch.isnan(torch.tensor([losses['train'], losses['val']])).any():
             print(f"\n=======================================================================================")
             print(f"Step {iter} Summary: Training Loss: {losses['train']:.4f} | Validation Loss: {losses['val']:.4f} | Time: {current_time}")
             print(f"=======================================================================================\n")
             # write to file
             if output_file_name != '':
               with open(output_file_path, 'a', encoding='utf-8') as f:
                   f.write(f"Step {iter} Summary: Training Loss: {losses['train']:.4f} | Validation Loss: {losses['val']:.4f} | Time: {current_time}\n\n")
        else:
             print(f"\n\nStep {iter}: Losses are NaN, skipping print and file write. Current time = {current_time}\n")


        # Early stopping based on validation loss. this is to prevent over fitting
        # if the validation loss doesn't improve for a certain number of iterations (patience), the training process is stopped
        # Only apply early stopping if validation loss is a valid number
        if not torch.isnan(torch.tensor(losses['val']).any()):
            if losses['val'] < best_val_loss:
                best_val_loss = losses['val']
                epochs_since_improvement = 0  # Reset counter if validation loss improves
            else:
                epochs_since_improvement += 1

            if epochs_since_improvement >= patience:
                print(f"Early stopping triggered! Validation loss has not improved for {patience} evaluation intervals.") # Added reason
                break  # Exit the loop
        else:
             print("Validation loss is NaN, skipping early stopping check.")


        # Saving the model's weights to a file (model_file_name)
        now = datetime.now()
        current_time = now.strftime("%H:%M:%S")
        if save_model == 1:
            print(f'Saving model to: {model_file_name}    Current time: {current_time}')
            # When saving, save the state dict of the MultimodalTransformer model
            torch.save(m.state_dict(), model_file_name)
            print("Model size:", round(os.path.getsize(model_file_name)/1024**2,2), "MB\n" )


    # Training steps
    # get_batch returns lists of tensors: [xb_mod1, xb_mod2, ...], [yb_mod1, yb_mod2, ...]
    xb_list, yb_list = get_batch('train', 1)

    # Pass lists of tensors to the multimodal model
    logits_list, losses_list = m(xb_list, yb_list)

    # Calculate total loss by summing modality losses
    # Ensure losses_list is not None and contains tensors before summing
    if losses_list and all(l is not None for l in losses_list):
        total_loss = sum(losses_list)

        optimizer.zero_grad(set_to_none=True)
        total_loss.backward() # Backpropagate the combined loss
        optimizer.step()
    else:
        # Handle cases where losses might not be calculated (e.g., if targets were None, though get_batch for 'train' should provide them)
        print("Warning: Losses not calculated for training step. Skipping backpropagation.")

    '''
    In essence, the training steps above represent a single training iteration where the model:
        1. Receives data,
        2. Makes predictions,
        3. Calculates the error,
        4. Determines how to adjust its parameters to reduce the error, and
        5. Applies those adjustments.
    line 1: gets a batch of training data (get_batch), in the form of input sequences (xb) and their corresponding target outputs (yb)
            these batches are used to train the model in small increments, making the process more efficient and manageable
    line 2: passes the data through the model to get predictions and calculate the loss
            logits_list, losses_list = m(xb_list, yb_list) # Updated to use 'm'
            logits are the model's raw predictions before any final activation function is applied (like softmax for classification)
            the code also calculates a loss value. This loss quantities how far off the model's predictions (logits) are from the actual target values (yb)
    line 3: this line resets any previously calculated gradients to zero
            optimizer.zero_grad(set_to_none=True)
    line 4: this line initiates the backpropagation process. It calculates the gradients of the loss with respect to all the model's trainable parameters
            total_loss.backward() # Backpropagate the combined loss
            (in simpler terms, it figures out how much each parameter contributed to the error (loss) and in which direction the parameter should be adjusted to reduce the error)
    line 5: this line updates the model's parameters using the calculated gradients
            optimizer.step()
            the optimizer (AdamW) takes a step towards minimizing the loss by adjusting the parameters in the direction indicated by the gradients
    '''

"""# Output

## Directional Metrics Explained

Directional metrics (success rate and certainty) are calculated in the `calculate_evaluation_metrics` function to assess how well the model predicts the *direction* of change for numeric data, particularly the last token in a sequence, which is relevant for tasks like predicting stock price movements.

**Applicability:** These metrics are calculated *only* for modalities where the data is numeric and the sequence length is sufficient (at least 1 for percentage changes, at least 2 for value changes).

**Success Rate:**
*   For each batch during evaluation, the function looks at the model's predicted token (based on the highest logit) and the actual target token for the *last* position in the sequence.
*   It then determines the "direction" of change for both the predicted and actual values. This direction is based on:
    *   **For Percentage Data:** Whether the value is positive (up), negative (down), or zero (flat).
    *   **For Value Data:** Whether the current value is greater than (up), less than (down), or equal to (flat) the *previous* value in the sequence.
*   A "win" is counted if the predicted direction matches the actual direction (e.g., both predicted and actual are "up," or both are "down," or both are "flat").
*   A "loss" is counted if the predicted direction does not match the actual direction.
*   These wins and losses are accumulated across all evaluation batches for each applicable modality.
*   The overall directional success rate is reported as the total wins divided by the total movements counted (wins + losses), expressed as a percentage.

**Directional Certainty:**
*   This metric measures the model's confidence in the predicted *direction* for the last token in a sequence.
*   For the last token's logits, a softmax function is applied to get the probability distribution over the entire vocabulary for that modality.
*   The predicted direction is determined based on the token with the highest probability (as in the success rate calculation).
*   The certainty for that prediction is calculated by summing the probabilities of *all* tokens in the vocabulary that fall within that *same predicted direction*.
*   For example, if the model predicts an "up" movement (based on the highest probability token), the directional certainty will be the sum of probabilities for *all* positive values in the vocabulary for percentage data, or the sum of probabilities for all values greater than the previous value for value data.
*   This sum of probabilities is accumulated across all evaluation batches for each applicable modality.
*   The overall average directional certainty is reported as the total accumulated certainty divided by the total number of predictions made (number of evaluation batches \* batch size).

In summary, the success rate tells you how often the model's predicted direction matches the actual direction, while the directional certainty tells you how confident the model is in its predicted direction (by summing probabilities of all outcomes that align with that direction). These metrics are calculated specifically for numeric modalities during the evaluation phase.
"""

# import numpy as np
# from sklearn.cluster import KMeans
# import numbers

# def cluster_numeric_data(data, n_clusters, clust_cap=0):
#   """
#   Clusters a list of numeric data points using KMeans clustering.

#   Args:
#     data: A list of numeric data points (e.g., prices or percentages).
#           All elements must be numeric types.
#     n_clusters: The desired number of clusters. Must be an integer greater than 0.
#     clust_cap: A value to cap the data points at (both positive and negative).
#                If 0, no capping is applied. Must be a non-negative number (default: 0).

#   Returns:
#     A tuple containing:
#     - A list of integers representing the cluster label for each data point,
#       sorted based on the cluster center values.
#     - A list of lists, where each inner list contains the data points
#       belonging to a specific cluster, sorted by cluster center value.

#   Raises:
#     TypeError: If inputs are not of the expected types.
#     ValueError: If inputs have invalid values (e.g., empty data list,
#                 non-positive n_clusters, negative clust_cap, non-numeric data).
#   """

#   # Input validation
#   if not isinstance(data, list):
#     raise TypeError("data must be a list.")
#   if not data:
#     raise ValueError("data must be a non-empty list.")
#   for i, item in enumerate(data):
#       if not isinstance(item, numbers.Number):
#           raise TypeError(f"Element at index {i} in 'data' is not a number.")

#   if not isinstance(n_clusters, int) or n_clusters <= 0:
#     raise ValueError("n_clusters must be a positive integer.")

#   if not isinstance(clust_cap, numbers.Number) or clust_cap < 0:
#       raise ValueError("clust_cap must be a non-negative number.")


#   # Apply capping if clust_cap is greater than 0
#   if clust_cap > 0:
#     capped_data = [min(item, clust_cap) for item in data]
#     capped_data = [max(item, -clust_cap) for item in capped_data]
#   else:
#     capped_data = data

#   # Reshape data for KMeans
#   data_array = np.array(capped_data).reshape(-1, 1)

#   # Perform KMeans clustering
#   kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=10) # Added random_state and n_init for reproducibility
#   kmeans.fit(data_array)

#   # Get cluster labels and centers
#   labels = kmeans.labels_
#   centers = kmeans.cluster_centers_

#   # Sort cluster centers and get sorted indices
#   sorted_indices = np.argsort(centers.flatten())

#   # Create a mapping from original labels to sorted labels
#   label_mapping = {old_label: new_label for new_label, old_label in enumerate(sorted_indices)}

#   # Reassign labels based on the sorted centers
#   sorted_labels = np.array([label_mapping[label] for label in labels])

#   # Create sorted clusters based on new labels
#   sorted_clusters = [[] for _ in range(n_clusters)]
#   for i, label in enumerate(sorted_labels):
#       # Ensure index is within bounds of the original data list
#       if i < len(capped_data):
#           sorted_clusters[label].append(capped_data[i])
#       else:
#           # This case should not happen if sorted_labels has the same length as capped_data
#           print(f"Warning: Index {i} out of bounds for data list in sorted_clusters creation.")


#   # Print cluster information
#   print('\n--- KMeans Clustering Results ---')
#   sorted_cluster_ranges = []
#   sorted_cluster_counts = []
#   for i, cluster in enumerate(sorted_clusters):
#       if cluster:
#           cluster_range = f"Cluster {i}: Range {min(cluster)} - {max(cluster)}"
#           cluster_count = len(cluster)
#       else:
#           cluster_range = f"Cluster {i}: Empty"
#           cluster_count = 0
#       sorted_cluster_ranges.append(cluster_range)
#       sorted_cluster_counts.append(cluster_count)
#       print(f"{cluster_range}, Count: {cluster_count}")
#   print('---------------------------------\n')


#   return sorted_labels.tolist(), sorted_clusters

# def range_prices(prices, num_whole_digits=2, decimal_places=2):
#   """
#   Converts prices to a specified range by scaling them by factors of 10
#   and rounds to a specified number of decimal places.

#   This is done to control/limit the number of unique prices,
#   thereby controlling the vocabulary size. This is helpful when dealing
#   with stocks priced in different price ranges.

#   Args:
#     prices: A list of float prices. Must be a list containing numeric types.
#     num_whole_digits: The desired number of whole digits for the ranged prices
#                       (e.g., 1 for ones, 2 for tens, etc.). Must be an integer (default: 2).
#     decimal_places: The desired number of decimal places for the ranged prices.
#                     Must be an integer greater than or equal to 0 (default: 2).

#   Returns:
#     A list of float prices that have been ranged and rounded.
#   """

#   # Input validation for prices
#   if not isinstance(prices, list):
#       raise TypeError("prices must be a list.")
#   for i, price in enumerate(prices):
#       if not isinstance(price, numbers.Number):
#           # Use IndexError to indicate the position of the problematic element
#           raise IndexError(f"Element at index {i} in 'prices' is not a number.")

#   # Input validation for num_whole_digits and decimal_places
#   if not isinstance(num_whole_digits, int):
#       raise TypeError("num_whole_digits must be an integer.")
#   if not isinstance(decimal_places, int):
#       raise TypeError("decimal_places must be an integer.")
#   if decimal_places < 0:
#       raise ValueError("decimal_places must be an integer greater than or equal to 0.")


#   ranged_prices = []

#   for price in prices:
#     if price == 0:
#       digits = 0
#     else:
#       digits = len(str(int(price)))

#     # Calculate the scaling factor
#     scaling_factor = 10**(digits - num_whole_digits)

#     # Apply scaling and rounding
#     scaled_price = round(price / scaling_factor, decimal_places)

#     # Correct prices that were rounded outside the intended range
#     if scaled_price >= 10**num_whole_digits:
#         scaled_price = 10**(num_whole_digits - 1)

#     ranged_prices.append(scaled_price)

#   return ranged_prices