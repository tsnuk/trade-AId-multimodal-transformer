# -*- coding: utf-8 -*-
"""input_schem_step6_complete_SHORT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19eobni_H-6yzpAFuftANtGylNaGXCkIj

```
# This is formatted as code
```

# New Code Start
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

import pandas as pd
import numpy as np
import numbers
import math

import os
from datetime import datetime
import random
from pathlib import Path
import yaml
import inspect

# GOOGLE COLAB SPECIFIC - COMMENTED OUT FOR LOCAL EXECUTION
# from google.colab import drive
# drive.mount('/content/drive')

# This cell contains helper functions for data preparation and file handling:
# - numerical_representation: Converts data to token IDs and creates vocabulary.
# - create_train_val_datasets: Splits data into training and validation sets.
# - write_initial_run_details: Writes initial run details to an output file.
# - report_non_numeric_error: Reports non-numeric data errors with context.
# - range_numeric_data: Scales and rounds numeric data.
# - bin_numeric_data: Bins numeric data into groups.
# - calculate_percent_changes: Calculates percentage changes.
# - _get_direction_sign: Helper for directional metrics.

# Note: The ModalityConfig dataclass definition was moved to a separate cell (a6ad1ffe)
#       to ensure it's defined before being used in this cell or others.

# Execute this cell to define these functions.

# a6ad1ffe

from dataclasses import dataclass, field
from typing import Optional, List, Any

@dataclass
class ModalityConfig:
    """
    Represents the configuration for a single data modality, including its data source
    and a list of processing steps to apply.
    """
    # Required fields for data source
    path: str
    column_number: int
    has_header: bool

    # Processing steps (list of dictionaries, each specifying a function and args)
    processing_steps: List[Any] = field(default_factory=list)

    # Optional fields not related to specific processing steps (can remain)
    randomness_size: Optional[int] = None # This might be better moved to training config later
    cross_attention: bool = False
    modality_name: Optional[str] = None

    def __bool__(self):
        """
        Allows checking if the ModalityConfig instance is considered "in use"
        similar to how empty lists/dictionaries were checked before.
        An instance is considered in use if it has a valid path, column number,
        and has_header defined.
        """
        return bool(self.path and self.column_number is not None and self.has_header is not None)

def generate_batch_starting_indices(data_size, block_size, batch_size, split, file_lengths, is_percents):
    '''
    Generates a batch of random starting indices for extracting sequences
    of a fixed length (block_size) from the data.

    When dealing with a combined dataset of multiple files or segments,
    this function ensures that the generated indices do not result in sequences
    that cross file or segment boundaries.

    Args:
        data_size: The total size of the dataset for the current split ('train' or 'val').
                   Must be a positive integer.
        block_size: The length of each sequence to be extracted (context window size).
                    Must be a positive integer.
                    Also, block_size must be less than `data_size`.
        batch_size: The number of starting indices to generate (the batch size).
                    Must be a positive integer.
        split: Indicates whether the data is for 'train' or 'val'. Must be 'train' or 'val'.
        file_lengths: A list of integers representing the length of the data
                      from each individual file or segment that makes up
                      the combined dataset for this modality.
                      Must be a list of positive integers.
        is_percents: A boolean indicating whether any modality's data is in percentage form.
                     (if True, the 1st element in each file in each modality, will be skipped when
                     generating starting indices). Applied to all modalities for consistent data lengths.

    Returns:
        torch.Tensor: A tensor of shape (batch_size,) containing the
                      random starting indices within the dataset.

    Raises:
        TypeError: If inputs are not of the expected types.
        ValueError: If inputs have invalid values (e.g., non-positive sizes,
                    invalid 'split' value, empty file_lengths list,
                    block_size >= data_size, or insufficient data to form
                    sequences of block_size).
    '''

    # --- Input Validation ---
    if not isinstance(data_size, int) or data_size <= 0:
        raise TypeError("'data_size' must be a positive integer.")
    if not isinstance(block_size, int) or block_size <= 0:
        raise TypeError("'block_size' must be a positive integer.")
    if block_size >= data_size:
        raise ValueError("'block_size' cannot be equal to or greater than 'data_size'.")
    if not isinstance(batch_size, int) or batch_size <= 0:
        raise TypeError("'batch_size' must be a positive integer.")
    if not isinstance(split, str) or (split != 'train' and split != 'val'):
        raise ValueError("'split' must be 'train' or 'val'.")
    if not isinstance(file_lengths, list) or not len(file_lengths) >= 1:
        raise TypeError("'file_lengths' must be a list containing at least 1 element.")
    if not isinstance(is_percents, bool):
        raise TypeError("'is_percents' must be a boolean.")


    block_size_xy = block_size + 1 # block_size_xy is meant to accommodate for both the input seq's length (block_size) plus the target seq being offset by 1


    if is_percents:
        # The 1st element in each file will be skipped when generating starting indices
        first_element_offset = 1
    else:
        first_element_offset = 0


    if len(file_lengths) == 1:
        # If there's only one continuous dataset (only one file was loaded for this modality),
        # then generate random starting indices ensuring sequences fit within the data (sequence + block_size_xy).
        # Adjust the range to start from 'first_element_offset'
        return torch.randint(first_element_offset, data_size - block_size_xy + 1, (batch_size,))


    if len(file_lengths) > 1:
        # When dealing with a combined dataset of multiple files, we need to ensure sequences don't cross file boundaries.
        dataset_file_lengths = [] # dataset_file_lengths will contain file lengths comprising this data set only (as opposed to file_lengths that contains file lengths of the entire data)
        num_files_loaded = len(file_lengths)
        file_size_accum = 0

        for f in range(num_files_loaded):

            if split == 'train':
                this_file_size = file_lengths[f]

            if split == 'val':
                # Here we're going backwards, starting from the end of file_lengths (because the valuation set is taken from the end of the data set)
                this_file_size = file_lengths[num_files_loaded - 1 - f]

            file_size_accum += this_file_size

            if file_size_accum <= data_size:
                dataset_file_lengths.append(this_file_size)

            if file_size_accum > data_size:
                # Add the portion of the last loaded file making this data set
                dataset_file_lengths.append(data_size - (file_size_accum - this_file_size))

            if file_size_accum >= data_size:
                if split == 'val':
                    # Now we reverse the order of lengths because we accumulated file sizes from the end of file_lengths backwards
                    dataset_file_lengths.reverse()
                break


        # Calculate the total number of valid starting positions across all relevant files
        # A valid starting position for a file of length L is from first_element_offset to L - block_size_xy.
        # Total valid positions in a file of length L is L - block_size_xy - first_element_offset + 1.
        total_valid_ix_positions = sum(max(0, length - block_size_xy - first_element_offset + 1) for length in dataset_file_lengths)


        # Handle the case where there are no valid starting positions
        if total_valid_ix_positions <= 0:
            # This could happen if all files are shorter than or equal to block_size_xy + first_element_offset - 1
            # return torch.empty(0, dtype=torch.long)
            raise ValueError(f"No valid starting positions available for the given block size and file lengths.")


        # Generate initial random indices within the range of total valid positions
        initial_indices = torch.randint(total_valid_ix_positions, (batch_size,))

        # Now, map these initial indices back to the correct starting positions in the
        # combined data, ensuring they fall within the valid range of a specific file.
        actual_indices = torch.empty(batch_size, dtype=torch.long)

        for i in range(batch_size):
            cumulative_valid_ix_positions = 0
            found_position = False

            # Iterate through only the relevant file lengths
            for k, length in enumerate(dataset_file_lengths):
                valid_ix_positions_in_this_file = max(0, length - block_size_xy - first_element_offset + 1) # Ensure non-negative

                if initial_indices[i] < cumulative_valid_ix_positions + valid_ix_positions_in_this_file:
                    # The initial index falls within the valid positions of this file
                    # We need to find its position within the file
                    position_within_file = initial_indices[i] - cumulative_valid_ix_positions
                    # The actual index is the sum of the lengths of previous files (start_of_this_file)
                    # + the position within the valid range of the file (position_within_file)
                    start_of_this_file = sum(dataset_file_lengths[:k])
                    actual_indices[i] = start_of_this_file + position_within_file + first_element_offset

                    found_position = True
                    break

                cumulative_valid_ix_positions += valid_ix_positions_in_this_file

                '''
                if cumulative_valid_ix_positions >= initial_indices[i]:
                    # We're now adding back all the block_size_xys that were omitted from initial_indices (through total_valid_ix_positions)
                    # in order to find the actual index position
                    actual_indices[i] = initial_indices[i] + (block_size_xy * k)
                '''


            if not found_position:
                 # This case should ideally not happen if total_valid_ix_positions was calculated correctly
                 # and initial_indices are within that range
                 raise ValueError(f"Could not map initial index {initial_indices[i]} to a valid ix position.")


        return actual_indices

# Cell sD23rGWUXmUL

# DUPLICATE IMPORTS COMMENTED OUT - Already imported at top of file (lines 16-51)
# import pandas as pd
# import os
# from pathlib import Path
# import numbers
# import math
# import torch
# import torch.nn as nn
# from torch.nn import functional as F
# import random
# from datetime import datetime
# import numpy as np
# from sklearn.cluster import KMeans
#
# from dataclasses import dataclass, field
# from typing import Optional, List, Any


# DUPLICATE CLASS COMMENTED OUT - Use line 54 version instead
# @dataclass
# class ModalityConfig:
#     """
#     Represents the configuration for a single data modality, including its data source
#     and a list of processing steps to apply.
#     """
#     # Required fields for data source
#     path: str
#     column_number: int
#     has_header: bool

#     # Processing steps (list of dictionaries, each specifying a function and args)
#     processing_steps: List[Any] = field(default_factory=list)

#     # Optional fields not related to specific processing steps (can remain)
#     randomness_size: Optional[int] = None # This might be better moved to training config later
#     cross_attention: bool = False
#     modality_name: Optional[str] = None

#     def __bool__(self):
#         """
#         Allows checking if the ModalityConfig instance is considered "in use"
#         similar to how empty lists/dictionaries were checked before.
#         An instance is considered in use if it has a valid path, column number,
#         and has_header defined.
#         """
#         return bool(self.path and self.column_number is not None and self.has_header is not None)


def load_file_data(input_info: ModalityConfig):
    """
    Reads data from a specified file or folder and extracts data from a
    given column. This data will be used to form a single modality for the
    multimodal processing framework. Handles CSV and TXT formats with optional header,
    attempting both comma and semicolon delimiters.

    Args:
        input_info: A ModalityConfig instance containing the modality configuration.

    Returns:
        A tuple containing:
        - A list of the loaded data points (can be of various data types: numeric, string, ...).
        - A list containing the names and lengths of the loaded files:
            [file1_name (str), file1_length (int), file2_name (int), file2_length (int), ...]

    Raises:
        TypeError: If input_info is not a ModalityConfig instance or its elements are not of the expected types.
        ValueError: If the data path is invalid or no supported files are found,
                    or if the specified column does not exist.
        RuntimeError: If an unexpected error occurs during file loading.
        ZeroDivisionError: If attempting to calculate percentage change with a zero value.
    """

    if not isinstance(input_info, ModalityConfig):
        raise TypeError("'input_info' must be a ModalityConfig instance.")

    # Access parameters using data class attributes
    data_path = input_info.path
    num_data_column = input_info.column_number
    has_header = input_info.has_header
    # convert_to_percentages = input_info.convert_to_percentages # Removed as it's now a processing step
    # decimal_places = input_info.decimal_places # Removed as it's now an arg for processing steps
    modality_name = input_info.modality_name


    # Validate required fields and types (basic validation, more comprehensive will be added later)
    if not isinstance(data_path, str):
        raise TypeError(f"Attribute 'path' in 'input_info' must be a string, but got {type(data_path).__name__}.")
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"Path '{data_path}' was not found.")

    if not isinstance(num_data_column, int):
        raise TypeError(f"Attribute 'column_number' in 'input_info' must be an integer, but got {type(num_data_column).__name__}.")
    if num_data_column < 1:
        raise ValueError("The specified data column number must be greater than or equal to 1.")

    if not isinstance(has_header, bool):
        raise TypeError(f"Attribute 'has_header' in 'input_info' must be a boolean, but got {type(has_header).__name__}.")

    # if not (isinstance(convert_to_percentages, bool)): # Removed as it's now a processing step
    #     raise TypeError(f"Attribute 'convert_to_percentages' in 'input_info' must be a boolean, but got {type(convert_to_percentages).__name__}.")

    if not (isinstance(modality_name, str) or modality_name is None):
         raise TypeError(f"Attribute 'modality_name' in 'input_info' must be a string or None, but got {type(modality_name).__name__}.")


    data_file_paths = []
    if os.path.isdir(data_path):
        # Path to a folder
        load_from = "folder"
        data_file_paths = [os.path.join(data_path, f) for f in os.listdir(data_path) if os.path.isfile(os.path.join(data_path, f)) and (f.endswith('.csv') or f.endswith('.txt'))]
        if not data_file_paths:
            raise ValueError(f"No CSV or TXT files found in folder '{data_path}'.")

    elif os.path.isfile(data_path):
        # Path to a file
        load_from = "file"
        if not (data_path.endswith('.csv') or data_path.endswith('.txt')):
            raise ValueError(f"The specified file '{data_path}' is not a CSV or TXT file.")
        data_file_paths.append(data_path)

    else:
         # This case should be caught by os.path.exists, but added for completeness
         raise FileNotFoundError(f"Path '{data_path}' is neither a file nor a directory.")


    # Read the datafile/s
    loaded_data = []
    data_info = [] # This list stores file names and lengths for this modality

    data_name_from_path = Path(data_path).name
    print(f"  Loading data from {load_from}: '{data_name_from_path}'")


    for full_path in data_file_paths:
        filename = os.path.basename(full_path)
        df = pd.DataFrame() # Initialize empty DataFrame
        read_successful = False

        # Try reading with comma delimiter first
        try:
            df = pd.read_csv(full_path, delimiter=',', engine='python', header=None, skiprows=1 if has_header else 0)
            if not df.empty:
                read_successful = True
                # print(f'  Successfully read file with comma delimiter: {filename}') # Optional: add for debugging
        except (pd.errors.EmptyDataError, pd.errors.ParserError, Exception) as e:
            last_error = e # Store the last error

        # If not successful, try reading with semicolon delimiter
        if not read_successful:
            try:
                df = pd.read_csv(full_path, delimiter=';', engine='python', header=None, skiprows=1 if has_header else 0)
                if not df.empty:
                    read_successful = True
                    # print(f'  Successfully read file with semicolon delimiter: {filename}') # Optional: add for debugging
            except (pd.errors.EmptyDataError, pd.errors.ParserError, Exception) as e:
                last_error = e # Store the last error


        # If after trying both delimiters, the DataFrame is still empty or read was not successful
        if not read_successful or df.empty:
            error_message = f"Failed to load data from file '{filename}' after trying both comma and semicolon delimiters."
            if 'last_error' in locals(): # Check if an error was caught
                error_message += f" Last error: {last_error}"
            print(error_message)
            # Raise a more specific error type if possible, e.g., pd.errors.EmptyDataError or pd.errors.ParserError
            if 'last_error' in locals() and isinstance(last_error, (pd.errors.EmptyDataError, pd.errors.ParserError)):
                 raise last_error
            else:
                 raise RuntimeError(error_message)


        if num_data_column > df.shape[1]:
            raise ValueError(f"The specified data column ({num_data_column}) does not exist in file '{filename}'. File has {df.shape[1]} columns.")

        column_data = df.iloc[:, num_data_column - 1]

        # Convert column data to a list
        column_data_list = column_data.tolist()

        # Percentage conversion is now handled as a processing step after loading,
        # so remove the conditional logic here.
        # if convert_to_percentages is True:
        #      # Check if data is numeric before calculating percentages
        #      data_is_numeric = all(isinstance(item, numbers.Number) for item in column_data_list)
        #      if not data_is_numeric:
        #           # Find and report the non-numeric element
        #           print(f"\nError: Percentage calculation specified for Modality '{modality_name if modality_name else data_name_from_path}' from file '{filename}', but data is not entirely numeric.")
        #           # Create temporary file_info for reporting the error location within this file
        #           temp_file_info = [filename, len(column_data_list)]
        #           # Call report_non_numeric_error to provide details and raise ValueError
        #           report_non_numeric_error(column_data_list, temp_file_info, f"{modality_name if modality_name else data_name_from_path}")
        #           # report_non_numeric_error raises ValueError, so the loop will stop

        #      else:
        #           # Calculate percentage changes and extend the loaded_data list
        #           try:
        #               # Pass the ModalityConfig instance to calculate_percent_changes
        #               percentage_changes = calculate_percent_changes(column_data_list, input_info)
        #               loaded_data.extend(percentage_changes)
        #               # Store the file name and length of the extracted data (percentage changes have same length)
        #               data_info.append(filename)
        #               data_info.append(len(percentage_changes)) # length of percentage changes is same as original column_data
        #               print(f'  Successfully extracted data from column {num_data_column} of file: {filename}, data length:{len(percentage_changes)}')

        #           except ZeroDivisionError as e:
        #               # Catch and re-raise ZeroDivisionError with more context
        #               raise ZeroDivisionError(f"Error processing file '{filename}': {e}") from e
        #           except Exception as e:
        #                # Catch other potential errors during percentage calculation
        #                raise RuntimeError(f"An unexpected error occurred during percentage calculation for file '{filename}': {e}") from e

        # else:
        # If not calculating percentages, just extend the loaded_data list
        loaded_data.extend(column_data_list)
        # Store the file name and length of the extracted data
        data_info.append(filename)
        data_info.append(len(column_data_list))
        print(f'  Successfully extracted data from column {num_data_column} of file: {filename}, data length:{len(column_data_list)}')


    if not loaded_data:
        raise ValueError(f"No data was successfully loaded from the path '{data_path}' with the specified criteria.")


    # Use modality_name for print message if available, otherwise use data_name_from_path
    display_modality_name = modality_name if isinstance(modality_name, str) else data_name_from_path
    print(f"\n\n  Data loading for Modality '{display_modality_name}' complete!\n")
    print(f"  Number of files loaded: {len(data_file_paths)}")
    print(f"  Total data length: {len(loaded_data)}")

    # Percentage conversion print message is now handled in the processing step
    # if convert_to_percentages is True:
    #     print(f"  + Data converted to percent changes")

    # Print vocabulary size (num of unique elements) - This will be done after processing steps
    # vocabulary = list(set(loaded_data))
    # print(f'  Vocabulary size (unique elements): {len(vocabulary)}')

    # Print first / last elements - This will be done after processing steps
    # if len(loaded_data) >= 10:
    #     print('  Dataset first / last elements:\n', '', *loaded_data[:5], '...', *loaded_data[-5:])


    # Check whether loaded_data is numeric, and if so, print additional data - This will be done after processing steps
    # all_numbers = True
    # for i, data in enumerate(loaded_data):
    #     if not isinstance(data, numbers.Number):
    #         all_numbers = False
    #         break

    # if all_numbers:
    #     print(f'  Min element: {min(loaded_data)}')
    #     print(f'  Max element: {max(loaded_data)}')


    return loaded_data, data_info


def report_non_numeric_error(data_list, file_info, this_modality):
    """
    Finds the first non-numeric element in a data list and raises a ValueError,
    reporting its location, including the file name and approximate element index within that file,
    as well as the element's value and type.

    Args:
        data_list: A list of data points.
        file_info: A list containing file names and their corresponding data lengths
                   in the format [file1_name, data1_length, file2_name, data2_length, ...].
        this_modality: An integer representing the 1-based index of the modality,
                       or a string representing the name of the modality.

    Raises:
        ValueError: If a non-numeric element is found in the data_list.
    """
    first_non_numeric_index = -1
    non_numeric_value = None
    non_numeric_type = None

    for idx, item in enumerate(data_list):
        if not isinstance(item, numbers.Number):
            first_non_numeric_index = idx
            non_numeric_value = item
            non_numeric_type = type(item).__name__
            break

    if first_non_numeric_index != -1:
        # Determine which file the non-numeric element came from
        current_total_length = 0
        file_name = "Unknown File"
        element_index_in_file = first_non_numeric_index

        # file_info is [file1_name, data1_length, file2_name, data2_length, ...]
        for f_idx in range(0, len(file_info), 2):
            current_file_name = file_info[f_idx]
            current_file_length = file_info[f_idx+1]
            if first_non_numeric_index < current_total_length + current_file_length:
                file_name = current_file_name
                element_index_in_file = first_non_numeric_index - current_total_length
                break
            current_total_length += current_file_length

        # Format the modality identifier for the error message
        # Updated to be more descriptive with modality name
        modality_identifier = f"Modality {this_modality}" if isinstance(this_modality, int) else f"Modality '{this_modality}'"


        raise ValueError(
            f"Non-numeric data found in {modality_identifier} at overall index {first_non_numeric_index} "
            f"(approximately element {element_index_in_file} in file '{file_name}'). "
            f"Element value: '{non_numeric_value}', Element type: {non_numeric_type}. "
            "Data must be entirely numeric for ranging or decimal places processing."
        )
    # Note: If no non-numeric is found, the function will simply return without raising an error.


def range_numeric_data(numeric_data, modality_params: ModalityConfig, num_whole_digits=None, decimal_places=None):
  """
  Converts numeric data to a specified range by scaling them by factors of 10
  and/or rounds to a specified number of decimal places.

  The purpose is to standardize data magnitude and precision, thereby controlling
  vocabulary size.

  This function scales numeric values to a range defined by `num_whole_digits`
  (including 0 and negative for scaling down) and rounds to `decimal_places`
  (or original precision if None), preserving the sign of negative numbers.

  Args:
    numeric_data: A list of numeric data points. Must be a list containing numeric types.
                  The data may contain values that are positive, zero, or negative.
                  (zero, or negative values could be expected with data types like percentages).
    modality_params: A ModalityConfig instance (used primarily for accessing modality_name for printing).
    num_whole_digits: The desired number of whole digits for the ranged prices
                      (e.g., 1 for ones, 2 for tens, etc.). Must be an integer or None (default: None).
    decimal_places: The desired number of decimal places for the ranged prices.
                    Must be an integer greater than or equal to 0 or None (default: None).

  Returns:
    A list of float values that have been ranged and/or rounded.

  Raises:
    TypeError: If inputs are not of the expected types.
    ValueError: If inputs have invalid values (e.g., empty list,
                negative decimal_places if not None, non-numeric data).
    IndexError: If an element in 'numeric_data' is not a number.
  """

  # Access modality name from ModalityConfig instance
  modality_name = modality_params.modality_name # Get modality name for printing

  # Input validation
  if not isinstance(numeric_data, list):
      raise TypeError("'numeric_data' must be a list.")
  if not numeric_data:
      raise TypeError("'numeric_data' must be a non-empty list.")

  for i, element in enumerate(numeric_data):
      if not isinstance(element, numbers.Number):
          raise IndexError(f"Element at index {i} in 'numeric_data' is not a number.")

  if num_whole_digits is not None and not isinstance(num_whole_digits, int):
      raise TypeError("'num_whole_digits' must be an integer or None.")

  if decimal_places is not None and not isinstance(decimal_places, int):
      raise TypeError("'decimal_places' must be an integer or None.")
  if decimal_places is not None and decimal_places < 0:
      raise ValueError("'decimal_places' must be greater than or equal to 0.")


  processed_data = []
  has_negative_values = any(element < 0 for element in numeric_data)

  apply_dec_places_for_print_range = 0
  if decimal_places is not None and decimal_places >= 0:
      apply_dec_places_for_print_range = decimal_places
  elif numeric_data:
      s = str(numeric_data[0])
      if '.' in s:
          decimal_part = s.split('.')[-1]
          apply_dec_places_for_print_range = len(decimal_part)

  for element in numeric_data:
      if element == 0:
          power_of_10 = 0
      else:
          abs_element = abs(element)
          if abs_element == 0:
              power_of_10 = 0
          else:
              power_of_10 = int(math.floor(math.log10(abs_element)))

      apply_dec_places = decimal_places if decimal_places is not None else (len(str(element).split('.')[-1]) if '.' in str(element) else 0)
      apply_dec_places = max(0, apply_dec_places)


      scaling_factor = 1
      if num_whole_digits is not None:
          desired_power_of_10 = num_whole_digits - 1
          scaling_factor = 10**(desired_power_of_10 - power_of_10)


      scaled_value = round(element * scaling_factor, apply_dec_places) if scaling_factor != 0 else 0.0

      if num_whole_digits is not None:
           lower_bound_abs = 10**(num_whole_digits - 1)
           upper_bound_abs_compare = 10**num_whole_digits

           abs_scaled_value = abs(scaled_value)

           if abs_scaled_value < lower_bound_abs and abs_scaled_value > 0:
               abs_scaled_value = lower_bound_abs
           if apply_dec_places > 0:
               if abs_scaled_value >= upper_bound_abs_compare:
                  abs_scaled_value = upper_bound_abs_compare - (10**(-apply_dec_places))
           else:
               if abs_scaled_value >= upper_bound_abs_compare:
                  abs_scaled_value = 10**num_whole_digits - 1

           scaled_value = abs_scaled_value * (-1 if element < 0 else 1)


      processed_data.append(scaled_value)


  # Use modality_name for print message if available, otherwise use a generic name
  display_modality_name = modality_name if isinstance(modality_name, str) else "Modality"

  # Print ranging information
  if num_whole_digits is not None:
      lower_bound_print = 10**(num_whole_digits - 1)
      upper_bound_print = 10**num_whole_digits - (10**(-apply_dec_places_for_print_range) if apply_dec_places_for_print_range > 0 else 1)

      range_str = f'{lower_bound_print:.{apply_dec_places_for_print_range}f} to {upper_bound_print:.{apply_dec_places_for_print_range}f}'
      prefix = '\u00B1 ' if has_negative_values else ''

      print(f"  + Data scaled to a range of: {prefix}{range_str} ('whole digits' is {num_whole_digits}) for {display_modality_name}")
  else:
      print(f"  - No ranging specified ('whole digits' is None) for {display_modality_name}")


  # Print rounding information
  if decimal_places is not None:
      print(f'  + Values have been rounded to: {decimal_places} decimal places for {display_modality_name}')
  else:
      print(f"  - No rounding specified ('decimal places' is None) for {display_modality_name}")


  vocabulary = list(set(processed_data))
  print(f'  New vocabulary size for {display_modality_name}: {len(vocabulary)}')


  # Print first and last elements of processed data
  if len(processed_data) >= 10:
      print(f'  Dataset first / last elements (processed data for {display_modality_name}):\n', '', end=' ')
      for val in processed_data[:5]:
           print(f'{int(round(val)) if decimal_places == 0 and abs(val - round(val)) < 1e-9 else val}', end=' ')
      print('...', end=' ')
      for val in processed_data[-5:]:
           print(f'{int(round(val)) if decimal_places == 0 and abs(val - round(val)) < 1e-9 else val}', end=' ')
      print()
  elif processed_data:
      print(f'  Processed data for {display_modality_name}:', end=' ')
      for val in processed_data:
          print(f'{int(round(val)) if decimal_places == 0 and abs(val - round(val)) < 1e-9 else val}', end=' ')
      print()


  if processed_data:
      min_val = min(processed_data)
      max_val = max(processed_data)
      min_print = int(round(min_val)) if decimal_places == 0 and abs(min_val - round(min_val)) < 1e-9 else min_val
      max_print = int(round(max_val)) if decimal_places == 0 and abs(max_val - round(max_val)) < 1e-9 else max_val
      print(f'  Min element (processed) for {display_modality_name}: {min_print}')
      print(f'  Max element (processed) for {display_modality_name}: {max_print}') # Corrected typo here


  return processed_data

def bin_numeric_data(data, modality_params: ModalityConfig, num_bins=None, outlier_percentile=5, exponent=2.0):
    """
    Divides a list of numeric data into a specified number of groups with
    non-uniform ranges, based on an exponential-like distribution, after
    removing outliers using percentiles, handling both positive and negative values symmetrically.

    Args:
        data: A list of numeric data points.
        modality_params: A ModalityConfig instance containing binning parameters.
        num_bins: The desired number of bins for the data. Must be an integer > 0 or None.
                  (default: None, expects to be passed from config)
        outlier_percentile: The percentile to use for removing outliers.
                            Values below this percentile and above (100 - this percentile)
                            will be excluded from the range calculation for both positive
                            and absolute negative values. Must be between 0 and 50.
                            (default: 5, removing values below 5th and above 95th percentile).
        exponent: The exponent to control the non-linear division of the range.
                  A value > 1 creates smaller ranges closer to zero and larger ranges further away.
                  A value = 1 creates uniform ranges. Must be a number >= 1. (default: 2.0).

    Returns:
        A list of integers, where each integer represents the group assignment
        (ranging from -num_groups to num_groups, with 0 for values near zero)
        for the corresponding data point in the original data list.

    Raises:
        TypeError: If inputs are not of the expected types.
        ValueError: If inputs have invalid values (e.g., empty data list,
                    non-positive num_groups, invalid outlier_percentile, invalid exponent).
    """
    # Access num_bins, outlier_percentile, exponent from direct arguments, not ModalityConfig
    # num_groups = modality_params.num_bins # Removed
    num_groups = num_bins # Use the direct argument

    modality_name = modality_params.modality_name # Get modality name for printing


    # Input validation
    if not isinstance(data, list) or not data:
        raise ValueError("'data' must be a non-empty list.")
    for i, item in enumerate(data):
        if not isinstance(item, numbers.Number):
            raise TypeError(f"Element at index {i} in 'data' is not a number.")

    if not isinstance(num_groups, int) or num_groups <= 0:
        # Note: This check relies on num_bins being a valid integer in the ModalityConfig
        raise ValueError("'num_bins' (passed as argument) must be a positive integer.")

    if not isinstance(outlier_percentile, numbers.Number) or not (0 <= outlier_percentile <= 50):
         raise ValueError("'outlier_percentile' must be a number between 0 and 50.")

    if not isinstance(exponent, numbers.Number) or exponent < 1:
        raise ValueError("'exponent' must be a number greater than or equal to 1.")

    # Separate positive, negative, and zero values
    positive_data = [x for x in data if x > 0]
    negative_data = [x for x in data if x < 0]
    # zero_data_indices = [i for i, x in enumerate(data) if x == 0] # This is not used

    # Convert to numpy arrays for percentile calculation
    positive_np = np.array(positive_data) if positive_data else np.array([])
    negative_abs_np = np.abs(np.array(negative_data)) if negative_data else np.array([])

    # Determine effective maximum absolute value considering outliers
    effective_max_abs = 0.0
    if positive_np.size > 0:
        effective_max_positive = np.percentile(positive_np, 100 - outlier_percentile)
        effective_max_abs = max(effective_max_abs, effective_max_positive)

    if negative_abs_np.size > 0:
        effective_max_negative_abs = np.percentile(negative_abs_np, 100 - outlier_percentile)
        effective_max_abs = max(effective_max_abs, effective_max_negative_abs)

    # Handle case where effective_max_abs is very small or zero
    if effective_max_abs <= 1e-9: # Use a small tolerance for near-zero
        # Use modality_name for print message if available, otherwise use a generic name
        display_modality_name = modality_name if isinstance(modality_name, str) else "Modality"
        print(f"Warning: Effective max absolute value is near zero for {display_modality_name}. All data will be assigned to group 0.")
        # Assign all data to group 0 and return
        group_assignments = [0] * len(data)
        vocabulary = list(set(group_assignments))
        print(f'  New Vocabulary size (populated bins) for {display_modality_name}: {len(vocabulary)}')
        print(f"  Group 0 Count for {display_modality_name}:", len(data), "data points")
        return group_assignments # Return the list of zeros


    # Generate positive group boundaries
    positive_group_boundaries = [0.0] # Start from zero
    for i in range(1, num_groups + 1):
        normalized_i = i / num_groups
        scaled_position = normalized_i**exponent
        boundary = effective_max_abs * scaled_position
        positive_group_boundaries.append(boundary)

    # Ensure the last positive boundary is the true maximum absolute value
    true_max_abs = max(np.max(positive_np) if positive_np.size > 0 else 0,
                       np.max(negative_abs_np) if negative_abs_np.size > 0 else 0)
    # Adjust the last boundary slightly if it's exactly the true max to ensure inclusion
    if true_max_abs > 0:
        positive_group_boundaries[-1] = true_max_abs + (true_max_abs * 1e-9) # Add a small tolerance


    # Generate negative group boundaries (symmetric to positive)
    # Reverse the positive boundaries and negate them
    negative_group_boundaries = [-b for b in reversed(positive_group_boundaries)]


    # Assign data points to groups
    group_assignments = [0] * len(data) # Initialize with 0 for zero values

    for i, value in enumerate(data):
        if value > 0:
            # Find the group index for positive values
            group_index = 0
            for j in range(num_groups):
                 # Check if value is within the boundary range [boundary_low, boundary_high)
                 if value >= positive_group_boundaries[j] and value < positive_group_boundaries[j+1]:
                      group_index = j + 1 # Group numbers 1 to num_groups
                      break
            # Handle the case where the value is exactly on the upper boundary (should go to the last group)
            if value == positive_group_boundaries[-1]:
                 group_index = num_groups


            group_assignments[i] = group_index

        elif value < 0:
            # Find the group index for negative values
            group_index = 0
            # Iterate through negative boundaries (from most negative towards zero)
            # Note: negative_group_boundaries is in increasing order (e.g., [-100, -50, -20, 0])
            for j in range(num_groups):
                 # Check if value is within the boundary range [boundary_low, boundary_high)
                 if value >= negative_group_boundaries[j] and value < negative_group_boundaries[j+1]:
                       # Map index j to group number -num_groups to -1
                       group_index = -(num_groups - j)
                       break
            # Handle the case where the value is exactly on the first negative boundary (should go to the first group)
            if value == negative_group_boundaries[0]:
                 group_index = -num_groups


            group_assignments[i] = group_index


    # Use modality_name for print message if available, otherwise use a generic name
    display_modality_name = modality_name if isinstance(modality_name, str) else "Modality"

    # Print binning info
    print(f"  Data binned into the following positive, negative, and zero groups ('num groups' is {num_groups}) for {display_modality_name}:\n")

    # Combine and print group boundaries/descriptions and counts
    # Negative Groups
    for i in range(num_groups):
         group_label = -(num_groups - i)
         boundary_low = negative_group_boundaries[i]
         boundary_high = negative_group_boundaries[i+1]
         group_count = group_assignments.count(group_label)
         if i == 0: # For the lowest group, show "and below"
             print(f"  Group {group_label}:  [{boundary_high:.2f} and below)  Count: {group_count}")
         else:
             print(f"  Group {group_label}:  [{boundary_low:.2f}, {boundary_high:.2f})  Count: {group_count}")


    # Zero Group
    group_label = 0
    zero_count = group_assignments.count(group_label)
    print(f"\n  Group {group_label}:  Values equal to zero.  Count: {zero_count}\n")

    # Positive Groups
    for i in range(num_groups):
         group_label = i + 1
         boundary_low = positive_group_boundaries[i]
         boundary_high = positive_group_boundaries[i+1]
         group_count = group_assignments.count(group_label)
         if i == num_groups - 1: # For the highest group, show "and above"
              print(f"  Group {group_label}:  [{boundary_low:.2f} and above]  Count: {group_count}")
         else:
              print(f"  Group {group_label}:  [{boundary_low:.2f}, {boundary_high:.2f})  Count: {group_count}")


    total_assigned = sum(group_assignments.count(i) for i in range(-num_groups, num_groups + 1))
    if total_assigned != len(data):
        print(f"Warning: Total assigned data points ({total_assigned}) does not match input data length ({len(data)}) for {display_modality_name}.")


    # Print new vocab size
    vocabulary = list(set(group_assignments))
    print(f'\n  New vocabulary size (populated bins) for {display_modality_name}: {len(vocabulary)}')


    # Print first and last elements of binned data
    if len(group_assignments) >= 10:
        print(f'  Dataset first / last elements (binned data for {display_modality_name}):\n', '', end=' ')
        # Determine decimal places for printing based on original data (if applicable) or default
        decimal_places_for_print = 0
        if data and isinstance(data[0], float):
             s = str(data[0])
             if '.' in s:
                 decimal_part = s.split('.')[-1]
                 decimal_places_for_print = len(decimal_part)

        for val in group_assignments[:5]:
            print(f'{val}', end=' ') # Binned data is already integer
        print('...', end=' ')
        for val in group_assignments[-5:]:
            print(f'{val}', end=' ') # Binned data is already integer
        print()
    elif group_assignments:
        print(f'  Processed data for {display_modality_name}:', end=' ')
        for val in group_assignments:
            print(f'{val}', end=' ') # Binned data is already integer
        print()


    if group_assignments:
      min_val = min(group_assignments)
      max_val = max(group_assignments)
      print(f'  Min element (binned) for {display_modality_name}: {min_val}')
      max_val_print = int(round(max_val)) if abs(max_val - round(max_val)) < 1e-9 else max_val
      print(f'  Max element (binned) for {display_modality_name}: {max_val_print}')


    return group_assignments

def calculate_percent_changes(data, modality_params: ModalityConfig, decimal_places=None):
    """
    Calculates the percentage change between adjacent numeric data points
    and returns a list of the same length by prepending a 0.

    Args:
        data: A list of numeric data points. Must be a list containing numeric types.
              The data may contain values that are positive, zero, or negative.
        modality_params: A ModalityConfig instance (can be used for future extensions,
                         but specific processing args are passed directly).
        decimal_places: The desired number of decimal places for the percentage changes.
                        Must be an integer greater than or equal to 0 or None (default: None).

    Returns:
        A list of float percentage changes, starting with 0, with the same length as
        the input data list.

    Raises:
        TypeError: If inputs are not of the expected types.
        ValueError: If inputs have invalid values (e.g., empty data list,
                    negative decimal_places if not None).
        ZeroDivisionError: If an attempt is made to divide by zero when calculating
                           percentage change.
    """
    # Access decimal_places from the direct argument, not ModalityConfig
    # decimal_places = modality_params.decimal_places # Removed

    # Input validation
    if not isinstance(data, list) or not data:
        raise ValueError("'data' must be a non-empty list.")
    for i, item in enumerate(data):
        if not isinstance(item, numbers.Number):
            raise TypeError(f"Element at index {i} in 'data' is not a number.")

    # Validate decimal_places: must be int >= 0 or None
    if decimal_places is not None and not isinstance(decimal_places, int):
        raise TypeError("'decimal_places' must be an integer or None.")
    if decimal_places is not None and decimal_places < 0:
        raise ValueError("'decimal_places' must be greater than or equal to 0.")

    # Determine the actual number of decimal places to use for rounding
    actual_decimal_places = decimal_places if decimal_places is not None else 2


    percent_changes = [0.0] # Prepend 0 as the first element (in order to keep the processed data at the same length as the input data)
                            # (this element will later be skipped over when generating batch starting indices)

    for i in range(len(data) - 1):
        current_value = data[i]
        next_value = data[i+1]

        if current_value == 0:
            # Handle division by zero appropriately. Depending on the data,
            # Here, we'll raise an error to alert the user.
            raise ZeroDivisionError(f"Cannot calculate percentage change: division by zero at index {i}. Current value is 0.")

        percent_change = ((next_value - current_value) / current_value) * 100.0
        percent_changes.append(round(percent_change, actual_decimal_places))

    # Ensure the returned list has the same length as the input list
    if len(percent_changes) != len(data):
        # This should not happen with the current logic, but as a safeguard
        print(f"Warning: Returned list length ({len(percent_changes)}) does not match input list length ({len(data)}).")

    return percent_changes

# Cell x-iRMxObxBuj

def numerical_representation(data_points):
  """
  Converts a list of data points (numeric or other types) into a numerical
  representation by mapping each unique element to an integer index.

  Args:
    data_points: A list of data points. Can be numeric or other types.

  Returns:
    A tuple containing:
    - A list of integers representing the numerical representation of the input data.
    - A list of the unique elements (vocabulary) sorted in ascending order.
  """

  # Create vocabulary of unique elements
  vocabulary = sorted(list(set(data_points)))

  # Map elements to indices
  data_mapping = {element: index for index, element in enumerate(vocabulary)}

  # Transform data_points to its numerical representation
  transformed_data = [data_mapping[element] for element in data_points]

  return transformed_data, vocabulary

# Cell XcX9D3RVyLU3

def create_train_val_datasets(numeric_rep_data, val_size, num_val_files, file_lengths):
    """
    Splits a combined list of numerical data into training and validation datasets.

    The splitting is done based on either a specified percentage of the total data
    or by allocating a specified number of the *last* data files loaded
    to the validation set.

    Args:
        numeric_rep_data: A list of numerical data representing prices or
                          other types of data, already converted to their numerical
                          representation (e.g., token IDs). This is the combined data
                          from all loaded files for a single modality.
        val_size: A float between 0.1 and 0.3 representing the percentage
                  of the combined data to be used for validation. This is only used if
                  `num_val_files` is 0.
        num_val_files: An integer specifying the number of the *last* files
                       loaded for this modality to be used entirely for the validation set.
                       If 0, `val_size` is used for splitting. Must be a non-negative integer.
                       If greater than 0, it must be less than the total number of files loaded.
        file_lengths: A list of integers representing the length of the data
                      from each individual file or segment that makes up
                      the combined dataset for this modality.
                      Must be a list of positive integers.

    Returns:
        A tuple containing:
        - train_dataset: The list containing the training data. This list will be converted
                         to a tensor at a later stage (in the get_batch function).
        - val_dataset: The tensor containing the validation data.

    Raises:
        TypeError: If inputs are not of the expected types.
        ValueError: If inputs have invalid values (e.g., inconsistent lengths,
                    invalid val_size, invalid num_val_files).
    """

    if not isinstance(numeric_rep_data, (list)):
        raise TypeError("'numeric_rep_data' must be a list.")

    if not isinstance(num_val_files, int) or num_val_files < 0:
        raise TypeError("'num_val_files' must be an integer equal to or larger than 0.")

    if not isinstance(file_lengths, list) or not len(file_lengths) >= 1:
        raise TypeError("'file_lengths' must be a list containing at least 1 element.")

    total_files_loaded = len(file_lengths)

    if num_val_files > 0 and num_val_files >= total_files_loaded:
        # Here we verify that num_val_files is smaller than the number of files that were uploaded (so as to leave some files for the train set)
        raise ValueError(f"'num_val_files' ({num_val_files}) must be smaller than the total number of files uploaded ({total_files_loaded}).")


    val_num_elements = 0
    train_num_elements = 0


    # Train and val set sizes determined by 'num_val_files'
    if num_val_files > 0:
        # Sum the lengths of the last 'num_val_files' file_lengths for the validation set
        start_index = len(file_lengths) - 1
        for j in range(num_val_files):
            val_num_elements += file_lengths[start_index - j]

        train_num_elements = len(numeric_rep_data) - val_num_elements


    # Train and val set sizes determined by 'val_size'
    else:
        if not isinstance(val_size, float):
            raise TypeError("'val_size' must be a number between 0.1 and 0.3.")
        elif val_size > 0.3 or val_size < 0.1:
            raise ValueError("'val_size' must be a number between 0.1 and 0.3.")
        else:
            val_num_elements = int(val_size * len(numeric_rep_data))
            train_num_elements = len(numeric_rep_data) - val_num_elements


    train_dataset = numeric_rep_data[:train_num_elements]
    val_dataset = torch.tensor(numeric_rep_data[train_num_elements:], dtype=torch.long)


    return train_dataset, val_dataset

def write_initial_run_details(file_path, hyperparams, data_info, modality_configs, run_stats):
    """
    Writes the initial run details (hyperparameters, data info, modality configs)
    to the specified output file.

    Args:
        file_path (str): The full path to the output file.
        hyperparams (dict): A dictionary containing the model hyperparameters.
        data_info (dict): A dictionary containing general data information (e.g., split sizes).
        modality_configs (list): A list of dictionaries, where each dictionary
                                 contains the configuration details for a modality.
        run_stats (dict): A dictionary containing overall run statistics (e.g., number of parameters).
    """
    if file_path: # Only write if a file path is provided
        with open(file_path, 'a', encoding='utf-8') as f:
            now = datetime.now()
            current_time_date = now.strftime("%Y-%m-%d %H:%M:%S")
            f.write(f"\n\n{current_time_date}\n")
            f.write("\nModel Settings and Data Information:\n")

            # Write Hyperparameters
            f.write("Hyperparameters:\n")
            for key, value in hyperparams.items():
                f.write(f"  {key}: {value}\n")
            f.write("\n")

            # Write Run Statistics
            f.write("Run Statistics:\n")
            for key, value in run_stats.items():
                f.write(f"  {key}: {value}\n")
            f.write("\n")


            # Write Data Information
            f.write("Data Information:\n")
            for key, value in data_info.items():
                 f.write(f"  {key}: {value}\n")
            f.write("\n")

            # Write Input Schemas/Modality Configurations
            f.write("Input Schemas (Modality Configurations):\n")
            for i, config in enumerate(modality_configs):
                f.write(f"  Modality {i+1}:\n")
                for key, value in config.items():
                    f.write(f"    {key}: {value}\n")
            f.write("\n")

# Create a sample YAML configuration file for hyperparameters and other initial parameters

# DUPLICATE IMPORTS COMMENTED OUT - Already imported at top of file
# import yaml
# import os # Import os to check for file existence
# from google.colab import drive # Assuming drive is already mounted - Uncomment if needed for drive access here

# Define the structure of the configuration data with explanations as comments
config_data = {
    'project_settings': {
        # Path to the main project folder in Google Drive. All other file paths can be relative to this path after loading.
        'project_file_path': 'C:/Users/tsalo/gpt_proj_6/multimodal_1/', # Local path for PyCharm
        # Name for the training output file (e.g., "training_log.txt"). Set to an empty string "" to disable logging.
        'output_file_name': 'training_log.txt',
        # Full path for the model file (e.g., "/content/drive/My Drive/Tal_Erez_shared_folder/output/TransformerModel.pth"). This is where the model weights will be saved/loaded.
        'model_file_name': 'C:/Users/tsalo/gpt_proj_6/multimodal_1/output/TransformerModel.pth',
        # Set to 1 to create a new model and start training from scratch. Set to 0 to attempt to load a model from model_file_name.
        'create_new_model': 1,
        # Set to 1 to save the model's parameters periodically during training and at the end. Set to 0 to disable saving.
        'save_model': 1,
        # The device to use for training and inference ("cuda" for GPU if available, "cpu" for CPU).
        'device': 'cuda' # or 'cpu'
    },
    'data_splitting': {
        # Method for splitting data into training and validation sets.
        # Use validation_size for a percentage split (0.0 to 1.0). num_validation_files will be ignored.
        'validation_size': 0.1,
        # Use num_validation_files to use a specific number of files from the end of the dataset for validation. validation_size will be ignored if this is > 0.
        'num_validation_files': 0
    },
    'training_parameters': {
        # The number of sequences processed in parallel in each training batch.
        'batch_size': 64,
        # The maximum sequence length (number of tokens) the model will process at once.
        'block_size': 256,
        # The total number of training iterations (batches) to run.
        'max_iters': 5000,
        # How often (in training iterations) to evaluate the model on the validation set and report loss.
        'eval_interval': 500,
        # The learning rate for the optimizer.
        'learning_rate': 3e-4,
    },
    'model_architecture': {
        # The dimensionality of the token embeddings and the internal representation.
        'n_embd': 384,
        # The number of attention heads in the MultiHeadAttention layers.
        'n_head': 6,
        # The number of transformer blocks (layers).
        'n_layer': 6,
        # The dropout rate used for regularization.
        'dropout': 0.2
    }
}

# Define the path for the YAML file
# Using a default path for now, will update to use project_file_path after loading config
yaml_file_path = 'C:/Users/tsalo/gpt_proj_6/multimodal_1/config.yaml' # Local path for PyCharm

# Print the path where the file is expected to be written
print(f"Attempting to write YAML configuration file to: {yaml_file_path}")

# Write the data to the YAML file
try:
    with open(yaml_file_path, 'w') as file:
        yaml.dump(config_data, file, default_flow_style=False, sort_keys=False) # sort_keys=False to keep the order for explanations
    print(f"YAML configuration file write attempt completed.")

    # Check if the file exists after writing
    if os.path.exists(yaml_file_path):
        print(f"Confirmation: YAML configuration file found at: {yaml_file_path}")
    else:
        print(f"Warning: YAML configuration file NOT found at: {yaml_file_path} immediately after writing attempt.")


except Exception as e:
    print(f"Error creating YAML file: {e}")

# S3fmsYL-7lVQ

# Initial Setup and Parameters

# DUPLICATE IMPORTS COMMENTED OUT - Already imported at top of file
# import torch
# import torch.nn as nn
# from torch.nn import functional as F
# import yaml # Import the yaml library
# from google.colab import drive
# import os
# from pathlib import Path
# import numbers
# import math
# import random
# from datetime import datetime
# import numpy as np
# from sklearn.cluster import KMeans


# --- Mount Google Drive ---
# To access files stored in your Google Drive, you need to mount it.
# from google.colab import drive
# drive.mount('/content/drive', force_remount=True) # Mount Drive - This should be in a separate cell if needed


# --- Load Configuration from YAML ---
# Define the path to the main configuration file
config_yaml_path = 'C:/Users/tsalo/gpt_proj_6/multimodal_1/config.yaml' # Local path for PyCharm

try:
    with open(config_yaml_path, 'r') as file:
        config_data = yaml.safe_load(file)
    print(f"Configuration loaded successfully from: {config_yaml_path}")
except FileNotFoundError:
    raise FileNotFoundError(f"Configuration file not found at: {config_yaml_path}. Please ensure it exists and the path is correct.")
except yaml.YAMLError as e:
    raise yaml.YAMLError(f"Error loading or parsing YAML configuration file '{config_yaml_path}': {e}")


# --- Assign Parameters from Loaded Configuration ---

# Project Settings
project_settings = config_data.get('project_settings', {})
project_file_path = project_settings.get('project_file_path', 'C:/Users/tsalo/gpt_proj_6/multimodal_1/') # Local path for PyCharm
output_file_name = project_settings.get('output_file_name', 'training_log.txt') # Default if not in config
model_file_name = project_settings.get('model_file_name', project_file_path + 'output/TransformerModel.pth') # Default, using project_file_path
create_new_model = project_settings.get('create_new_model', 1) # Default to create new
save_model = project_settings.get('save_model', 1) # Default to save model
device_str = project_settings.get('device', 'cuda') # Default to 'cuda'


# Data Splitting Parameters
data_splitting = config_data.get('data_splitting', {})
validation_size = data_splitting.get('validation_size', 0.1) # Default if not in config
num_validation_files = data_splitting.get('num_validation_files', 0) # Default if not in config

# Training Parameters
training_parameters = config_data.get('training_parameters', {})
batch_size = training_parameters.get('batch_size', 64) # Default if not in config
block_size = training_parameters.get('block_size', 256) # Default if not in config
max_iters = training_parameters.get('max_iters', 5000) # Default if not in config
eval_interval = training_parameters.get('eval_interval', 500) # Default if not in config
learning_rate = training_parameters.get('learning_rate', 3e-4) # Default if not in config

# Model Architecture Parameters
model_architecture = config_data.get('model_architecture', {})
n_embd = model_architecture.get('n_embd', 384) # Default if not in config
n_head = model_architecture.get('n_head', 6) # Default if not in config
n_layer = model_architecture.get('n_layer', 6) # Default if not in config
dropout = model_architecture.get('dropout', 0.2) # Default if not in config


# --- Device Setup ---
# Check for CUDA availability and set the device
device = torch.device(device_str if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
if device_str == 'cuda' and not torch.cuda.is_available():
    print("Warning: CUDA not available, using CPU instead.")


# --- Set random seed for reproducibility ---
# (Assuming you have a random_seed variable defined elsewhere or can add it to config)
# For now, setting a fixed seed directly
# random_seed = config_data.get('random_seed', 42) # Example: add random_seed to config
random_seed = 42 # Using a fixed seed for now
torch.manual_seed(random_seed)
# If you are using CUDA, you might want to set these as well for deterministic behavior:
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(random_seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False # Set to False for reproducibility


# print out some hyperparameters for confirmation
print("\n--- Loaded Parameters ---")
print(f"Project File Path: {project_file_path}")
print(f"Output File Name: {output_file_name}")
print(f"Model File Name: {model_file_name}")
print(f"Create New Model: {bool(create_new_model)}")
print(f"Save Model: {bool(save_model)}")
print(f"Device: {device_str}")
print(f"Validation Size: {validation_size}")
print(f"Num Validation Files: {num_validation_files}")
print(f"Batch Size: {batch_size}")
print(f"Block Size: {block_size}")
print(f"Max Iters: {max_iters}")
print(f"Eval Interval: {eval_interval}")
print(f"Learning Rate: {learning_rate}")
print(f"N Embed: {n_embd}")
print(f"N Head: {n_head}")
print(f"N Layer: {n_layer}")
print(f"Dropout: {dropout}")
print(f"Random Seed: {random_seed}")
print("-------------------------\n")


# Create output directory if it doesn't exist
output_dir = os.path.dirname(output_file_name)
if output_dir and not os.path.exists(output_dir):
    os.makedirs(output_dir)
    print(f"Created output directory: {output_dir}")

# Create model directory if it doesn't exist
model_dir = os.path.dirname(model_file_name)
if model_dir and not os.path.exists(model_dir):
    os.makedirs(model_dir)
    print(f"Created model directory: {model_dir}")

def add_rand_to_data_points(numeric_data, rand_size, vocab_size):
    """
    Introduces small random changes to numeric data for data augmentation.

    To mitigate limited trading data volume compared to language training,
    this function synthetically increases the amount of data by adding a small random value
    within a specified range to each data point. This creates slightly varied
    training examples without significantly altering the overall data distribution,
    helping to improve training on existing patterns.

    The random value is chosen from the range [-rand_size, rand_size],
    and is added to each element in `numeric_data` only if the result stays within
    the bounds of the vocabulary size.

    This function should be applied only to the training data.

    Args:
      numeric_data: A list or a 1D tensor of integers representing the numeric data.
      rand_size: An integer between 1-3, or None, specifying the maximum absolute value
                 of the random addition. The random value will be in the range
                 [-rand_size, rand_size].
      vocab_size: An integer representing the size of the vocabulary.

    Returns:
      A list or tensor of integers with small random changes applied.

    Raises:
      TypeError: If inputs are not of the expected types.
      ValueError: If inputs are not of the expected values.
    """

    # if numeric_data was input as a tensor, then temporarily turn it into a list
    if isinstance(numeric_data, torch.Tensor):
        numeric_data = numeric_data.tolist()
        numeric_data_is_a_tensor = True
    else:
        numeric_data_is_a_tensor = False

    # Input validation for numeric_data
    if not isinstance(numeric_data, list):
        raise TypeError("'numeric_data' must be a list.")
    for i, item in enumerate(numeric_data):
        if not isinstance(item, numbers.Number):
            raise TypeError(f"Element at index {i} in 'numeric_data' is not a number.")
        if not isinstance(item, int):
            raise TypeError(f"Element at index {i} in 'numeric_data' is not an integer.")

    # Input validation for rand_size
    if not isinstance(rand_size, int):
        raise TypeError("'rand_size' must be an integer.")
    if rand_size < 1 or rand_size > 3:
        raise ValueError("'rand_size' must be between 1 and 3.")

    # Input validation for vocab_size
    if not isinstance(vocab_size, int):
        raise TypeError("'vocab_size' must be an integer.")
    if vocab_size <= 0:
        raise ValueError("'vocab_size' must be a positive integer.")


    rand_list = [0]

    for r in range(rand_size):
        rand_list.extend([r+1, -(r+1)])

    for n in range(len(numeric_data)):
        # Check if adding the maximum possible random value still keeps the element within vocabulary bounds
        if max(rand_list) < numeric_data[n] < vocab_size - max(rand_list):
            # Add rand to data point
            numeric_data[n] += random.choice(rand_list)


    # turn numeric_data back to a tensor
    if numeric_data_is_a_tensor:
        numeric_data = torch.tensor(numeric_data, dtype=torch.long)


    return numeric_data

# Create a sample YAML configuration file for input schemas

# DUPLICATE IMPORTS COMMENTED OUT - Already imported at top of file
# import yaml
# from google.colab import drive # Assuming drive is already mounted

# Define the structure of the configuration data
config_data = {
    'modalities': [
        {
            'modality_name': '200 stocks', # Moved to the beginning
            'path': 'C:/Users/tsalo/gpt_proj_6/multimodal_1/data_1/tick_10m/',
            'column_number': 13,
            'has_header': True,
            'cross_attention': True,
            'randomness_size': None, # Moved randomness_size here as it's not a sequential processing step
            'processing_steps': [
                {'function': 'range_numeric_data', 'args': {'num_whole_digits': 2, 'decimal_places': 1}}
            ]
        },
        {
            'modality_name': '200 stocks - percents', # Moved to the beginning
            'path': 'C:/Users/tsalo/gpt_proj_6/multimodal_1/data_1/tick_10m/',
            'column_number': 13,
            'has_header': True,
            'cross_attention': False,
            'randomness_size': None, # Moved randomness_size here
            'processing_steps': [
                {'function': 'calculate_percent_changes', 'args': {'decimal_places': 2}},
                {'function': 'bin_numeric_data', 'args': {'num_bins': 6, 'outlier_percentile': 0.1, 'exponent': 2.2}} # Include args for binning
            ]
        },
        {
            'modality_name': 'Time', # Moved to the beginning
            'path': 'C:/Users/tsalo/gpt_proj_6/multimodal_1/data_1/tick_10m/',
            'column_number': 9,
            'has_header': True,
            'cross_attention': False,
            'randomness_size': None, # Moved randomness_size here
            'processing_steps': [] # No specific processing steps for Time
        },
        {
            'modality_name': 'Day of week', # Moved to the beginning
            'path': 'C:/Users/tsalo/gpt_proj_6/multimodal_1/data_1/tick_10m/',
            'column_number': 5,
            'has_header': True,
            'cross_attention': False,
            'randomness_size': None, # Moved randomness_size here
            'processing_steps': [] # No specific processing steps for Day of week
        }
        # Add configurations for modality_schema_5 to 10 here if needed.
    ]
}

# Define the path for the YAML file
yaml_file_path = 'C:/Users/tsalo/gpt_proj_6/multimodal_1/input_schemas.yaml' # Local path for PyCharm

# Write the data to the YAML file
try:
    with open(yaml_file_path, 'w') as file:
        yaml.dump(config_data, file, default_flow_style=False)
    print(f"YAML configuration file created successfully at: {yaml_file_path}")
except Exception as e:
    print(f"Error creating YAML file: {e}")

# Cell b92c0d1f

# Data Preparation:
# - Load raw data from files based on configurations from a YAML file
# - Apply processing steps defined in the configuration
# - Create a vocabulary of unique elements and convert it into a numerical representation
# - Split the data into training and validation sets

# DUPLICATE IMPORTS COMMENTED OUT - Already imported at top of file
# import yaml
# import inspect # Import inspect to get function signature for validation

# Define the path to the YAML configuration file
yaml_config_path = 'C:/Users/tsalo/gpt_proj_6/multimodal_1/input_schemas.yaml' # Local path for PyCharm

# Load configurations from the YAML file
try:
    with open(yaml_config_path, 'r') as file:
        config_data = yaml.safe_load(file)
    print(f"Configuration loaded successfully from: {yaml_config_path}")
except FileNotFoundError:
    raise FileNotFoundError(f"Configuration file not found at: {yaml_config_path}")
except yaml.YAMLError as e:
    raise yaml.YAMLError(f"Error loading or parsing YAML configuration file: {e}")


all_modality_data = []  # For each modality, will contain a list of raw data elements, or of processed elements (if specified and if numeric)
all_file_info = []  # For each modality, will contain a list of the loaded file information: [file1_name, data1_length, file2_name, data2_length, ...]
# all_modality_params will now store ModalityConfig instances loaded from the config file
all_modality_params = []

modality_num = 0
# is_percents = False # This flag is only used in calculate_evaluation_metrics and estimate_loss, consider removing or moving if not needed globally - Removed as it can be accessed from all_modality_params
input_schema_in_use = False # Flag to check if at least one valid input schema was found


# Check if 'modalities' key exists and is a list
if 'modalities' not in config_data or not isinstance(config_data['modalities'], list):
    raise ValueError("Configuration file must contain a list under the key 'modalities'.")

# Iterate through the modality configurations loaded from the YAML file
for i, modality_config_dict in enumerate(config_data['modalities']):
    # Check if the loaded item is a dictionary and is not empty
    if isinstance(modality_config_dict, dict) and modality_config_dict:

        # Create a ModalityConfig instance from the dictionary
        try:
            # Use .get() with default values to handle missing optional keys gracefully
            this_input_schema = ModalityConfig(
                path=modality_config_dict.get('path'),
                column_number=modality_config_dict.get('column_number'),
                has_header=modality_config_dict.get('has_header'),
                processing_steps=modality_config_dict.get('processing_steps', []), # Default to empty list
                randomness_size=modality_config_dict.get('randomness_size'),
                cross_attention=modality_config_dict.get('cross_attention', False), # Default to False
                modality_name=modality_config_dict.get('modality_name')
            )
        except Exception as e:
            raise ValueError(f"Error creating ModalityConfig instance from configuration entry {i+1}: {e}")


        # --- Schema Validation (now validating the ModalityConfig instance) ---
        # The ModalityConfig __bool__ check handles whether path, column_number, has_header are present and not None.
        if not this_input_schema:
            raise ValueError(f"Configuration entry {i+1} does not have required fields (path, column_number, has_header).")

        # Additional type checks for required fields
        if not isinstance(this_input_schema.path, str):
            raise TypeError(f"Attribute 'path' in configuration entry {i+1} ('{this_input_schema.modality_name or 'Unnamed'}') must be a string, but got {type(this_input_schema.path).__name__}.")
        # File existence check is done in load_file_data

        if not isinstance(this_input_schema.column_number, int):
            raise TypeError(f"Attribute 'column_number' in configuration entry {i+1} ('{this_input_schema.modality_name or 'Unnamed'}') must be an integer, but got {type(this_input_schema.column_number).__name__}.")
        if this_input_schema.column_number < 1:
             raise ValueError(f"Attribute 'column_number' in configuration entry {i+1} ('{this_input_schema.modality_name or 'Unnamed'}') must be greater than or equal to 1, but got {this_input_schema.column_number}.")

        if not isinstance(this_input_schema.has_header, bool):
             raise TypeError(f"Attribute 'has_header' in configuration entry {i+1} ('{this_input_schema.modality_name or 'Unnamed'}') must be a boolean, but got {type(this_input_schema.has_header).__name__}.")

        # Validate processing_steps structure
        if not isinstance(this_input_schema.processing_steps, list):
            raise TypeError(f"Attribute 'processing_steps' in configuration entry {i+1} ('{this_input_schema.modality_name or 'Unnamed'}') must be a list, but got {type(this_input_schema.processing_steps).__name__}.")

        for step_index, step in enumerate(this_input_schema.processing_steps):
            if not isinstance(step, dict):
                 raise TypeError(f"Each step in 'processing_steps' for configuration entry {i+1} ('{this_input_schema.modality_name or 'Unnamed'}') must be a dictionary, but step {step_index+1} is a {type(step).__name__}.")
            if 'function' not in step:
                 raise ValueError(f"Each step in 'processing_steps' for configuration entry {i+1} ('{this_input_schema.modality_name or 'Unnamed'}') must have a 'function' key, but step {step_index+1} does not.")
            if not isinstance(step['function'], str):
                 raise TypeError(f"The 'function' key in step {step_index+1} of 'processing_steps' for configuration entry {i+1} ('{this_input_schema.modality_name or 'Unnamed'}') must be a string, but got {type(step['function']).__name__}.")
            if 'args' in step and not isinstance(step['args'], dict):
                 raise TypeError(f"The 'args' key in step {step_index+1} of 'processing_steps' for configuration entry {i+1} ('{this_input_schema.modality_name or 'Unnamed'}') must be a dictionary, but got {type(step['args']).__name__}.")


        # Check other optional fields if they are not None
        if this_input_schema.randomness_size is not None:
            if not isinstance(this_input_schema.randomness_size, int):
                raise TypeError(f"Attribute 'randomness_size' in configuration entry {i+1} ('{this_input_schema.modality_name or 'Unnamed'}') must be an integer or None, but got {type(this_input_schema.randomness_size).__name__}.")
            if not (1 <= this_input_schema.randomness_size <= 3):
                 raise ValueError(f"Attribute 'randomness_size' in configuration entry {i+1} ('{this_input_schema.modality_name or 'Unnamed'}') must be between 1 and 3 (inclusive) when an integer, but got {this_input_schema.randomness_size}.")

        if this_input_schema.cross_attention is not None and not isinstance(this_input_schema.cross_attention, bool):
             raise TypeError(f"Attribute 'cross_attention' in configuration entry {i+1} ('{this_input_schema.modality_name or 'Unnamed'}') must be a boolean or None, but got {type(this_input_schema.cross_attention).__name__}.")

        if this_input_schema.modality_name is not None and not isinstance(this_input_schema.modality_name, str):
             raise TypeError(f"Attribute 'modality_name' in configuration entry {i+1} ('{this_input_schema.modality_name or 'Unnamed'}') must be a string or None, but got {type(this_input_schema.modality_name).__name__}.")
        # --- End Schema Validation ---


        print("\n\n----------------------------------------------------------\n\n")
        print("Preparing data...")

        modality_num += 1
        # Use provided modality_name or default to a generic name
        display_modality_name = this_input_schema.modality_name if isinstance(this_input_schema.modality_name, str) else f"Modality {modality_num}"
        print(f"\n{display_modality_name}")


        # Load data - pass the full ModalityConfig instance
        this_modality_data, this_file_info = load_file_data(this_input_schema)

        # --- Apply Processing Steps Dynamically ---
        print(f"\n\n  Applying Processing Steps to Modality '{display_modality_name}'...\n")
        processed_data = this_modality_data # Start with the loaded data

        # Dictionary to map function names to function objects
        # Add functions from the current global scope that might be used as processing steps
        available_processing_functions = {
            'range_numeric_data': range_numeric_data,
            'bin_numeric_data': bin_numeric_data,
            'calculate_percent_changes': calculate_percent_changes,
            # Add other potential processing functions here as needed
        }

        for step_index, step in enumerate(this_input_schema.processing_steps):
            function_name = step['function']
            args = step.get('args', {}) # Default to empty dictionary if 'args' is missing

            if function_name not in available_processing_functions:
                raise ValueError(f"Unknown processing function '{function_name}' specified in step {step_index+1} for Modality '{display_modality_name}'. Available functions: {list(available_processing_functions.keys())}")

            processing_function = available_processing_functions[function_name]

            print(f"    Applying step {step_index+1}: '{function_name}' with args {args}")

            try:
                # Dynamically call the function with the current data and arguments
                # Need to check function signature to pass modality_params if required
                sig = inspect.signature(processing_function)
                params = sig.parameters

                # Prepare arguments to pass to the function
                call_args = {'data': processed_data}
                if 'modality_params' in params:
                    call_args['modality_params'] = this_input_schema # Pass the ModalityConfig instance

                # Add arguments from the config, ensuring they match function parameters
                for arg_name, arg_value in args.items():
                    if arg_name in params:
                         call_args[arg_name] = arg_value
                    else:
                         print(f"Warning: Argument '{arg_name}' specified in config for function '{function_name}' (step {step_index+1}) does not match any parameter in the function's signature. It will be ignored.")


                # Call the function
                # Pass 'data' explicitly, and unpack the rest of the args dictionary
                if 'data' in call_args:
                    data_arg = call_args.pop('data')
                    processed_data = processing_function(data_arg, **call_args)
                else:
                     # This case should not happen if our convention is followed, but as a safeguard
                     raise RuntimeError(f"Processing function '{function_name}' (step {step_index+1}) does not accept a 'data' argument.")


            except Exception as e:
                # Catch any errors during function execution and provide context
                raise RuntimeError(f"Error executing processing step '{function_name}' (step {step_index+1}) for Modality '{display_modality_name}': {e}") from e

        # After applying all processing steps, the final processed_data is ready
        all_modality_data.append(processed_data)
        all_file_info.append(this_file_info) # file_info remains the same as loaded
        # Store the ModalityConfig instance directly in all_modality_params
        all_modality_params.append(this_input_schema)


        input_schema_in_use = True # Mark that at least one valid schema was processed


# After the loop, check if any input schemas were used
if not input_schema_in_use:
  raise ValueError("No valid modality configurations were found or processed from the YAML file.")


print("\n\n\n✔ Data loading for all specified modalities complete")
num_modalities = len(all_modality_data)

# Check for equal modality lengths (after processing)
if num_modalities > 1:
    first_modality_length = len(all_modality_data[0])
    for i in range(1, num_modalities):
        if len(all_modality_data[i]) != first_modality_length:
            raise ValueError(
                f"Modality {i+1} has a different data length ({len(all_modality_data[i])}) "
                f"than the first modality ({first_modality_length}) after processing. "
                "All modalities must have the same data length."
            )
    print("✔ All modalities have equal data lengths after processing")


# Convert all lists of input data into their numerical representation,
# and create a vocabulary of unique elements for each.
all_numeric_reps = []
all_vocabularies = []

print("\n\n----------------------------------------------------------\n\n")
print("Creating Vocabularies and Numerical Representations...")

for m in range(num_modalities):
  # Access modality name using the attribute from the ModalityConfig instance
  this_modality_name = all_modality_params[m].modality_name if all_modality_params[m] is not None else f"Modality {m+1}"
  display_modality_name = this_modality_name if isinstance(this_modality_name, str) else f"Modality {m+1}"
  print(f"\n{display_modality_name}")

  # numerical_representation should work on the final processed data for each modality
  numeric_rep, vocab = numerical_representation(all_modality_data[m])
  all_numeric_reps.append(numeric_rep)
  all_vocabularies.append(vocab)
  print(f"  Vocabulary size: {len(vocab)}")
  print(f"  Numerical representation length: {len(numeric_rep)}")


# Split the data into training (all_train_sets) and validation (all_val_sets) sets for all modalities,
# and converted all datasets into PyTorch tensors.
# But first, create a list 'file_lengths' containing the file lengths (or more accurately,
# the lengths of data segments taken from those files) of the files uploaded to create the first modality.
# (the reason for using file lengths from the first modality and applying it to all modalities- insuring similar
# splitting across all modalities, specifically when using num_validation_files).

file_lengths = []

# all_file_info[0] is [file1_name, data1_length, file2_name, data2_length, ...]
# Extract lengths which are at odd indices (1, 3, 5, ...)
# Use the file lengths from the *first* modality for splitting consistency across all modalities
if all_file_info and len(all_file_info) > 0:
  for f_idx in range(1, len(all_file_info[0]), 2):
    file_lengths.append(all_file_info[0][f_idx])
else:
    # Handle case where no file info was collected (should be caught by input_schema_in_use check, but as safeguard)
    print("Warning: No file information collected, unable to use file lengths for splitting.")
    # Fallback: Create a single file length equal to the total data length if possible
    if num_modalities > 0 and len(all_numeric_reps) > 0:
        file_lengths = [len(all_numeric_reps[0])]
    else:
        file_lengths = [] # Cannot determine file lengths

if not file_lengths:
     # This would happen if no data was loaded or if the first modality had no file info
     raise RuntimeError("Unable to determine file lengths for data splitting.")


all_train_sets = []
all_val_sets = []

print("\n\n----------------------------------------------------------\n\n")
print("Creating Training and Validation datasets...\n")

for i in range(num_modalities):
  # Use the file_lengths derived from the first modality for splitting all modalities
  # create_train_val_datasets expects the combined data (numeric_rep)
  this_train_set_list, this_val_set_list = create_train_val_datasets(all_numeric_reps[i], validation_size, num_validation_files, file_lengths)

  # Handle mixed return types from create_train_val_datasets
  # train_dataset is a list, val_dataset is already a tensor
  this_train_set_tensor = torch.tensor(this_train_set_list, dtype=torch.long)
  this_val_set_tensor = this_val_set_list if isinstance(this_val_set_list, torch.Tensor) else torch.tensor(this_val_set_list, dtype=torch.long)

  all_train_sets.append(this_train_set_tensor)
  all_val_sets.append(this_val_set_tensor)

  # Print the method by which train/val set sizes were determined
  # Print only once (if i == 0), (applies for all modalities)
  if i == 0:
    if num_validation_files > 0:
      # Lengths determined by num_validation_files
      print(f"Data splitting by file length (num_validation_files = {num_validation_files}):")
      print(f"Validation sets comprise the combined length of the last {num_validation_files} files from Modality 1")
      print(f"Training sets comprise the length of the remaining data")
      '''
      # Print the file names used for validation in the first modality
      # all_file_info[0] is [file1_name, data1_length, file2_name, data2_length, ...]
      # For the validation set we need to go backwards, so start from the second to last element (index len(all_file_info[0]) - 2) and step backwards by 2
      val_files_counter = 0
      for j in range(len(all_file_info[0]) - 2, -1, -2):
        this_file_name = all_file_info[0][j]
        print(f"  - {this_file_name}")
        val_files_counter += 1
        if val_files_counter == num_validation_files:
          break
      '''

    else:
      # Lengths determined by validation_size
      val_pct = validation_size * 100
      if val_pct == round(val_pct):
        formatted_val_pct = int(val_pct) # Convert to integer if it's a whole number
      else:
        formatted_val_pct = round(val_pct, 2) # Round to 2 decimal places if it's a fraction
      print(f"Validation sets will comprise {formatted_val_pct}% of the total data length (validation_size = {validation_size})")
      print(f"Training sets will comprise the remaining {100 - formatted_val_pct}% of the data")

  # Access modality name using the attribute from the ModalityConfig instance
  this_modality_params = all_modality_params[i]
  this_modality_name = this_modality_params.modality_name if this_modality_params is not None else f"Modality {i+1}"
  display_modality_name = this_modality_name if isinstance(this_modality_name, str) else f"Modality {i+1}"
  print(f"\n{display_modality_name}")
  # Use .item() or .tolist() if needed for printing tensor lengths, but len() should work directly on tensors
  print(f"  Validation data length: {len(this_val_set_tensor)}")
  print(f"  Training data length: {len(this_train_set_tensor)}")

  # Print randomness specified for this modality
  # Access rand_size using the attribute from the ModalityConfig instance
  this_rand_size = this_modality_params.randomness_size if this_modality_params is not None else None
  if isinstance(this_rand_size, int) and 1 <= this_rand_size <= 3:
    print(f"  + Random noise range of: \u00B1{this_rand_size} will be applied to the training set of this modality")
  elif this_rand_size is None:
    print(f'  - Random noise not set for this modality')

  # Print cross-attention specified for this modality
  # Access cross_attend using the attribute from the ModalityConfig instance
  this_cross_attend = this_modality_params.cross_attention if this_modality_params is not None else False
  if this_cross_attend is True:
    print(f"  + Cross-attention is enabled (this modality will attend to all other modalities)")
  elif this_cross_attend is False:
    print(f'  - Cross-attention is not enabled for this modality')


print("\n\n\n✔ Data preparation for all modalities complete")

# hQ1IqENWubCj

# DUPLICATE IMPORTS COMMENTED OUT - Already imported at top of file
# import torch
# import torch.nn as nn
# from torch.nn import functional as F
# from dataclasses import dataclass, field
# from typing import Optional, List, Any

# Model Architecture Definitions

# DUPLICATE CLASS COMMENTED OUT - Use line 2343 version instead
# class Head(nn.Module):
#     """ one head of self-attention """

#     def __init__(self, head_size, n_embd, dropout, block_size): # Added n_embd, dropout, block_size
#         super().__init__()
#         self.key = nn.Linear(n_embd, head_size, bias=False)
#         self.query = nn.Linear(n_embd, head_size, bias=False)
#         self.value = nn.Linear(n_embd, head_size, bias=False)
#         self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # Use block_size

#         self.dropout = nn.Dropout(dropout) # Use dropout

#     def forward(self, x):
#         B,T,C = x.shape
#         k = self.key(x)   # (B,T,hs)
#         q = self.query(x) # (B,T,hs)
#         # compute attention scores ("affinities")
#         wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)
#         wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # Use block_size
#         wei = F.softmax(wei, dim=-1) # (B, T, T)
#         wei = self.dropout(wei)
#         # perform the weighted aggregation of the values
#         v = self.value(x) # (B,T,hs)
#         out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)
#         return out

# DUPLICATE CLASS COMMENTED OUT - Use line 2370 version instead
# class MultiHeadAttention(nn.Module):
#     """ multiple heads of self-attention in parallel """

#     def __init__(self, num_heads, head_size, n_embd, dropout, block_size): # Added n_embd, dropout, block_size
#         super().__init__()
#         self.heads = nn.ModuleList([Head(head_size, n_embd, dropout, block_size) for _ in range(num_heads)]) # Pass parameters
#         self.proj = nn.Linear(n_embd, n_embd) # Use n_embd
#         self.dropout = nn.Dropout(dropout) # Use dropout

#     def forward(self, x):
#         out = torch.cat([h(x) for h in self.heads], dim=-1)
#         out = self.dropout(self.proj(out))
#         return out

# DUPLICATE CLASS COMMENTED OUT - Use line 2474 version instead
# class FeedFoward(nn.Module):
#     """ a simple linear layer followed by a non-linearity """

#     def __init__(self, n_embd, dropout): # Added n_embd, dropout
#         super().__init__()
#         self.net = nn.Sequential(
#             nn.Linear(n_embd, 4 * n_embd), # Use n_embd
#             nn.ReLU(),
#             nn.Linear(4 * n_embd, n_embd), # Use n_embd
#             nn.Dropout(dropout), # Use dropout
#         )

#     def forward(self, x):
#         return self.net(x)

# DUPLICATE CLASS COMMENTED OUT - Use line 2490 version instead
# class Block(nn.Module):
#     """ Transformer block: communication followed by computation """

#     def __init__(self, n_embd, n_head, dropout, block_size): # Added n_embd, n_head, dropout, block_size
#         # n_embd: embedding dimension, n_head: the number of heads we'd like
#         super().__init__()
#         head_size = n_embd // n_head
#         self.sa = MultiHeadAttention(n_head, head_size, n_embd, dropout, block_size) # Pass parameters
#         self.ffwd = FeedFoward(n_embd, dropout) # Pass parameters
#         self.ln1 = nn.LayerNorm(n_embd) # Use n_embd
#         self.ln2 = nn.LayerNorm(n_embd) # Use n_embd

#     def forward(self, x):
#         x = x + self.sa(self.ln1(x))
#         x = x + self.ffwd(self.ln2(x))
#         return x


# DUPLICATE CLASS COMMENTED OUT - Use line 2509 version instead
# class MultimodalTransformer(nn.Module):
#     # Added all_vocab_sizes and all_modality_params
#     def __init__(self, num_modalities: int, all_vocab_sizes: List[int], all_modality_params: List[ModalityConfig], n_embd: int, n_head: int, n_layer: int, dropout: int, block_size: int):
#         super().__init__()
#         self.num_modalities = num_modalities
#         self.block_size = block_size # Store block_size as attribute
#
#         # Token embedding layers for each modality
#         self.token_embedding_tables = nn.ModuleList([
#             nn.Embedding(vocab_size, n_embd) for vocab_size in all_vocab_sizes
#         ])
#
#         # Positional embedding (shared across modalities)
#         self.position_embedding_table = nn.Embedding(block_size, n_embd)
#
#         # Transformer blocks
#         self.blocks = nn.Sequential(*[Block(n_embd, n_head, dropout, block_size) for _ in range(n_layer)]) # Pass parameters
#
#         # Final layer norm
#         self.ln_f = nn.LayerNorm(n_embd * num_modalities) # Layer norm after concatenating embeddings
#
#         # Linear head for each modality
#         self.lm_heads = nn.ModuleList([
#             nn.Linear(n_embd * num_modalities, vocab_size) for vocab_size in all_vocab_sizes
#         ])
#
#         # Store modality parameters
#         self.all_modality_params = all_modality_params

#
#     def forward(self, xb_list: List[torch.Tensor], yb_list: Optional[List[torch.Tensor]] = None):
#         """
#         Forward pass for the multimodal transformer.
#
#         Args:
#             xb_list: A list of tensors, one for each modality, containing the input token
#                      indices for a batch. Shape: List of [(batch_size, block_size)]
#             yb_list: An optional list of tensors, one for each modality, containing the target
#                      token indices for a batch. Shape: Optional[List of [(batch_size, block_size)]]
#
#         Returns:
#             A tuple containing:
#             - logits_list: A list of tensors, one for each modality, containing the logits
#                            for each token in the batch. Shape: List of [(batch_size, block_size, vocab_size)]
#             - losses_list: A list of scalar loss values, one for each modality, or None if yb_list is None.
#                            Shape: List of [scalar] or None
#         """
#         B, T = xb_list[0].shape # B = batch size, T = block_size
#
#         # Get token embeddings for each modality
#         token_embeddings = [self.token_embedding_tables[i](xb_list[i]) for i in range(self.num_modalities)] # List of (B, T, n_embd)
#
#         # Get positional embeddings (shared)
#         pos = torch.arange(T, device=xb_list[0].device)
#         position_embeddings = self.position_embedding_table(pos) # (T, n_embd)

#         # Add positional embeddings to each token embedding
#         x_list = [token_embeddings[i] + position_embeddings for i in range(self.num_modalities)] # List of (B, T, n_embd)

#         # Concatenate embeddings across modalities for the transformer blocks
#         # Need to ensure all tensors have the same shape before concatenating along the last dimension
#         # This assumes each modality's embedding size is n_embd.
#         x_combined = torch.cat(x_list, dim=-1) # (B, T, n_embd * num_modalities)


#         # Apply transformer blocks
#         # The blocks currently expect input shape (B, T, n_embd).
#         # We need to modify the Block and MultiHeadAttention to handle multimodal input,
#         # or apply attention/feedforward within each modality and then combine.
#         # For now, we will assume the blocks operate on the concatenated embedding,
#         # which implies the attention mechanism needs to be aware of the modality boundaries
#         # or we need separate attention for each modality before concatenation.

#         # Let's assume for this implementation that the Blocks are applied *after*
#         # some form of multimodal interaction or feature extraction.
#         # A simpler approach is to apply separate blocks per modality and then combine,
#         # or use a multimodal attention mechanism.

#         # Given the current Block and MultiHeadAttention structure,
#         # they are designed for a single input embedding (B, T, C).
#         # To make them multimodal, we might need to pass the list of embeddings
#         # or modify the attention mechanism to handle concatenated embeddings properly.

#         # Let's revert to applying blocks per modality first, then combining.
#         # This requires changing the Block definition or how it's used here.
#         # Or, we can assume the current Block definition is for a single modality's
#         # n_embd dimension and apply it iteratively or in parallel.

#         # Let's assume the Blocks are applied to the concatenated embedding for now,
#         # but acknowledge this might need refinement based on the intended multimodal interaction.
#         # If the intention is cross-attention, the attention mechanism needs to be updated.

#         # Based on the original Block definition, it expects (B, T, n_embd).
#         # Applying it to x_combined (B, T, n_embd * num_modalities) will not work directly.

#         # Let's reconsider the structure: Embeddings -> Positional Encoding -> Multimodal Interaction/Blocks -> Heads

#         # Option 1: Apply separate blocks for each modality, then combine.
#         # This would require num_modalities * n_layer blocks or a way to reuse blocks.
#         # Not ideal for cross-modal interactions within blocks.

#         # Option 2: Modify the Block/Attention to handle multimodal input (e.g., cross-attention).
#         # This is the more complex but potentially more powerful approach.

#         # Option 3: Apply a shared set of blocks to a concatenated embedding, assuming
#         # the attention mechanism is modified or the concatenation is sufficient.
#         # The current Block definition is not set up for this.

#         # Let's assume the intent is a simple concatenation followed by shared blocks
#         # and linear heads for each modality. This requires the blocks to operate
#         # on the combined embedding dimension (n_embd * num_modalities).
#         # This means the Block definition needs to be updated to use n_embd * num_modalities.
#         # Or, the Blocks are applied to each modality's embedding separately.

#         # Let's assume the Blocks are applied to each modality's embedding separately,
#         # and then the results are concatenated before the final layer norm and heads.
#         # This requires creating n_layer sets of blocks, one set per modality, or
#         # applying the same set of blocks iteratively to each modality's embedding.

#         # Let's try applying the same set of blocks iteratively to each modality's embedding.
#         processed_embeddings_list = []
#         for i in range(self.num_modalities):
#              # Apply the shared blocks to each modality's embedding
#              processed_embedding = self.blocks(x_list[i]) # (B, T, n_embd)
#              processed_embeddings_list.append(processed_embedding)

#         # Concatenate the processed embeddings
#         x_combined_processed = torch.cat(processed_embeddings_list, dim=-1) # (B, T, n_embd * num_modalities)


#         # Apply final layer norm
#         x_combined_processed = self.ln_f(x_combined_processed) # (B, T, n_embd * num_modalities)


#         # Get logits for each modality using separate linear heads
#         logits_list = [self.lm_heads[i](x_combined_processed) for i in range(self.num_modalities)] # List of (B, T, vocab_size_i)


#         losses_list = None
#         if yb_list is not None:
#             losses_list = []
#             for i in range(self.num_modalities):
#                 B, T, C_i = logits_list[i].shape # C_i = vocab_size for modality i
#                 # Reshape logits and targets for cross_entropy
#                 logits_flat = logits_list[i].view(B * T, C_i)
#                 targets_flat = yb_list[i].view(B * T)
#                 loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=-1) # Use ignore_index if needed
#                 losses_list.append(loss)

#         return logits_list, losses_list
#
#     # Added generate method
#     def generate(self, idx_list: List[torch.Tensor], max_new_tokens: int):
#         """
#         Generates a sequence of tokens for each modality given a starting sequence.

#         Args:
#             idx_list: A list of tensors, one for each modality, containing the starting
#                       token indices for generation. Shape: List of [(B, T)] where T <= block_size.
#             max_new_tokens: The maximum number of tokens to generate for each modality.

#         Returns:
#             A list of tensors, one for each modality, containing the generated sequences.
#             Shape: List of [(B, T + max_new_tokens)]
#         """
#         # Ensure block_size is an attribute of the model
#         block_size = self.block_size # Access block_size from instance attribute

#         # idx_list is a list of (B, T) tensors
#         generated_sequences_list = [idx.clone() for idx in idx_list] # Start with the initial sequences

#         for _ in range(max_new_tokens):
#             # Crop idx_list to the last block_size tokens
#             # Need to apply cropping to each tensor in the list
#             idx_crop_list = [idx[:, -block_size:] for idx in generated_sequences_list] # List of (B, min(T, block_size))

#             # Get predictions (logits) for the next token for each modality
#             # Call the forward pass with the cropped input, targets are None during generation
#             logits_list, _ = self(idx_crop_list, yb_list=None) # List of (B, T_cropped, vocab_size)

#             # Focus only on the last time step (the predicted next token) for each modality
#             logits_last_step_list = [logits[:, -1, :] for logits in logits_list] # List of (B, vocab_size)

#             # Apply softmax to get probabilities for each modality
#             probs_list = [F.softmax(logits, dim=-1) for logits in logits_last_step_list] # List of (B, vocab_size)

#             # Sample from the distribution for each modality
#             idx_next_list = [torch.multinomial(probs, num_samples=1) for probs in probs_list] # List of (B, 1)

#             # Append sampled index to the running sequence for each modality
#             generated_sequences_list = [
#                 torch.cat((generated_sequences_list[i], idx_next_list[i]), dim=1)
#                 for i in range(self.num_modalities)
#             ]

#         return generated_sequences_list

# Running the transformer

# --- Model Architecture Definitions (from cell GPP9cM9Qftga) ---
# DUPLICATE IMPORTS COMMENTED OUT - Already imported at top of file
# import torch
# import torch.nn as nn
# from torch.nn import functional as F
# from dataclasses import dataclass, field
# from typing import Optional, List, Any
# import random
# import numbers
# import numpy as np
# from datetime import datetime # Import datetime here
# import yaml # Import yaml for config loading
# import os # Import os for path operations
# import pandas as pd # Import pandas
# from pathlib import Path # Import Path
import math # Import math for isnan check
import importlib # Import importlib for dynamic function loading


class Head(nn.Module):
    """ one head of self-attention """

    def __init__(self, head_size, n_embd, dropout, block_size): # Added n_embd, dropout, block_size
        super().__init__()
        self.key = nn.Linear(n_embd, head_size, bias=False)
        self.query = nn.Linear(n_embd, head_size, bias=False)
        self.value = nn.Linear(n_embd, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # Use block_size

        self.dropout = nn.Dropout(dropout) # Use dropout

    def forward(self, x):
        B,T,C = x.shape
        k = self.key(x)   # (B,T,hs)
        q = self.query(x) # (B,T,hs)
        # compute attention scores ("affinities")
        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # Use block_size
        wei = F.softmax(wei, dim=-1) # (B, T, T)
        wei = self.dropout(wei)
        # perform the weighted aggregation of the values
        v = self.value(x) # (B,T,hs)
        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)
        return out

class MultiHeadAttention(nn.Module):
    """ multiple heads of self-attention in parallel """

    def __init__(self, num_heads, head_size, n_embd, dropout, block_size): # Added n_embd, dropout, block_size
        super().__init__()
        self.heads = nn.ModuleList([Head(head_size, n_embd, dropout, block_size) for _ in range(num_heads)]) # Pass parameters
        self.proj = nn.Linear(n_embd, n_embd) # Use n_embd
        self.dropout = nn.Dropout(dropout) # Use dropout

    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        out = self.dropout(self.proj(out))
        return out

class CrossAttention(nn.Module):
    """ Cross-attention mechanism for multimodal interaction """

    def __init__(self, num_heads, head_size, num_kv_modalities):
        super().__init__()
        self.num_heads = num_heads
        self.head_size = head_size
        self.num_kv_modalities = num_kv_modalities # Number of modalities providing keys/values

        self.query_proj = nn.Linear(n_embd, num_heads * head_size, bias=False)

        # Separate key and value projections for each key/value modality
        self.kv_projections = nn.ModuleList([
            nn.Linear(n_embd, 2 * num_heads * head_size, bias=False) for _ in range(num_kv_modalities)
        ])

        self.proj = nn.Linear(num_heads * head_size, n_embd)
        self.dropout = nn.Dropout(dropout)

    def forward(self, query_x, kv_x_list):
        # query_x: Tensor from the modality acting as the query. Shape: (B, T, n_embd)
        # kv_x_list: List of tensors from other modalities providing keys/values. List of [(B, T, n_embd)]

        B, T, C = query_x.shape
        H = self.num_heads
        hs = self.head_size # head_size

        q = self.query_proj(query_x).view(B, T, H, hs).transpose(1, 2) # (B, H, T, hs)

        # Concatenate keys and values from all key/value modalities
        all_keys = []
        all_values = []
        for i, kv_x in enumerate(kv_x_list):
            kv_projected = self.kv_projections[i](kv_x).view(B, T, H, 2 * hs).transpose(1, 2) # (B, H, T, 2*hs)
            k, v = kv_projected.split(hs, dim=-1) # (B, H, T, hs), (B, H, T, hs)
            all_keys.append(k)
            all_values.append(v)

        # Stack keys and values across the modality dimension
        # Resulting shapes: (B, H, num_kv_modalities * T, hs)
        stacked_keys = torch.cat(all_keys, dim=2)
        stacked_values = torch.cat(all_values, dim=2)


        # Compute attention scores
        # q shape: (B, H, T, hs)
        # stacked_keys shape: (B, H, num_kv_modalities * T, hs)
        # Resulting wei shape: (B, H, T, num_kv_modalities * T)
        wei = q @ stacked_keys.transpose(-2, -1) * (hs)**0.5 # Fixed hs exponent

        # Masking: Apply causal mask to prevent attending to future tokens within each *original* sequence
        # We need to create a mask that respects the original sequence boundaries within the concatenated KV
        # Create a block causal mask for a single sequence of length T
        single_seq_mask = torch.tril(torch.ones(T, T, device=query_x.device)).bool()

        # Expand this mask to cover the concatenated KV dimension
        # The mask for a query token at position 't' can only attend to KV tokens
        # that correspond to original tokens at positions <= t *within their respective original sequences*.
        # We need a mask of shape (T, num_kv_modalities * T)
        cross_modal_mask = torch.zeros(T, self.num_kv_modalities * T, device=query_x.device).bool()

        for mod_idx in range(self.num_kv_modalities):
             # The block for modality 'mod_idx' in the concatenated KV is from index mod_idx*T to (mod_idx+1)*T
             cross_modal_mask[:, mod_idx*T:(mod_idx+1)*T] = single_seq_mask

        # Apply the cross-modal mask
        # unsqueeze for head dimension: (1, 1, T, num_kv_modalities * T)
        wei = wei.masked_fill(cross_modal_mask.unsqueeze(0).unsqueeze(0)[:, :, :T, :self.num_kv_modalities * T] == 0, float('-inf'))


        # Apply softmax
        wei = F.softmax(wei, dim=-1) # (B, H, T, num_kv_modalities * T)

        # Apply dropout
        wei = self.dropout(wei)

        # Perform the weighted aggregation of the values
        # wei shape: (B, H, T, num_kv_modalities * T)
        # stacked_values shape: (B, H, num_kv_modalities * T, hs)
        # out shape: (B, H, T, hs)
        out = wei @ stacked_values

        # Reshape and apply final linear projection
        out = out.transpose(1, 2).contiguous().view(B, T, H * hs) # (B, T, H*hs)
        out = self.dropout(self.proj(out)) # (B, T, n_embd)

        return out


class FeedFoward(nn.Module):
    """ a simple linear layer followed by a non-linearity """

    def __init__(self, n_embd, dropout): # Added n_embd, dropout
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd), # Use n_embd
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd), # Use n_embd
            nn.Dropout(dropout), # Use dropout
        )

    def forward(self, x):
        return self.net(x)

class Block(nn.Module):
    """ Transformer block: communication followed by computation """

    def __init__(self, n_embd, n_head, dropout, block_size): # Added n_embd, n_head, dropout, block_size
        # n_embd: embedding dimension, n_head: the number of heads we'd like
        super().__init__()
        head_size = n_embd // n_head
        self.sa = MultiHeadAttention(n_head, head_size, n_embd, dropout, block_size) # Pass parameters
        self.ffwd = FeedFoward(n_embd, dropout) # Pass parameters
        self.ln1 = nn.LayerNorm(n_embd) # Use n_embd
        self.ln2 = nn.LayerNorm(n_embd) # Use n_embd

    def forward(self, x):
        x = x + self.sa(self.ln1(x))
        x = x + self.ffwd(self.ln2(x))
        return x


class MultimodalTransformer(nn.Module):
#     # Added all_vocab_sizes and all_modality_params
    def __init__(self, num_modalities: int, all_vocab_sizes: List[int], all_modality_params: List[ModalityConfig], n_embd: int, n_head: int, n_layer: int, dropout: int, block_size: int):
        super().__init__()
        self.num_modalities = num_modalities
        self.block_size = block_size # Store block_size as attribute

        # Token embedding layers for each modality
        self.token_embedding_tables = nn.ModuleList([
            nn.Embedding(vocab_size, n_embd) for vocab_size in all_vocab_sizes
        ])

        # Positional embedding (shared across modalities)
        self.position_embedding_table = nn.Embedding(block_size, n_embd)

        # Transformer blocks
        self.blocks = nn.Sequential(*[Block(n_embd, n_head, dropout, block_size) for _ in range(n_layer)]) # Pass parameters

        # Final layer norm
        self.ln_f = nn.LayerNorm(n_embd * num_modalities) # Layer norm after concatenating embeddings

        # Linear head for each modality
        self.lm_heads = nn.ModuleList([
            nn.Linear(n_embd * num_modalities, vocab_size) for vocab_size in all_vocab_sizes
        ])

        # Store modality parameters
        self.all_modality_params = all_modality_params


    def forward(self, xb_list: List[torch.Tensor], yb_list: Optional[List[torch.Tensor]] = None):
        """
        Forward pass for the multimodal transformer.

        Args:
            xb_list: A list of tensors, one for each modality, containing the input token
                     indices for a batch. Shape: List of [(batch_size, block_size)]
            yb_list: An optional list of tensors, one for each modality, containing the target
                     token indices for a batch. Shape: Optional[List of [(batch_size, block_size)]]

        Returns:
            A tuple containing:
            - logits_list: A list of tensors, one for each modality, containing the logits
                           for each token in the batch. Shape: List of [(batch_size, block_size, vocab_size)]
            - losses_list: A list of scalar loss values, one for each modality, or None if yb_list is None.
                           Shape: List of [scalar] or None
        """
        B, T = xb_list[0].shape # B = batch size, T = block_size

        # Get token embeddings for each modality
        token_embeddings = [self.token_embedding_tables[i](xb_list[i]) for i in range(self.num_modalities)] # List of (B, T, n_embd)

        # Get positional embeddings (shared)
        pos = torch.arange(T, device=xb_list[0].device)
        position_embeddings = self.position_embedding_table(pos) # (T, n_embd)

        # Add positional embeddings to each token embedding
        x_list = [token_embeddings[i] + position_embeddings for i in range(self.num_modalities)] # List of (B, T, n_embd)

        # Concatenate embeddings across modalities for the transformer blocks
        # Need to ensure all tensors have the same shape before concatenating along the last dimension
        # This assumes each modality's embedding size is n_embd.
        x_combined = torch.cat(x_list, dim=-1) # (B, T, n_embd * num_modalities)


        # Apply transformer blocks
        # The blocks currently expect input shape (B, T, n_embd).
        # We need to modify the Block and MultiHeadAttention to handle multimodal input,
        # or apply attention/feedforward within each modality and then combine.
        # For now, we will assume the blocks operate on the concatenated embedding,
        # which implies the attention mechanism needs to be aware of the modality boundaries
        # or we need separate attention for each modality before concatenation.

        # Let's assume for this implementation that the Blocks are applied *after*
        # some form of multimodal interaction or feature extraction.
        # A simpler approach is to apply separate blocks per modality and then combine,
        # or use a multimodal attention mechanism.

        # Given the current Block and MultiHeadAttention structure,
        # they are designed for a single input embedding (B, T, C).
        # Applying it to x_combined (B, T, n_embd * num_modalities) will not work directly.

        # Let's reconsider the structure: Embeddings -> Positional Encoding -> Multimodal Interaction/Blocks -> Heads

        # Option 1: Apply separate blocks for each modality, then combine.
        # This would require num_modalities * n_layer blocks or a way to reuse blocks.
        # Not ideal for cross-modal interactions within blocks.

        # Option 2: Modify the Block/Attention to handle multimodal input (e.g., cross-attention).
        # This is the more complex but potentially more powerful approach.

        # Option 3: Apply a shared set of blocks to a concatenated embedding, assuming
        # the attention mechanism is modified or the concatenation is sufficient.
        # The current Block definition is not set up for this.

        # Let's try applying the same set of blocks iteratively to each modality's embedding.
        processed_embeddings_list = []
        for i in range(self.num_modalities):
             # Apply the shared blocks to each modality's embedding
             processed_embedding = self.blocks(x_list[i]) # (B, T, n_embd)
             processed_embeddings_list.append(processed_embedding)

        # Concatenate the processed embeddings
        x_combined_processed = torch.cat(processed_embeddings_list, dim=-1) # (B, T, n_embd * num_modalities)


        # Apply final layer norm
        x_combined_processed = self.ln_f(x_combined_processed) # (B, T, n_embd * num_modalities)


        # Get logits for each modality using separate linear heads
        logits_list = [self.lm_heads[i](x_combined_processed) for i in range(self.num_modalities)] # List of (B, T, vocab_size_i)


        losses_list = None
        if yb_list is not None:
            losses_list = []
            for i in range(self.num_modalities):
                B, T, C_i = logits_list[i].shape # C_i = vocab_size for modality i
                # Reshape logits and targets for cross_entropy
                logits_flat = logits_list[i].view(B * T, C_i)
                targets_flat = yb_list[i].view(B * T)
                loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=-1) # Use ignore_index if needed
                losses_list.append(loss)

        return logits_list, losses_list

    # Added generate method
    def generate(self, idx_list: List[torch.Tensor], max_new_tokens: int):
        """
        Generates a sequence of tokens for each modality given a starting sequence.

        Args:
            idx_list: A list of tensors, one for each modality, containing the starting
                      token indices for generation. Shape: List of [(B, T)] where T <= block_size.
            max_new_tokens: The maximum number of tokens to generate for each modality.

        Returns:
            A list of tensors, one for each modality, containing the generated sequences.
            Shape: List of [(B, T + max_new_tokens)]
        """
        # Ensure block_size is an attribute of the model
        block_size = self.block_size # Access block_size from instance attribute

        # idx_list is a list of (B, T) tensors
        generated_sequences_list = [idx.clone() for idx in idx_list] # Start with the initial sequences

        for _ in range(max_new_tokens):
            # Crop idx_list to the last block_size tokens
            # Need to apply cropping to each tensor in the list
            idx_crop_list = [idx[:, -block_size:] for idx in generated_sequences_list] # List of (B, min(T, block_size))

            # Get predictions (logits) for the next token for each modality
            # Call the forward pass with the cropped input, targets are None during generation
            logits_list, _ = self(idx_crop_list, yb_list=None) # List of (B, T_cropped, vocab_size)

            # Focus only on the last time step (the predicted next token) for each modality
            logits_last_step_list = [logits[:, -1, :] for logits in logits_list] # List of (B, vocab_size)

            # Apply softmax to get probabilities for each modality
            probs_list = [F.softmax(logits, dim=-1) for probs in logits_last_step_list] # List of (B, vocab_size)

            # Sample from the distribution for each modality
            idx_next_list = [torch.multinomial(probs, num_samples=1) for probs in probs_list] # List of (B, 1)

            # Append sampled index to the running sequence for each modality
            generated_sequences_list = [
                torch.cat((generated_sequences_list[i], idx_next_list[i]), dim=1)
                for i in range(self.num_modalities)
            ]

        return generated_sequences_list


# Add the definition of get_batch here
def get_batch(split: str, all_modality_params: Optional[List[ModalityConfig]] = None):
    """
    Retrieves a batch of data from the training or validation set for all modalities.

    Args:
        split: A string indicating which dataset to use ('train' or 'val').
        all_modality_params: Optional list of ModalityConfig instances. Used for
                             randomness during training ('train' split).

    Returns:
        A tuple containing two lists of tensors:
        - xb_list: List of input sequences for the batch, one tensor per modality.
                   Shape: List of [(batch_size, block_size)]
        - yb_list: List of target sequences for the batch, one tensor per modality.
                   Shape: List of [(batch_size, block_size)]
    """
    # Select the appropriate data split
    # all_train_sets and all_val_sets are lists of tensors, one tensor per modality
    data = all_train_sets if split == 'train' else all_val_sets
    # The data for each modality is a single tensor: data[modality_index]

    # Determine a random starting offset for the batch
    # The offset should be within the bounds of the shortest sequence in the current split,
    # considering the block_size.
    # Ensure all data tensors have at least block_size + 1 elements to form full batches and targets
    min_data_length = min(len(d) for d in data) if data else 0

    if min_data_length < block_size + 1:
         raise ValueError(f"Data length ({min_data_length}) is less than block_size + 1 ({block_size + 1}). Cannot form complete batches.")


    # The maximum starting index for a sequence of length block_size is min_data_length - block_size
    ix = torch.randint(min_data_length - block_size, (batch_size,)) # Random starting indices for each sequence in the batch


    xb_list = [] # List to hold input batches for each modality
    yb_list = [] # List to hold target batches for each modality

    for modality_index in range(num_modalities):
        # Get the data tensor for the current modality
        modality_data_tensor = data[modality_index] # Shape: (total_data_length,)

        # Use the random indices 'ix' to extract sequences for the batch
        # Optimized tensor operations instead of list comprehensions
        batch_indices = ix.unsqueeze(1) + torch.arange(block_size).unsqueeze(0)  # Shape: (batch_size, block_size)
        xb_modality = modality_data_tensor[batch_indices] # Shape: (batch_size, block_size)
        yb_modality = modality_data_tensor[batch_indices + 1] # Shape: (batch_size, block_size)


        # --- Apply Randomness (Data Augmentation) during training ---
        if split == 'train' and all_modality_params is not None:
            modality_params = all_modality_params[modality_index]
            randomness_size = modality_params.randomness_size if modality_params else None

            if isinstance(randomness_size, int) and 1 <= randomness_size <= 3:
                # Apply random noise to xb_modality
                # Create noise tensor with shape matching xb_modality
                # Noise values are integers in the range [-randomness_size, randomness_size]
                noise = torch.randint(
                    -randomness_size, randomness_size + 1,
                    xb_modality.shape, device=xb_modality.device
                ).long() # Ensure noise is long integer type

                # Add noise to xb_modality
                # Need to handle potential out-of-vocabulary issues after adding noise
                # Simple addition might result in token indices outside the vocabulary size
                # A better approach might be to perturb the *original* numeric values
                # before converting to tokens, or to handle out-of-vocab indices appropriately
                # in the embedding layer (e.g., map to a special UNK token or clamp).

                # For simplicity in this update, we will just add the noise and then clamp
                # the results to stay within the vocabulary size bounds.
                # Assumes vocabulary indices are 0-based and contiguous.
                vocab_size = len(all_vocabularies[modality_index]) # Need access to all_vocabularies

                # Add noise and clamp
                xb_modality = torch.clamp(xb_modality + noise, 0, vocab_size - 1)


        xb_list.append(xb_modality)
        yb_list.append(yb_modality)

    # Move tensors to the device
    xb_list = [xb.to(device) for xb in xb_list]
    yb_list = [yb.to(device) for yb in yb_list]

    return xb_list, yb_list


# Add the definition of _get_direction_sign here
def _get_direction_sign(current_value, previous_value, is_percentage_data):
    """
    Determines the direction sign (1 for up, -1 for down, 0 for flat)
    based on the current and previous numeric values and whether the data is percentages.

    Args:
        current_value: The current numeric value.
        previous_value: The previous numeric value (only used if not percentage data).
        is_percentage_data: Boolean indicating if the data represents percentages.

    Returns:
        An integer: 1 for up, -1 for down, 0 for flat.
    """
    if is_percentage_data:
        if isinstance(current_value, numbers.Number) and not math.isnan(current_value): # Check for numeric and not NaN
            if current_value > 0: return 1
            elif current_value < 0: return -1
            else: return 0 # Handles current_value == 0
        else:
            return None # Cannot determine direction for non-numeric or NaN current value
    else:
        # For value data, direction is based on change from previous value
        if not isinstance(previous_value, numbers.Number) or math.isnan(previous_value) or not isinstance(current_value, numbers.Number) or math.isnan(current_value):
             # Cannot calculate direction if previous or current value is not numeric or is NaN
             return None # Indicate that direction cannot be determined

        change = current_value - previous_value
        if change > 0: return 1
        elif change < 0: return -1
        else: return 0 # Handles change == 0


# Add the definition of calculate_evaluation_metrics here
def calculate_evaluation_metrics(logits_list, yb_list, num_modalities, all_vocabularies, all_modality_params, all_file_info, batch_size, is_percents):
    """
    Calculates success rate (based on predicting the correct direction of change for numeric data)
    and certainty (confidence in the prediction) for each modality from evaluation logits and targets.

    This function processes the output of the model (logits) and the actual target values (yb)
    for a given batch during evaluation. It iterates through each modality and, for numeric data,
    determines if the model's prediction for the last token in a sequence correctly predicted
    the direction of change compared to the previous token.

    For the success rate calculation, *any* predicted direction (up, down, or flat) is compared
    against the actual direction. Wins are counted when the predicted direction matches the
    actual direction (e.g., both up, both down, or both flat). Losses are counted when they do not match.

    The certainty metric represents the model's confidence in the predicted *direction* of change for
    the last token. It is calculated by summing the probabilities (from the softmax of the last token's
    logits) of all possible next tokens that fall within the same direction (up, down, or flat) as the
    single token with the highest predicted probability.

    For applications like stock price prediction, accurately forecasting the direction of movement can
    be highly beneficial, as predicting the exact next price can be significantly more challenging and
    isn't always necessary.

    Note that unusually high directional certainty rates may occur, especially in early training stages
    or with certain data distributions, even if the overall success rate is near chance. This indicates
    the model is strongly skewed towards predicting a particular direction, regardless of accuracy.

    These directional metrics provide additional insights into the model's behavior during evaluation.
    The model's learning process, however, is driven solely by minimizing the calculated loss.

    The function accumulates these metrics across the batch and reports the results for each modality.

    Args:
        logits_list: A list of tensors, one for each modality, containing the logits
                     for each token in the batch. Shape: List of [(batch_size, block_size, vocab_size)]
        yb_list: A list of tensors, one for each modality, containing the target token
                 indices for each token in the batch. Shape: List of [(batch_size, block_size)]
        num_modalities: The total number of modalities being processed. (int)
        all_vocabularies: A list of lists, where each inner list is the vocabulary
                          (unique elements) for a specific modality, sorted in ascending order.
        all_modality_params: A list of ModalityConfig instances, one for each modality, containing
                             the processing parameters.
        all_file_info: A list of lists, where each inner list contains the file information
                       for a specific modality, in the format [file1_name, data1_length, ...].
        batch_size: The number of sequences processed in parallel in each batch. (int)

    Returns:
        A tuple containing four lists:
        - batch_wins_list: List of wins for each modality in the current batch.
        - batch_losses_list: List of losses for each modality in the current batch.
        - batch_certainty_list: List of total certainty for each modality in the current batch.
        - batches_processed_list: List of 1 (if batch was processed for directional metrics) or 0 for each modality.
    """
    batch_wins_list = [0] * num_modalities
    batch_losses_list = [0] * num_modalities
    batch_certainty_list = [0.0] * num_modalities
    batches_processed_list = [0] * num_modalities # To indicate if directional metrics were calculated for this modality in this batch


    for modality_index in range(num_modalities):
        # Get modality name from all_modality_params
        modality_params = all_modality_params[modality_index]
        modality_name = modality_params.modality_name if modality_params else f"Modality {modality_index+1}" # Fallback if params is None (shouldn't happen now)

        # Use the first file name as a fallback if modality_name is not provided or is empty string
        if not modality_name or not isinstance(modality_name, str):
             # Get the name of the first file loaded for this modality from all_file_info
             # all_file_info[modality_index][0] is the name of the first file
             if all_file_info and len(all_file_info) > modality_index and all_file_info[modality_index] and all_file_info[modality_index][0]: # Added checks
                 modality_name = os.path.basename(all_file_info[modality_index][0])
             else:
                 modality_name = f"Modality {modality_index+1}" # Fallback if no file info is available


        if len(logits_list) > modality_index and len(yb_list) > modality_index:

            modality_vocab = all_vocabularies[modality_index]
            # Determine if data is percentage data by checking processing steps in ModalityConfig
            is_percentage_data = any(step.get('function') == 'calculate_percent_changes' for step in modality_params.processing_steps)

            # Check if the modality data is numeric and sequence length is sufficient for directional calculations
            data_is_numeric = all(isinstance(item, numbers.Number) for item in modality_vocab) # Check if vocabulary is numeric
            min_seq_len = 1 if is_percentage_data else 2
            if data_is_numeric and yb_list[modality_index].ndim >= 2 and yb_list[modality_index].shape[1] >= min_seq_len:

                logits_modality = logits_list[modality_index][:, -1, :] # Logits for the last token
                targets_modality = yb_list[modality_index][:, -1] # Target index for the last token

                if targets_modality.shape[0] > 0: # Ensure there are elements in the target batch
                    batches_processed_list[modality_index] = 1 # Mark this modality as processed for directional metrics in this batch
                    batch_wins_modality = 0
                    batch_losses_modality = 0
                    batch_certainty_modality_sum = 0.0


                    for j in range(logits_modality.shape[0]): # Iterate through each sequence in the batch
                        predicted_token_logits = logits_modality[j]
                        predicted_token_index = torch.argmax(predicted_token_logits).item()
                        predicted_token_value = modality_vocab[predicted_token_index]

                        actual_token_index = targets_modality[j].item()
                        actual_token_value = modality_vocab[actual_token_index]

                        # Get previous actual value if needed for non-percentage data
                        prev_actual_token_value = None
                        if not is_percentage_data and yb_list[modality_index].shape[1] >= 2:
                             prev_actual_token_value = modality_vocab[yb_list[modality_index][j, -2].item()]


                        # Determine Predicted and Actual Direction Signs using helper function
                        predicted_direction_sign = _get_direction_sign(predicted_token_value, prev_actual_token_value, is_percentage_data)
                        actual_direction_sign = _get_direction_sign(actual_token_value, prev_actual_token_value, is_percentage_data)


                        # --- Count wins/losses based on direction signs ---
                        # Only count if both predicted and actual directions could be determined
                        if predicted_direction_sign is not None and actual_direction_sign is not None:
                            if predicted_direction_sign == actual_direction_sign:
                                 batch_wins_modality += 1
                            else:
                                 batch_losses_modality += 1


                            # --- Directional Certainty Calculation for this batch instance (j) ---
                            probs = F.softmax(predicted_token_logits, dim=-1)
                            summed_certainty_for_direction = 0.0

                            # Iterate through all possible next tokens in the vocabulary
                            for token_index, token_value in enumerate(modality_vocab):
                                # Only consider numeric vocabulary values for certainty
                                if isinstance(token_value, numbers.Number):
                                    # Determine the direction sign of this possible token relative to the relevant previous value
                                    possible_direction_sign = _get_direction_sign(token_value, prev_actual_token_value, is_percentage_data)

                                    # Check if this possible token's direction sign matches the *predicted* direction sign for this batch instance (j)
                                    if possible_direction_sign is not None and possible_direction_sign == predicted_direction_sign:
                                        summed_certainty_for_direction += probs[token_index].item()

                            # Add the calculated certainty for this batch instance (j) to the batch total
                            batch_certainty_modality_sum += summed_certainty_for_direction
                        else:
                             # If direction could not be determined for this instance, it's not counted for wins/losses or certainty
                             pass


                    # Store the total results for this batch and modality
                    batch_wins_list[modality_index] = batch_wins_modality
                    batch_losses_list[modality_index] = batch_losses_modality
                    batch_certainty_list[modality_index] = batch_certainty_modality_sum


            else:
                 # If directional metrics were skipped for this batch and modality, indicate why (optional print, can be removed for cleaner output during training)
                 # modality_data = all_modality_data[modality_index] # Access processed data to check type
                 # data_is_numeric_check = all(isinstance(item, numbers.Number) for item in modality_data)
                 # if not data_is_numeric_check:
                 #      print(f"Warning: Data for Modality {modality_index+1}: '{modality_name}' is not numeric. Directional metrics skipped for this batch.")
                 # elif yb_list[modality_index].ndim < 2 or yb_list[modality_index].shape[1] < min_seq_len:
                 #      print(f"Warning: Sequence length for Modality {modality_index+1}: '{modality_name}' is less than {min_seq_len} ({yb_list[modality_index].shape[1] if yb_list[modality_index].ndim >= 2 else 'N/A'}). Cannot calculate directional metrics for this batch.")
                 pass # Suppress verbose warnings during training


        else:
            # print(f"Could not perform success rate or certainty calculation for Modality {modality_index+1}: '{modality_name}' due to missing logits or targets for this batch.")
            pass # Suppress verbose warnings during training


    return batch_wins_list, batch_losses_list, batch_certainty_list, batches_processed_list


# --- Configuration Loading ---
# Define the path to the YAML configuration file (using project_file_path)
# Assuming project_file_path is defined in a previous cell or globally available
yaml_config_path = project_file_path + 'output/' + 'config.yaml' # Assuming config.yaml is saved in the output folder

# Load configurations from the YAML file
try:
    with open(yaml_config_path, 'r') as file:
        config_data = yaml.safe_load(file)
    print(f"Configuration loaded successfully from: {yaml_config_path}")
except FileNotFoundError:
    raise FileNotFoundError(f"Configuration file not found at: {yaml_config_path}")
except yaml.YAMLError as e:
    raise yaml.YAMLError(f"Error loading or parsing YAML configuration file: {e}")

# Extract hyperparameters and initial parameters
hyperparameters = config_data.get('hyperparameters', {})
# DUPLICATE VARIABLES COMMENTED OUT - Use lines 1489-1494 version instead
# batch_size = hyperparameters.get('batch_size', 8)
# block_size = hyperparameters.get('block_size', 6)
# max_iters = hyperparameters.get('max_iters', 20000)
# eval_interval = hyperparameters.get('eval_interval', 50)
# learning_rate = hyperparameters.get('learning_rate', 3e-4)
eval_iters = hyperparameters.get('eval_iters', 40) # Load eval_iters here
n_embd = hyperparameters.get('n_embd', 16)
n_head = hyperparameters.get('n_head', 4)
n_layer = hyperparameters.get('n_layer', 4)
dropout = hyperparameters.get('dropout', 0.2)

initial_parameters = config_data.get('initial_parameters', {})
# Use loaded project_file_path as a default if not in config
project_file_path = initial_parameters.get('project_file_path', 'C:/Users/tsalo/gpt_proj_6/multimodal_1/')
# Use loaded project_file_path for default model_file_name if not in config
model_file_name = initial_parameters.get('model_file_name', project_file_path + 'output/' + 'TransformerModel.pth')
# Generate a dynamic output_file_name based on timestamp if not in config
output_file_name = initial_parameters.get('output_file_name', f'output_run_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt')
validation_size = initial_parameters.get('validation_size', 0.1)
num_validation_files = initial_parameters.get('num_validation_files', 0)
create_new_model = initial_parameters.get('create_new_model', 0)
save_model = initial_parameters.get('save_model', 1)
patience = initial_parameters.get('patience', 6) # Load patience from config

# Define device after loading parameters
device = 'cuda' if torch.cuda.is_available() else 'cpu'

print("Hyperparameters loaded:")
print(f"batch_size: {batch_size}")
print(f"block_size: {block_size}")
print(f"max_iters: {max_iters}")
print(f"eval_interval: {eval_interval}")
print(f"learning_rate: {learning_rate}")
print(f"eval_iters: {eval_iters}")
print(f"n_embd: {n_embd}")
print(f"n_head: {n_head}")
print(f"n_layer: {n_layer}")
print(f"dropout: {dropout}")

print("\nInitial parameters loaded:")
print(f"project_file_path: {project_file_path}")
print(f"model_file_name: {model_file_name}")
print(f"output_file_name: {output_file_name}")
print(f"validation_size: {validation_size}")
print(f"num_validation_files: {num_validation_files}")
print(f"create_new_model: {create_new_model}")
print(f"save_model: {save_model}")
print(f"patience: {patience}")
print(f"device: {device}")


# --- Data Loading and Processing (from cell b92c0d1f) ---
# The code for data loading and processing, including the definition of
# all_vocabularies, all_modality_data, etc., is now expected to be present
# in the environment due to the execution of cell b92c0d1f.
# No need to redefine or re-execute the data loading logic here.


# Create a list of vocabulary sizes for all modalities
all_vocab_sizes = [len(vocab) for vocab in all_vocabularies]


print('\n\n==========================================================\n\n')
# Instantiate the model based on create_new_model flag
if create_new_model == 1:
    print("Creating a new model...")
    # Pass the list of vocab sizes and all_modality_params to the model constructor
    # all_modality_params now contains ModalityConfig instances
    m = MultimodalTransformer(num_modalities, all_vocab_sizes, all_modality_params, n_embd=n_embd, n_head=n_head, n_layer=n_layer, dropout=dropout, block_size=block_size).to(device)
    optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)
else:
    print(f"Attempting to load model from: {model_file_name}...")
    # Pass the list of vocab sizes and all_modality_params when instantiating the model for loading
    # all_modality_params now contains ModalityConfig instances
    m = MultimodalTransformer(num_modalities, all_vocab_sizes, all_modality_params, n_embd=n_embd, n_head=n_head, n_layer=n_layer, dropout=dropout, block_size=block_size).to(device)
    try:
        m.load_state_dict(torch.load(model_file_name, weights_only=True))
        print("Model loaded successfully.")
        optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)
        print("Optimizer created with loaded model parameters.")
    except FileNotFoundError:
        print(f"Model file not found at: {model_file_name}.\nCreating a new model instead.")
        # Pass the list of vocab sizes and all_modality_params to the model constructor
        # all_modality_params now contains ModalityConfig instances
        m = MultimodalTransformer(num_modalities, all_vocab_sizes, all_modality_params, n_embd=n_embd, n_head=n_head, n_layer=n_layer, dropout=dropout, block_size=block_size).to(device)
        optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)
        print("Optimizer created for the new model.")
    except Exception as e:
        print(f"An error occurred while loading the model: {e}")
        print("Creating a new model instead.")
        # Pass the list of vocab sizes and all_modality_params to the model constructor
        # all_modality_params now contains ModalityConfig instances
        m = MultimodalTransformer(num_modalities, all_vocab_sizes, all_modality_params, n_embd=n_embd, n_head=n_head, n_layer=n_layer, dropout=dropout, block_size=block_size).to(device)
        optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)
        print("Optimizer created for the new model.")


# Calculate and write the number of parameters after the model 'm' is instantiated
num_params = sum(p.numel() for p in m.parameters())/1e6
print(f"Model parameter size: {round(num_params, 2)}M\n")

# --- Prepare data structures for initial file writing ---

# 1. Hyperparameters dictionary
hyperparams = {
    "n_embd": n_embd,
    "n_head": n_head,
    "n_layer": n_layer,
    "block_size": block_size,
    "batch_size": batch_size,
    "dropout": dropout,
    "learning_rate": learning_rate
}

# 2. Run Statistics dictionary
run_stats = {
    "Model parameter size (M)": round(num_params, 2)
}

# 3. Data Information dictionary
# Assuming train/val sizes are the same for all modalities
train_size = len(all_train_sets[0])
val_size_actual = len(all_val_sets[0])
split_method = f"validation_size={validation_size}" if num_validation_files == 0 else f"num_validation_files={num_validation_files}"

# Extract vocab sizes and data lengths for data_info summary
modality_vocab_sizes_summary = ", ".join([f"Modality {i+1}={len(all_vocabularies[i])}" for i in range(num_modalities)])
modality_data_lengths_summary = ", ".join([f"Modality {i+1}={len(all_modality_data[i])}" for i in range(num_modalities)])


data_info = {
    "Number of modalities": num_modalities,
    "Train set size": train_size,
    "Val set size": val_size_actual,
    "Split method": split_method,
    "Modality vocabulary sizes": modality_vocab_sizes_summary,
    "Modality data lengths": modality_data_lengths_summary
}

# 4. Modality Configurations list of dictionaries
modality_configs = []
for i in range(num_modalities):
    modality_params = all_modality_params[i] # This is a ModalityConfig instance
    modality_file_info = all_file_info[i]

    # Access attributes directly from the ModalityConfig instance
    # Convert potential None values and boolean values to string placeholders
    config = {
        "Source": os.path.basename(modality_file_info[0]) if modality_file_info and len(modality_file_info) > 0 and modality_file_info[0] else 'N/A', # Added check for len and existence
        "Modality Name": str(modality_params.modality_name) if modality_params.modality_name is not None else "None",
        # These processing step parameters are now within the processing_steps list in the config
        # We can iterate through processing_steps to summarize, or just indicate they are defined
        "Processing Steps Defined": True if modality_params.processing_steps else False,
        "Rand Size": str(modality_params.randomness_size) if modality_params.randomness_size is not None else "None",
        "Cross-Attend": str(modality_params.cross_attention), # Convert boolean to string
        # Original info is now available directly from the ModalityConfig instance
        "Original Col Num": modality_params.column_number,
        "Original Has Header": modality_params.has_header
    }
    modality_configs.append(config)

# --- End of data structure preparation ---


# Write initial run details to output file
output_file_path = project_file_path + 'output/' + output_file_name
if output_file_name != '':
    # Create the directory if it doesn't exist
    output_dir = os.path.dirname(output_file_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"Created output directory for log file: {output_dir}")

    write_initial_run_details(output_file_path, hyperparams, data_info, modality_configs, run_stats)
    # Add a header for the evaluation results section after the initial details
    with open(output_file_path, 'a', encoding='utf-8') as f:
        f.write("\n\n--- Evaluation Results ---\n") # Add the header


# Training loop:
best_val_loss = float('inf')  # Initialize best validation loss
# patience is now loaded from config
epochs_since_improvement = 0  # Track number of epochs without improvement

# Track if the non-numeric data warning has been printed for each modality in this evaluation run
# These might be better placed within the estimate_loss function scope
non_numeric_warning_printed_train = [False] * num_modalities
non_numeric_warning_printed_val = [False] * num_modalities

def estimate_loss():
    out = {}
    m.eval() # Use 'm' instead of 'model'
    for state in ['train', 'val']:
        total_losses = [] # List to store total loss for each evaluation iteration

        now = datetime.now()
        current_time = now.strftime("%H:%M:%S")
        print(f'\nEvaluating {state} set ({eval_iters} iterations)... Current time: {current_time}')
        # Initialize counters for success rate and certainty calculation for all modalities
        all_modalities_total_batches_processed = [0] * num_modalities
        all_modalities_total_correct = [0] * num_modalities
        all_modalities_total_incorrect = [0] * num_modalities
        all_modalities_total_certainty = [0] * num_modalities

        # Track if the non-numeric data warning has been printed for each modality in this evaluation run
        non_numeric_warning_printed = [False] * num_modalities

        for k in range(eval_iters):
            # get_batch returns lists of tensors: [xb_mod1, xb_mod2, ...], [yb_mod1, yb_mod2, ...]
            xb_list, yb_list = get_batch(state, all_modality_params)

            # Pass lists of tensors to the multimodal model
            logits_list, losses_list = m(xb_list, yb_list) # Use 'm' instead of 'model'

            # Calculate total loss for this evaluation iteration by summing modality losses
            # Ensure losses_list is not None and contains tensors
            if losses_list and all(l is not None for l in losses_list):
                 total_loss_this_iter = sum(losses_list)
                 total_losses.append(total_loss_this_iter.item()) # Store the scalar loss value
            else:
                 # Handle cases where losses might not be calculated (e.g., during generation if targets are None)
                 print(f"Warning: Losses not calculated for iteration {k} in state {state}. Skipping loss recording for this iter.")

            # Call calculate_evaluation_metrics to calculate evaluation metrics for this batch
            # is_percents argument is now redundant and can be removed from the function signature and calls
            # The calculate_evaluation_metrics function accesses percentage status from all_modality_params
            batch_correct, batch_incorrect, batch_certainty, batches_processed_list = calculate_evaluation_metrics(
                logits_list, yb_list, num_modalities, all_vocabularies, all_modality_params, all_file_info, batch_size, False # is_percents parameter - using False as default
            )

            # Check if any modality was skipped due to non-numeric data and print a warning once per eval run
            for modality_index in range(num_modalities):
                if not non_numeric_warning_printed[modality_index]:
                     modality_vocab = all_vocabularies[modality_index]
                     data_is_numeric = all(isinstance(item, numbers.Number) for item in modality_vocab)
                     if not data_is_numeric:
                          modality_params = all_modality_params[modality_index]
                          modality_name = modality_params.modality_name if modality_params else f"Modality {modality_index+1}"
                          if not modality_name or not isinstance(modality_name, str):
                               if all_file_info and len(all_file_info) > modality_index and all_file_info[modality_index]:
                                   modality_name = os.path.basename(all_file_info[modality_index][0])
                               else:
                                   modality_name = f"Modality {modality_index+1}"
                          non_numeric_warning_printed[modality_index] = True

            # Accumulate the results returned by the separate function
            for modality_index in range(num_modalities):
                 all_modalities_total_correct[modality_index] += batch_correct[modality_index]
                 all_modalities_total_incorrect[modality_index] += batch_incorrect[modality_index]
                 all_modalities_total_certainty[modality_index] += batch_certainty[modality_index]
                 all_modalities_total_batches_processed[modality_index] += batches_processed_list[modality_index]

        # Calculate and store the average loss for this state
        if total_losses:
            out[state] = sum(total_losses) / len(total_losses)
        else:
            out[state] = float('inf')  # Handle case where no losses were recorded

        # Report accumulated success rate and certainty for all modalities
        print_state = 'Train' if state == 'train' else 'Val'
        print(f"\n-------  Directional Metrics Summary  -------")
        print(f"\n{print_state} set:")
        for modality_index in range(num_modalities):
            modality_params = all_modality_params[modality_index]
            modality_name = modality_params.modality_name if modality_params else f"Modality {modality_index+1}"
            if not modality_name or not isinstance(modality_name, str):
                 if all_file_info and len(all_file_info) > modality_index and all_file_info[modality_index]:
                     modality_name = os.path.basename(all_file_info[modality_index][0])
                 else:
                     modality_name = f"Modality {modality_index+1}"

            print(f"\nModality {modality_index+1}: '{modality_name}'")
            this_num_batches_processed = all_modalities_total_batches_processed[modality_index]

            if this_num_batches_processed > 0:
                print(f'  Total batches processed: {this_num_batches_processed * batch_size}')
                print(f'  Correct direction predictions: {all_modalities_total_correct[modality_index]}')
                print(f'  Incorrect direction predictions: {all_modalities_total_incorrect[modality_index]}')
                total_movements = all_modalities_total_correct[modality_index] + all_modalities_total_incorrect[modality_index]
                if total_movements > 0:
                    success_rate = all_modalities_total_correct[modality_index] / total_movements
                    print(f'  Success rate: {success_rate:.2%}')
                    avg_certainty = all_modalities_total_certainty[modality_index] / total_movements
                    print(f'  Average certainty: {avg_certainty:.4f}')

    m.train()
    return out

print("Starting training and evaluation loops...")
print("This process involves a lot of computation and can take a considerable amount of time\n")


for iter in range(max_iters): # the loop iterates for a maximum number of iterations (max_iters)
                              # it periodically estimates the loss and prints it
                              # it also generates text samples using the model's generate method
                              # in each iteration, the loop:
                              # 1. gets a batch of training data (get_batch)
                              # 2. passes the data through the model to get predictions and calculate the loss
                              # 3. updates the model's parameters using the optimizer to minimize the loss

    # Evaluate loss every eval_interval iterations or at the end
    now = datetime.now()
    current_time = now.strftime("%H:%M:%S")
    if iter % 100 == 0 : print(f'Training progress: Iteration {iter} of {max_iters}\n')
    if iter % eval_interval == 0 or iter == max_iters - 1:
        # Pass the warning tracking list to estimate_loss
        print(f"Starting evaluation (step {iter})...")
        # Pass eval_iters as an argument to estimate_loss
        losses = estimate_loss() # Uses eval_iters from global scope
        now = datetime.now()
        current_time = now.strftime("%H:%M:%S")
        # Check if losses are valid before printing
        if not torch.isnan(torch.tensor([losses['train'], losses['val']])).any():
             print(f"\n=======================================================================================")
             print(f"Step {iter} Summary: Training Loss: {losses['train']:.4f} | Validation Loss: {losses['val']:.4f} | Time: {current_time}")
             print(f"=======================================================================================\n")
             # write to file
             if output_file_name != '':
               with open(output_file_path, 'a', encoding='utf-8') as f: # Use full path
                   f.write(f"Step {iter} Summary: Training Loss: {losses['train']:.4f} | Validation Loss: {losses['val']:.4f} | Time: {current_time}\n\n")
        else:
             print(f"\n\nStep {iter}: Losses are NaN, skipping print and file write. Current time = {current_time}\n")


        # Early stopping based on validation loss. this is to prevent over fitting
        # if the validation loss doesn't improve for a certain number of iterations (patience), the training process is stopped
        # Only apply early stopping if validation loss is a valid number
        if not torch.isnan(torch.tensor(losses['val'])).any(): # Use .any() for tensor check
            if losses['val'] < best_val_loss:
                best_val_loss = losses['val']
                epochs_since_improvement = 0  # Reset counter if validation loss improves
            else:
                epochs_since_improvement += 1

            if epochs_since_improvement >= patience: # patience is loaded from config
                print(f"Early stopping triggered! Validation loss has not improved for {patience} evaluation intervals.") # Added reason
                break  # Exit the loop
        else:
             print("Validation loss is NaN, skipping early stopping check.")


        # Saving the model's weights to a file (model_file_name)
        now = datetime.now()
        current_time = now.strftime("%H:%M:%S")
        if save_model == 1:
            print(f'Saving model to: {model_file_name}    Current time: {current_time}')
            # When saving, save the state dict of the MultimodalTransformer model
            # Need to ensure model_file_name includes the full path if project_file_path is used
            # model_file_name is loaded as a full path in S3fmsYL-7lVQ
            torch.save(m.state_dict(), model_file_name)
            print("Model size:", round(os.path.getsize(model_file_name)/1024**2,2), "MB\n" )


    # Training steps
    # get_batch returns lists of tensors: [xb_mod1, xb_mod2, ...], [yb_mod1, yb_mod2, ...]
    # get_batch needs access to all_train_sets, all_val_sets, device, block_size, batch_size, randomness_size
    # randomness_size is in all_modality_params, which is accessible globally
    # all_modality_params is a list of ModalityConfig instances, need to pass this to get_batch
    xb_list, yb_list = get_batch('train', all_modality_params) # Pass all_modality_params


    # Pass lists of tensors to the multimodal model
    # m is the model instance
    logits_list, losses_list = m(xb_list, yb_list)


    # Calculate total loss by summing modality losses
    # Ensure losses_list is not None and contains tensors before summing
    if losses_list and all(l is not None for l in losses_list):
        total_loss = sum(losses_list)

        optimizer.zero_grad(set_to_none=True)
        total_loss.backward() # Backpropagate the combined loss
        optimizer.step()
    else:
        # Handle cases where losses might not be calculated (e.g., if targets were None, though get_batch for 'train' should provide them)
        print("Warning: Losses not calculated for training step. Skipping backpropagation.")

    '''
    In essence, the training steps above represent a single training iteration where the model:
        1. Receives data,
        2. Makes predictions,
        3. Calculates the error,
        4. Determines how to adjust its parameters to reduce the error, and
        5. Applies those adjustments.
    line 1: gets a batch of training data (get_batch), in the form of input sequences (xb) and their corresponding target outputs (yb)
            these batches are used to train the model in small increments, making the process more efficient and manageable
    line 2: passes the data through the model to get predictions and calculate the loss
            logits_list, losses_list = m(xb_list, yb_list) # Updated to use 'm'
            logits are the model's raw predictions before any final activation function is applied (like softmax for classification)
            the code also calculates a loss value. This loss quantities how far off the model's predictions (logits) are from the actual target values (yb)
    line 3: this line resets any previously calculated gradients to zero
            optimizer.zero_grad(set_to_none=True)
    line 4: this line initiates the backpropagation process. It calculates the gradients of the loss with respect to all the model's trainable parameters
            total_loss.backward() # Backpropagate the combined loss
            (in simpler terms, it figures out how much each parameter contributed to the error (loss) and in which direction the parameter should be adjusted to reduce the error)
    line 5: this line updates the model's parameters using the optimizer to minimize the loss
            optimizer.step()
            the optimizer (AdamW) takes a step towards minimizing the loss by adjusting the parameters in the direction indicated by the gradients
    '''

# Assuming the training loop finishes (either by max_iters or early stopping), save the final model state
now = datetime.now()
current_time = now.strftime("%H:%M:%S")
if save_model == 1:
    print(f'Training finished. Saving final model to: {model_file_name}    Current time: {current_time}')
    # Need to ensure model_file_name includes the full path if project_file_path is used
    # model_file_name is loaded as a full path in S3fmsYL-7lVQ
    torch.save(m.state_dict(), model_file_name)
    print("Model size:", round(os.path.getsize(model_file_name)/1024**2,2), "MB\n" )

"""#End New Code"""

