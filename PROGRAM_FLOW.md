# MULTIMODAL TRANSFORMER TRAINING FLOW

## Complete Program Execution Flow

```
START: main.py execution
â”‚
|- 1. CONFIGURATION LOADING (main.py:30-60)
|   |- config_manager.py -> load_config()
|   |   |- Loads config.yaml -> system settings
|   |   '- Loads input_schemas.yaml -> modality configurations
|   |- schema.py -> SchemaManager.load_from_yaml()
|   |   |- Validates modality configurations
|   |   '- Creates InputSchema objects
|   '- Sets global config variables
â”‚
|- 2. DATA LOADING & PROCESSING (main.py:61-235)
|   |- For each modality:
|   |   |- data_utils.py -> load_and_process_data()
â”‚   â”‚   â”‚   â”œâ”€ Loads CSV/TXT files from path
â”‚   â”‚   â”‚   â”œâ”€ Extracts specified column
â”‚   â”‚   â”‚   â””â”€ Returns raw numeric data + file info
â”‚   â”‚   â”œâ”€ processing_pipeline.py â†’ ProcessingPipeline.process()
â”‚   â”‚   â”‚   â”œâ”€ Applies processing steps sequentially:
â”‚   â”‚   â”‚   â”‚   â”œâ”€ convert_to_percent_changes()
â”‚   â”‚   â”‚   â”‚   â”œâ”€ range_numeric_data()
â”‚   â”‚   â”‚   â”‚   â””â”€ bin_numeric_data()
â”‚   â”‚   â”‚   â””â”€ processing_registry.py â†’ validates functions
â”‚   â”‚   â””â”€ Creates vocabulary from processed data
â”‚   â””â”€ ğŸ“Š VOCABULARY BUILDING output
â”‚
â”œâ”€ 3ï¸âƒ£ DATASET SPLITTING (main.py:245-280)
â”‚   â”œâ”€ data_utils.py â†’ create_train_val_datasets()
â”‚   â”‚   â”œâ”€ If num_validation_files > 0: File-based split
â”‚   â”‚   â””â”€ Else: Percentage-based split using validation_size
â”‚   â””â”€ ğŸ—‚ï¸ DATASET SPLITTING output
â”‚
â”œâ”€ 4ï¸âƒ£ MODEL INITIALIZATION (main.py:282-320)
â”‚   â”œâ”€ model.py â†’ MultimodalTransformer()
â”‚   â”‚   â”œâ”€ Creates embedding layers for each modality
â”‚   â”‚   â”œâ”€ Initializes MultimodalBlock layers
â”‚   â”‚   â”‚   â”œâ”€ MultiHeadAttention (self-attention)
â”‚   â”‚   â”‚   â”œâ”€ CrossAttention (cross-modal attention)
â”‚   â”‚   â”‚   â””â”€ FeedForward layers
â”‚   â”‚   â””â”€ MultimodalPreBlock/PostBlock
â”‚   â”œâ”€ If create_new_model=0: Load existing model weights
â”‚   â””â”€ Move model to device (CPU/CUDA)
â”‚
â”œâ”€ 5ï¸âƒ£ TRAINING SETUP (main.py:321-390)
â”‚   â”œâ”€ Creates PyTorch optimizer (AdamW)
â”‚   â”œâ”€ Sets up training log file
â”‚   â”œâ”€ data_utils.py â†’ write_training_log_header()
â”‚   â”‚   â””â”€ Writes configuration summary to log
â”‚   â””â”€ ğŸš€ TRAINING STARTUP output
â”‚
â”œâ”€ 6ï¸âƒ£ MAIN TRAINING LOOP (main.py:391-520)
â”‚   â”‚
â”‚   â”œâ”€ For each iteration (0 to max_iters):
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€ ğŸ”„ TRAINING STEP
â”‚   â”‚   â”‚   â”œâ”€ training_utils.py â†’ train_step()
â”‚   â”‚   â”‚   â”‚   â”œâ”€ Gets random training batches
â”‚   â”‚   â”‚   â”‚   â”œâ”€ model.forward() â†’ MultimodalTransformer
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€ Embedding lookup for each modality
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€ Processes through transformer blocks:
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€ Self-attention (all modalities)
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€ Cross-attention (if enabled)
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€ FeedForward
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€ Output predictions
â”‚   â”‚   â”‚   â”‚   â”œâ”€ Calculates loss (CrossEntropyLoss)
â”‚   â”‚   â”‚   â”‚   â”œâ”€ Backpropagation
â”‚   â”‚   â”‚   â”‚   â””â”€ Optimizer step
â”‚   â”‚   â”‚   â””â”€ Returns training loss
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€ ğŸ“Š EVALUATION (every eval_interval steps)
â”‚   â”‚   â”‚   â”œâ”€ training_utils.py â†’ calculate_evaluation_metrics()
â”‚   â”‚   â”‚   â”‚   â”œâ”€ Runs model on validation batches
â”‚   â”‚   â”‚   â”‚   â”œâ”€ Calculates validation loss
â”‚   â”‚   â”‚   â”‚   â”œâ”€ Calculates directional accuracy
â”‚   â”‚   â”‚   â”‚   â””â”€ Returns metrics dictionary
â”‚   â”‚   â”‚   â”œâ”€ ğŸ¯ LOSS METRICS output (console)
â”‚   â”‚   â”‚   â”œâ”€ ğŸ“ˆ STEP log entry (training_log.txt)
â”‚   â”‚   â”‚   â””â”€ Model saving (if save_model=1)
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€ Early stopping check (if validation improves)
â”‚   â”‚
â”‚   â””â”€ Loop continues until max_iters reached
â”‚
â”œâ”€ 7ï¸âƒ£ TRAINING COMPLETION (main.py:521-560)
â”‚   â”œâ”€ Final model save (if save_model=1)
â”‚   â”œâ”€ Final evaluation metrics
â”‚   â”œâ”€ Cleanup (file_cache.py â†’ cleanup_cache())
â”‚   â””â”€ ğŸ TRAINING COMPLETE output
â”‚
â””â”€ ğŸ”š END: Program termination
```

## ğŸ“ Key Files and Their Roles

| File | Primary Responsibility |
|------|----------------------|
| **main.py** | Main execution orchestrator |
| **config_manager.py** | Configuration loading/validation |
| **schema.py** | YAML schema management |
| **data_utils.py** | Data loading, processing, splitting |
| **processing_pipeline.py** | Data processing coordination |
| **processing_registry.py** | Function validation/resolution |
| **model.py** | Neural network architecture |
| **training_utils.py** | Training/evaluation operations |
| **file_cache.py** | File caching and cleanup |

## ğŸ—‚ï¸ Configuration Files

| File | Purpose |
|------|---------|
| **config.yaml** | System settings & hyperparameters |
| **input_schemas.yaml** | Modality definitions & processing |

## ğŸ“Š Output Files

| File | Content |
|------|---------|
| **training_log.txt** | Detailed training log |
| **TransformerModel.pth** | Saved model weights |

## ğŸ”„ Key Processing Phases

### Phase 1: Configuration & Validation
- Load YAML configurations
- Validate modality settings
- Set up global parameters

### Phase 2: Data Pipeline
- Load CSV/TXT data files
- Apply processing functions (percentages, ranging, binning)
- Build vocabularies
- Split into train/validation sets

### Phase 3: Model Setup
- Initialize transformer architecture
- Set up self-attention and cross-attention layers
- Configure optimizer and training parameters

### Phase 4: Training Loop
- Training steps with backpropagation
- Periodic evaluation and logging
- Model checkpointing
- Early stopping detection

### Phase 5: Completion
- Final model save
- Cleanup and termination